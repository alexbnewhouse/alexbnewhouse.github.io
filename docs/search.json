[
  {
    "objectID": "political-text-classification.html",
    "href": "political-text-classification.html",
    "title": "DistilBERT for Political Text Classification",
    "section": "",
    "text": "This project demonstrates the application of transformer-based natural language processing for identifying extremist content in political discussions. Using DistilBERT, a lightweight BERT variant, we achieve 94% F1-score on a labeled dataset of political forum posts, significantly outperforming traditional machine learning baselines. The model shows practical applications for content moderation, academic research on political radicalization, and policy development for platform governance."
  },
  {
    "objectID": "political-text-classification.html#abstract",
    "href": "political-text-classification.html#abstract",
    "title": "DistilBERT for Political Text Classification",
    "section": "",
    "text": "This project demonstrates the application of transformer-based natural language processing for identifying extremist content in political discussions. Using DistilBERT, a lightweight BERT variant, we achieve 94% F1-score on a labeled dataset of political forum posts, significantly outperforming traditional machine learning baselines. The model shows practical applications for content moderation, academic research on political radicalization, and policy development for platform governance."
  },
  {
    "objectID": "political-text-classification.html#research-context",
    "href": "political-text-classification.html#research-context",
    "title": "DistilBERT for Political Text Classification",
    "section": "Research Context",
    "text": "Research Context\nOnline political discourse increasingly shapes real-world political outcomes, with extremist content posing particular challenges for platform governance and democratic stability. This project applies state-of-the-art NLP techniques to automatically identify extremist political content, supporting:\n\nContent moderation at scale for social media platforms\nAcademic research on political radicalization processes\nPolicy development for online platform governance\nEarly warning systems for potential offline violence\n\n\nResearch Questions\n\nHow effectively can transformer models identify extremist content in political text?\nWhat performance gains do modern NLP approaches offer over traditional machine learning?\nHow can data augmentation improve model performance on limited labeled data?"
  },
  {
    "objectID": "political-text-classification.html#methodology",
    "href": "political-text-classification.html#methodology",
    "title": "DistilBERT for Political Text Classification",
    "section": "Methodology",
    "text": "Methodology\nOur approach employs DistilBERT, a distilled version of BERT that retains 97% of BERT’s performance while being 60% smaller and faster. The methodology includes:\n\nText preprocessing and cleaning for social media data\nData augmentation using contextual word embeddings\nTransfer learning from pre-trained language models\nComparative evaluation against traditional ML baselines\nReal-world application to unlabeled political forum data"
  },
  {
    "objectID": "political-text-classification.html#data-collection-preprocessing",
    "href": "political-text-classification.html#data-collection-preprocessing",
    "title": "DistilBERT for Political Text Classification",
    "section": "Data Collection & Preprocessing",
    "text": "Data Collection & Preprocessing\n\nDataset Characteristics\nOur labeled dataset consists of political forum posts manually annotated for extremist content:\n\nSize: 1,438 posts (after augmentation)\nSources: Political discussion forums and social media\nLabels: Binary classification (extremist/non-extremist)\nDomain: Contemporary political discourse\n\n\n\nPreprocessing Pipeline\n\n\nCode\nimport pandas as pd\nimport torch\nimport transformers\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom transformers import DistilBertModel, DistilBertTokenizer\nfrom sklearn import metrics\nfrom tqdm import tqdm\nimport numpy as np\nimport re\n\n# Text preprocessing functions\ndef remove_URL(text):\n    \"\"\"Remove URLs from text\"\"\"\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_numbers(text):\n    \"\"\"Remove standalone numbers\"\"\"\n    text = ''.join([i for i in text if not i.isdigit()])\n    return text\n\ndef remove_html(text):\n    \"\"\"Remove HTML tags\"\"\"\n    html = re.compile(r'&lt;.*?&gt;')\n    return html.sub(r'',text)\n\ndef remove_username(text):\n    \"\"\"Remove @username mentions\"\"\"\n    url = re.compile(r'@[A-Za-z0-9_]+')\n    return url.sub(r'',text)\n\ndef pre_process_text(text):\n    \"\"\"Apply full preprocessing pipeline\"\"\"\n    text = remove_URL(text)\n    text = remove_numbers(text)\n    text = remove_html(text)\n    text = remove_username(text)\n    return \" \".join(text.split())\n\n# Load and preprocess data\ndf = pd.read_csv(\"labeled_political_data.csv\")\ndf = df.rename(columns={\"sentiment\": \"target\"})\ndf = df[[\"text\", \"target\"]].copy()\n\n# Remove null values and apply preprocessing\ndf = df[df['text'].notnull()]\ndf['text'] = df['text'].apply(pre_process_text)\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Target distribution:\\n{df['target'].value_counts()}\")\n\n\n\n\nData Augmentation Strategy\nTo address limited training data, we employ contextual word embedding augmentation:\n\n\nCode\nimport nlpaug.augmenter.word as naw\n\n# Contextual augmentation using DistilBERT\naug = naw.ContextualWordEmbsAug(\n    model_path='distilbert-base-uncased',\n    device='cpu',\n    action=\"substitute\"\n)\n\n# Generate augmented examples\naugmented_texts = [aug.augment(text) for text in tqdm(df['text'])]\naugmented_df = pd.DataFrame({\n    \"text\": augmented_texts,\n    \"target\": df['target']\n})\n\n# Combine original and augmented data\ndf = pd.concat([df, augmented_df]).reset_index(drop=True)\nprint(f\"Final dataset size: {df.shape[0]} examples\")"
  },
  {
    "objectID": "political-text-classification.html#model-architecture",
    "href": "political-text-classification.html#model-architecture",
    "title": "DistilBERT for Political Text Classification",
    "section": "Model Architecture",
    "text": "Model Architecture\n\nDistilBERT Configuration\n\n\nCode\n# Model hyperparameters\nMAX_LEN = 512\nBATCH_SIZE = 32\nEPOCHS = 15\nLEARNING_RATE = 1e-05\n\n# Initialize tokenizer\nBERT_PATH = \"distilbert-base-uncased\"\ntokenizer = DistilBertTokenizer.from_pretrained(\n    BERT_PATH,\n    do_lower_case=True\n)\n\n\n\n\nCustom Dataset Classes\n\n\nCode\nclass PoliticalTextDataset(Dataset):\n    \"\"\"Custom dataset for political text classification\"\"\"\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __getitem__(self, index):\n        text = str(self.data.text.iloc[index])\n        text = pre_process_text(text)\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_token_type_ids=True,\n            truncation=True\n        )\n\n        return {\n            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'targets': torch.tensor(self.data.target.iloc[index], dtype=torch.float)\n        }\n\n    def __len__(self):\n        return len(self.data)\n\n\n\n\nModel Architecture\n\n\nCode\nclass DistilBERTClassifier(torch.nn.Module):\n    \"\"\"DistilBERT-based binary classifier for political text\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.distill_bert = transformers.DistilBertModel.from_pretrained(BERT_PATH)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(768, 1)\n\n    def forward(self, ids, mask):\n        # Get DistilBERT outputs\n        distilbert_output = self.distill_bert(ids, mask)\n        hidden_state = distilbert_output[0]  # (batch_size, seq_len, hidden_size)\n\n        # Use [CLS] token representation\n        pooled_output = hidden_state[:, 0]  # (batch_size, hidden_size)\n\n        # Apply dropout and classification layer\n        output = self.dropout(pooled_output)\n        return self.classifier(output)\n\n# Initialize model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = DistilBERTClassifier()\nmodel.to(device)\n\nprint(f\"Model initialized on device: {device}\")\nprint(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
  },
  {
    "objectID": "political-text-classification.html#training-process",
    "href": "political-text-classification.html#training-process",
    "title": "DistilBERT for Political Text Classification",
    "section": "Training Process",
    "text": "Training Process\n\nData Splitting and Loading\n\n\nCode\n# Split data\ntrain_size = 0.7\ntrain_dataset = df.sample(frac=train_size, random_state=42).reset_index(drop=True)\nvalid_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n\nprint(f\"Training set: {train_dataset.shape[0]} examples\")\nprint(f\"Validation set: {valid_dataset.shape[0]} examples\")\n\n# Create data loaders\ntraining_set = PoliticalTextDataset(train_dataset, tokenizer, MAX_LEN)\nvalidation_set = PoliticalTextDataset(valid_dataset, tokenizer, MAX_LEN)\n\ntrain_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=False)\n\n\n\n\nTraining Loop\n\n\nCode\ndef train_model(model, train_loader, valid_loader, epochs):\n    \"\"\"Training loop with validation\"\"\"\n\n    # Loss function and optimizer\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    best_f1 = 0\n    training_history = []\n\n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        total_loss = 0\n\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(ids, mask).squeeze()\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        # Validation phase\n        val_f1 = evaluate_model(model, valid_loader)\n        avg_loss = total_loss / len(train_loader)\n\n        print(f'Epoch {epoch+1}: Train Loss: {avg_loss:.4f}, Val F1: {val_f1:.4f}')\n\n        # Save best model\n        if val_f1 &gt; best_f1:\n            best_f1 = val_f1\n            torch.save(model.state_dict(), 'best_political_classifier.pth')\n\n        training_history.append({\n            'epoch': epoch + 1,\n            'train_loss': avg_loss,\n            'val_f1': val_f1\n        })\n\n    return training_history\n\ndef evaluate_model(model, data_loader):\n    \"\"\"Evaluate model and return F1 score\"\"\"\n    model.eval()\n    predictions = []\n    true_labels = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            outputs = model(ids, mask)\n            preds = torch.sigmoid(outputs).cpu().numpy() &gt;= 0.5\n\n            predictions.extend(preds.flatten())\n            true_labels.extend(targets.cpu().numpy())\n\n    return metrics.f1_score(true_labels, predictions)\n\n# Train the model\nhistory = train_model(model, train_loader, valid_loader, EPOCHS)"
  },
  {
    "objectID": "political-text-classification.html#results-evaluation",
    "href": "political-text-classification.html#results-evaluation",
    "title": "DistilBERT for Political Text Classification",
    "section": "Results & Evaluation",
    "text": "Results & Evaluation\n\nPerformance Comparison\nOur DistilBERT model significantly outperforms traditional machine learning approaches:\n\n\n\nModel\nF1-Score\nBalanced Accuracy\nTraining Time\n\n\n\n\nRandom Baseline\n0.384\n0.480\n-\n\n\nLogistic Regression\n0.637\n0.812\n2 minutes\n\n\nRandom Forest\n0.498\n0.927\n5 minutes\n\n\nGradient Boosting\n0.670\n0.947\n8 minutes\n\n\nDistilBERT\n0.940\n0.935\n45 minutes\n\n\n\n\n\nModel Performance Analysis\n\n\nCode\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Generate predictions on validation set\nmodel.eval()\nval_predictions = []\nval_true = []\n\nwith torch.no_grad():\n    for batch in valid_loader:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets']\n\n        outputs = model(ids, mask)\n        preds = torch.sigmoid(outputs).cpu().numpy() &gt;= 0.5\n\n        val_predictions.extend(preds.flatten())\n        val_true.extend(targets.numpy())\n\n# Print detailed classification report\nprint(\"## Classification Report\")\nprint(classification_report(val_true, val_predictions,\n                          target_names=['Non-Extremist', 'Extremist']))\n\n# Confusion Matrix Visualization\ncm = confusion_matrix(val_true, val_predictions)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Non-Extremist', 'Extremist'],\n            yticklabels=['Non-Extremist', 'Extremist'])\nplt.title('Confusion Matrix - DistilBERT Political Text Classifier')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\n\n\n\nKey Findings\n\nSuperior Performance: DistilBERT achieves 94% F1-score, representing a 27% improvement over the best traditional ML approach\nRobust Classification: High precision and recall for both classes indicate reliable extremist content detection\nEfficient Architecture: DistilBERT provides near-BERT performance with significantly reduced computational requirements\nData Augmentation Benefits: Contextual augmentation improved performance by ~8% over the baseline"
  },
  {
    "objectID": "political-text-classification.html#real-world-application",
    "href": "political-text-classification.html#real-world-application",
    "title": "DistilBERT for Political Text Classification",
    "section": "Real-World Application",
    "text": "Real-World Application\n\nInference Pipeline\n\n\nCode\ndef predict_extremist_content(text, model, tokenizer):\n    \"\"\"Predict whether text contains extremist content\"\"\"\n\n    # Preprocess text\n    processed_text = pre_process_text(text)\n\n    # Tokenize\n    inputs = tokenizer.encode_plus(\n        processed_text,\n        None,\n        add_special_tokens=True,\n        padding='max_length',\n        max_length=MAX_LEN,\n        return_tensors='pt',\n        truncation=True\n    )\n\n    # Model inference\n    model.eval()\n    with torch.no_grad():\n        ids = inputs['input_ids'].to(device)\n        mask = inputs['attention_mask'].to(device)\n\n        outputs = model(ids, mask)\n        probability = torch.sigmoid(outputs).cpu().numpy()[0][0]\n        prediction = probability &gt; 0.5\n\n    return {\n        'prediction': bool(prediction),\n        'confidence': float(probability),\n        'label': 'Extremist' if prediction else 'Non-Extremist'\n    }\n\n# Example usage\nsample_texts = [\n    \"I disagree with the current immigration policy and think we need reform.\",\n    \"The political system is corrupt and needs to be overthrown by any means necessary.\"\n]\n\nfor text in sample_texts:\n    result = predict_extremist_content(text, model, tokenizer)\n    print(f\"Text: {text[:50]}...\")\n    print(f\"Prediction: {result['label']} (confidence: {result['confidence']:.3f})\")\n    print()\n\n\n\n\nCase Study: Forum Analysis\nApplied to a large corpus of political forum posts (N=50,000), our model identified:\n\n8,481 posts (17%) flagged as potentially extremist\nHigh-confidence predictions (&gt;0.9) for 3,247 posts\nTemporal patterns showing increased extremist content around election periods\nTopic clustering revealing common themes in flagged content"
  },
  {
    "objectID": "political-text-classification.html#limitations-future-work",
    "href": "political-text-classification.html#limitations-future-work",
    "title": "DistilBERT for Political Text Classification",
    "section": "Limitations & Future Work",
    "text": "Limitations & Future Work\n\nCurrent Limitations\n\nDomain Specificity: Model trained on specific political forums may not generalize to all platforms\nContextual Challenges: Sarcasm and irony remain difficult to detect accurately\nTemporal Drift: Political language evolves rapidly, requiring model updates\nBias Concerns: Training data may reflect annotator biases\n\n\n\nFuture Directions\n\nMulti-domain Training: Expand to diverse political platforms and languages\nTemporal Robustness: Implement continuous learning strategies\nExplainability: Add attention visualization for model interpretability\nEthical Framework: Develop guidelines for responsible deployment"
  },
  {
    "objectID": "political-text-classification.html#technical-implementation",
    "href": "political-text-classification.html#technical-implementation",
    "title": "DistilBERT for Political Text Classification",
    "section": "Technical Implementation",
    "text": "Technical Implementation\n\nModel Deployment\n\n\nCode\n# Save complete model for deployment\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'tokenizer': tokenizer,\n    'model_config': {\n        'max_length': MAX_LEN,\n        'model_name': 'distilbert-base-uncased'\n    }\n}, 'political_classifier_complete.pth')\n\n# Load model for inference\ndef load_trained_model(model_path):\n    \"\"\"Load pre-trained model for inference\"\"\"\n    checkpoint = torch.load(model_path, map_location=device)\n\n    model = DistilBERTClassifier()\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n\n    return model, checkpoint['tokenizer'], checkpoint['model_config']"
  },
  {
    "objectID": "political-text-classification.html#conclusions",
    "href": "political-text-classification.html#conclusions",
    "title": "DistilBERT for Political Text Classification",
    "section": "Conclusions",
    "text": "Conclusions\nThis project demonstrates the effectiveness of transformer-based models for automated detection of extremist political content. Key contributions include:\n\nMethodological Innovation: Successfully adapted DistilBERT for political text classification with 94% F1-score\nPractical Application: Developed scalable pipeline for real-world content moderation\nComparative Analysis: Demonstrated significant advantages over traditional ML approaches\nResearch Impact: Provided tools for studying online political radicalization\n\nThe results have implications for: - Platform Governance: Automated content moderation systems - Academic Research: Large-scale analysis of political discourse - Policy Development: Evidence-based approaches to online extremism\n\nReproducibility\nAll code and documentation are available for replication. The model architecture and training procedures follow established best practices for transformer-based text classification.\n\nThis research contributes to computational approaches for understanding and mitigating online political extremism, supporting both academic inquiry and practical applications in digital platform governance."
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Alex Newhouse",
    "section": "",
    "text": "alex.newhouse@colorado.edu | Boulder, CO | linkedin.com/in/alexbnewhouse"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Alex Newhouse",
    "section": "Education",
    "text": "Education\nUniversity of Colorado Boulder, Boulder, CO\nDoctor of Philosophy, Political Science, Anticipated Graduation: May 2027\n\nResearch focus: Measuring online-offline relationships in political violence using machine learning, natural language processing, social network analysis, and time-series causal inference\nComparative Politics and Political Methodology subfields\nPSCI committee: Andrew Q. Philips (co-chair), Jennifer Fitzgerald (co-chair), Alexandra Siegel\nOther advisor: Brian Keegan (Information Science, CU Boulder)\n\nGeorgia Institute of Technology, Atlanta, GA (Remote Student)\nMaster of Science, Analytics, Graduation: Fall 2021\n\nCoursework in Python, R, JavaScript, Deep and Machine Learning, and Data Visualization\n\nMiddlebury Institute of International Studies (MIIS), Monterey, CA\nMaster of Arts, Nonproliferation and Terrorism Studies, Graduation: Dec. 2018\n\nFinancial Crimes Management Specialization; Foreign Language of Study: Russian\n\nMiddlebury College, Middlebury, VT\nBachelor of Arts, Political Science and English Literature, Russian Minor, Graduation: May 2017\n\nGraduated Summa cum Laude, departmental honors in English"
  },
  {
    "objectID": "cv.html#research-assistanceships-and-labs",
    "href": "cv.html#research-assistanceships-and-labs",
    "title": "Alex Newhouse",
    "section": "Research Assistanceships and Labs",
    "text": "Research Assistanceships and Labs\n\nFellowships\nSenior Non-Resident Fellow, MIIS, Mar. 2025 – Present\n\n\nLabs\nGraduate Affiliate, Institute for Behavioral Science, 2023 – Present\nLab Member, Colorado Laboratory on Users, Media, and Networks, 2023 – Present\nLab Member, American Politics Research Lab, 2024 - Present\n\n\nPaid Research Positions\nGraduate Research Assistant, CU Boulder, 2025 – Present - Exploratory quantitative and qualitative analysis for Jennifer Fitzgerald and Jeffrey Nonmacher’s work on global perceptions of inequality\nGraduate Research Assistant, CU Boulder, 2025 – Present - Qualitative and quantitative support for Alexandra Siegel’s book project on the interactions between online influencers and political regimes\nGraduate Research Assistant, CU Boulder, 2024 – Present - “‘Mind the Gap!’ Policy Dissonance and Financial Markets”, supervised by Andrew Q. Philips and in collaboration with Allyson L. Benton\nGraduate Research Assistant, CU Boulder, 2023 – Present - “Partisan Polarization Shapes Visual Framing of COVID-19 in U.S. Media”, supervised by Andrew Q. Philips and in collaboration with Komal P. Kaur\nGraduate Research Assistant, CU Boulder, Summer 2024 - Data analysis and synthesis of original survey data on Australian attitudes around Voice to Parliament referendum, supervised by Carew Boulding\nGraduate Research Assistant, MIIS, Fall 2018 - Assisted with launch of the Center on Terrorism, Extremism, and Counterterrorism, and built first project to secure funding from technology industry partner, supervised by Jason Blazakis\nGraduate Research Assistant, MIIS, Fall 2018 - UNIDIR-funded project collecting data on global cybersecurity projects, supervised by Elaine Korzak\nGraduate Research Assistant, MIIS, Feb. 2018 – Dec. 2018 - Contributed to META Lab research and education projects, supervised by Philip Murphy\nGraduate Research Assistant, MIIS and Naval Postgraduate School, Feb. 2018 – Aug. 2018 - Lead data analyst on processing and analysis of Islamic State primary-source documents, supervised by Craig Whiteside\nUndergraduate Research Assistant, Middlebury, Aug. 2016 – May 2017 - Data analysis and background research on intervention, civil conflict, and the CNN Effect, supervised by Orion Lewis\nUndergraduate Research Assistant, MIIS, Feb. 2016 – May 2016 - Contributed to various research and analysis projects at the James Martin Center for Nonproliferation Studies"
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Alex Newhouse",
    "section": "Teaching Experience",
    "text": "Teaching Experience\n\nAs Teaching Assistant\n\nIntroduction to Western Political Thought, CU Boulder, Spring 2025\nIntroduction to Comparative Politics, CU Boulder, Fall 2024\nQuantitative Research Methods, CU Boulder, Spring 2024\nIntroduction to International Relations, CU Boulder, Fall 2023\n\n\n\nAs Adjunct Professor\n\nDigital Extremism, Middlebury Institute of International Studies, Spring 2023 (self-designed)\nOnline Extremism, Middlebury College, Jan. 2021 (self-designed)"
  },
  {
    "objectID": "cv.html#mentorship-and-supervision",
    "href": "cv.html#mentorship-and-supervision",
    "title": "Alex Newhouse",
    "section": "Mentorship and Supervision",
    "text": "Mentorship and Supervision\nManaged a total of 30 Middlebury undergraduate and graduate research assistants between 2019-2023.\nSample projects:\n\nPartnership with Roblox to detect and mitigate violent and hateful networks of users\nPartnership with Spectrum Labs to develop datasets of online toxicity, white supremacy, and violent behavior in 7 languages\nLong-term research and policy support for a video social media platform’s policy enforcement team\n\nSupervised independent projects for 15 Middlebury undergraduate interns and 4 graduate fellows.\nSample projects supervised:\n\n“Italian Neofascism and the Years of Lead: A Closer Look at the Nuclei Armati Rivoluzionari.”\n“Archeofuturism and Its Parallels to Militant Accelerationism.”\n“The French Far-Right on Telegram: Laundering Extremism Through Support For Political Activism.”\n“Christian Identity’s New Role on the Extreme Right.”\n“The AI Revolution and Its Implications on Domestic Counterterrorism.”"
  },
  {
    "objectID": "cv.html#grants-and-fundraising",
    "href": "cv.html#grants-and-fundraising",
    "title": "Alex Newhouse",
    "section": "Grants and Fundraising",
    "text": "Grants and Fundraising\nMiddlebury Conflict Transformation Grant, FY 2022 - $50,000, Principal Investigator\nDHS Terrorism and Targeted Violence Prevention Grant, FY 2022 - $700,000, Principal Investigator\nDHS Terrorism and Targeted Violence Prevention Grant, FY 2021 – $630,000, Principal Investigator"
  },
  {
    "objectID": "cv.html#employment",
    "href": "cv.html#employment",
    "title": "Alex Newhouse",
    "section": "Employment",
    "text": "Employment\nMiddlebury Institute of International Studies, Monterey, CA\nSenior Research Fellow – Emerging Tech, CTEC, June 2023-August 2024\n\nSupervised CTEC’s portfolio of projects that engage with the video game industry\nLed cutting-edge research into the influence of emerging technologies like AI on worldwide extremism and terrorism trends\nAssisted with CTEC’s relationships with the tech industry\nEngaged with media, published research, and served as a leading voice at CTEC on various extremism-related issues\nCoordinated CTEC’s data science projects and analyst resources\n\nDeputy Director, CTEC, April 2021-May 2023\n\nOversaw CTEC’s research, operations, and development\nHired and managed a team of full-time, part-time, and student researchers\nDirectly supervised external relationships with technology companies, government agencies, academia, and nonprofits\nServed as Principal Investigator on several research projects, including two funded by grants from the Department of Homeland Security\nBuilt CTEC’s Data Science functions, including overseeing development of innovative tools using AI, machine learning, NLP, and social network analysis\n\nData Analyst and Research Lead, CTEC, Jan. 2019-April 2021\n\nBuilt and delivered an end-to-end data analysis and visualization platform for understanding trends in hate and violence on social video platforms at scale\nLed development of glossary of right-wing terminology, slang, and images\nRaised money to expand CTEC by developing client relationships in the tech industry\n\nUnited States House of Representatives, Washington, DC\nInvestigative Consultant, Select Committee to Investigate the January 6th Attack, Feb. 2022-Oct. 2022\n\nDesigned and executed cross-platform investigations on how online communities contributed to the January 6th attack on the U.S. Capitol\nProvided expertise in online extremism, including the activities of Oath Keepers, Proud Boys, QAnon, Three Percenters, and neo-fascists, to investigators and Members of Congress\nDrafted sections of public hearings conducted by Members of Congress in June-July 2022\nDelivered written material included in the Committee’s final report\n\nSony Interactive Entertainment, San Mateo, CA\nData Governance Analyst, Jan. 2019-Jan. 2020\n\nManaged and coordinated PlayStation’s data-sharing process\nAssisted with creation and implementation of privacy policies and standards\nOperationalized SIE’s privacy-by-design goals so that users have safety, security, control, and transparency over their own data\nCollaborated closely with privacy lawyers to develop a shared language that allows for understanding of and compliance with privacy legislation, like the GDPR and CCPA\n\nSony Interactive Entertainment, San Diego, CA\nGlobal Fraud Monitoring Analyst Intern, Summer 2018\n\nWorked on fraud monitoring and development of fraud prevention rules for PlayStation Network\nLed project streamlining cooperation between Customer Service and Global Fraud Management\nAssisted Holiday Season 2018 planning by creating Splunk dashboards, revising runbooks, and laying out on-call procedures\n\nUber, San Francisco, CA\nCrisis Management Intern, Fall 2017\n\nWorked with the Crisis Management/Business Continuity Team to develop business continuity plans for all divisions within Uber\nDeveloped set of metrics using SQL for assessing crisis response and impact, which the Crisis Management Team incorporated into its performance reports and incident database\nCooperated with a variety of stakeholders to help develop a global Crisis Management Playbook\nAssisted Protective Intelligence team with research for a city launch geopolitical risk assessment\n\nGameSpot – CBS Interactive, San Francisco, CA\nEditorial Intern, Summers 2014, 2015, 2016"
  },
  {
    "objectID": "cv.html#peer-reviewed-publications",
    "href": "cv.html#peer-reviewed-publications",
    "title": "Alex Newhouse",
    "section": "Peer-Reviewed Publications",
    "text": "Peer-Reviewed Publications\nBenton, A., Newhouse, A., and Philips, A. “Mind the Gap! Policy Dissonance and Financial Markets.” May 2025 (under review).\nNewhouse, A. and Kowert, R. “Extremist Identity Creation Through Performative Infighting on Steam.” Frontiers in Psychology (forthcoming).\nKowert, R., Kilmer, E., and Newhouse, A. “Taking it to the Extreme: Prevalence and Nature of Extremist Sentiment in Games.” Frontiers in Psychology. August 2024.\nKowert, R., Kilmer, E., and Newhouse, A. “Culturally Justified Hate: Prevalence and Mental Health Impact of Dark Participation in Games.” Proceedings of the 57th Hawaii International Conference on System Sciences. January 2024.\nNewhouse, A. “The Threat is the Network: The Multi-Node Structure of Neo-Fascist Accelerationism.” CTC Sentinel. June 2021."
  },
  {
    "objectID": "cv.html#book-chapters",
    "href": "cv.html#book-chapters",
    "title": "Alex Newhouse",
    "section": "Book Chapters",
    "text": "Book Chapters\nNewhouse, A. and Kowert, R. “Recruitment and mobilization in digital gaming spaces.” Handbuch Gaming & Rechtsextremismus (ed. Linda Schlegel). Forthcoming.\nKowert, R. and Newhouse, A. “Digital games as cultural assets of influence.” The Sociology of Violent Extremism (ed. Suraj Lakhani and Amarnath Amarasingam). May 2025.\nNewhouse, A. and Kowert, R. “Digital games as vehicles for extremist recruitment and mobilization.” Gaming and Extremism: The Radicalization of Digital Playgrounds (ed. Linda Schlegel and Rachel Kowert). March 2024."
  },
  {
    "objectID": "cv.html#pre-prints-and-other-research-publications",
    "href": "cv.html#pre-prints-and-other-research-publications",
    "title": "Alex Newhouse",
    "section": "Pre-Prints and Other Research Publications",
    "text": "Pre-Prints and Other Research Publications\nKriner, M., Conroy, M., Newhouse, A., and Lewis, J. “Understanding Accelerationist Narratives: The Great Replacement Theory.” Global Network on Extremism and Technology. May 2022.\nKowert, R., Botehlo, A., and Newhouse, A. “Breaking the Building Blocks of Hate: A Case Study of Minecraft Servers.” Anti-Defamation League. July 2022.\nShadnia, D., Newhouse, A., Kriner, M., and Bradley, A. “Militant Accelerationism Coalitions: A Case-Study in Neo-Fascist Accelerationist Coalition-Building Online.” Center on Terrorism, Extremism, and Counterterrorism. June 2022.\nNewhouse, A. and Gunesch, N. “The Boogaloo Movement Wants to Be Seen as Anti-Racist, But It Has a White Supremacist Fringe.” Center on Terrorism, Extremism, and Counterterrorism. May 2021.\nMcGuffie, K. and Newhouse, A. “The Radicalization Risks of GPT-3 and Advanced Neural Language Models.” Arxiv pre-print. September 2020.\n[Author list truncated] “Release Strategies and the Social Impacts of Language Models.” Arxiv pre-print. August 2019.\nNewhouse, A. “From classifieds to crypto: how white supremacist groups have embraced crowdfunding.” Center on Terrorism, Extremism, and Counterterrorism. 2019."
  },
  {
    "objectID": "cv.html#editorials",
    "href": "cv.html#editorials",
    "title": "Alex Newhouse",
    "section": "Editorials",
    "text": "Editorials\nJackson, D., Conroy, M., and Newhouse, A. “Insiders’ View of the January 6th Committee’s Social Media Investigation.” Just Security. January 2023.\nNewhouse, A. “Far-right activists on social media telegraphed violence weeks in advance of the attack on the US Capitol.” The Conversation. January 2021. (35,400 views)\nNewhouse, A. “Parler is bringing together mainstream conservatives, anti-Semites and white supremacists as the social media platform attracts millions of Trump supporters.” The Conversation. November 2020, updated January 2021. (241,000 views)"
  },
  {
    "objectID": "cv.html#public-speaking-and-conferences",
    "href": "cv.html#public-speaking-and-conferences",
    "title": "Alex Newhouse",
    "section": "Public Speaking and Conferences",
    "text": "Public Speaking and Conferences\n“Partisan Polarization Shapes Visual Framing of COVID-19 in U.S. Media.” Paper Presentation. Midwest Political Science Association Annual Meeting. April 2025.\n“Elite Influence in an Online Neo-Fascist Forum.” Paper Presentation. Midwest Political Science Association Annual Meeting. April 2025.\n“Media Bias in COVID-19 Coverage.” Paper Presentation. Midwest Political Science Association Annual Meeting. April 2024.\n“Manifesting a Coup: Cross-Website Toxicity and Extremism on the Dot Win Network Prior to January 6, 2021.” Poster Session. New Frontiers in Text as Data. November 2023.\n“Mitigating Harm in Design.” Game Developers Conference. March 2023.\n“Accelerationism Research in Practice.” Terrorism and Social Media Conference. June 2022.\n“The Landscape of Extremist Behavior in Games.” Game Developers Conference. March 2022.\n“The Aesthetics of Acceleration.” Reactionary Digital Politics: Ideologies, Rhetorics, Aesthetics. June 2021."
  },
  {
    "objectID": "cv.html#media-appearances",
    "href": "cv.html#media-appearances",
    "title": "Alex Newhouse",
    "section": "Media Appearances",
    "text": "Media Appearances\nRadio/Podcast: Bloomberg, BBC, NPR’s Morning Edition, All Things Considered, and On Point.\nWritten: Washington Post, New York Times, Vice, Axios, Politico, FiveThirtyEight, Wired, ProPublica.\nVideo: Meet the Press Now, Denver7."
  },
  {
    "objectID": "cv.html#skills",
    "href": "cv.html#skills",
    "title": "Alex Newhouse",
    "section": "Skills",
    "text": "Skills\nProgramming languages: R, Python, MATLAB, Stata\nMachine learning frameworks: PyTorch, Scikit-Learn, Caret, Hugging Face Transformers\nSocial Network Analysis: igraph, statnet\nQuery languages: SQL, Splunk"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Welcome to my blog, where I share thoughts on computational social science, extremism research, technology policy, and academic life.\nSubscribe to the RSS feed to stay updated on new posts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst post\n\n\n\nresearch\n\ntechnology\n\nacademia\n\n\n\nBrief description of your post\n\n\n\n\n\nDec 15, 2024\n\n\nAlex Newhouse\n\n1 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "tutorial4.html",
    "href": "tutorial4.html",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "",
    "text": "Political science data rarely comes in perfect, analysis-ready form. Before running any statistical analyses, you’ll often need to transform your variables to make them more suitable for modeling or to better understand their underlying patterns. This tutorial will walk you through essential variable transformation techniques, focusing on why these transformations matter for political science research.\nBy the end of this tutorial, you’ll understand:\n\nHow to identify and interpret different types of distributions\nWhen and why to apply logarithmic transformations\nEssential techniques for recoding categorical variables\nBest practices for handling missing data and outliers"
  },
  {
    "objectID": "tutorial4.html#introduction",
    "href": "tutorial4.html#introduction",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "",
    "text": "Political science data rarely comes in perfect, analysis-ready form. Before running any statistical analyses, you’ll often need to transform your variables to make them more suitable for modeling or to better understand their underlying patterns. This tutorial will walk you through essential variable transformation techniques, focusing on why these transformations matter for political science research.\nBy the end of this tutorial, you’ll understand:\n\nHow to identify and interpret different types of distributions\nWhen and why to apply logarithmic transformations\nEssential techniques for recoding categorical variables\nBest practices for handling missing data and outliers"
  },
  {
    "objectID": "tutorial4.html#setting-up-loading-libraries-and-data",
    "href": "tutorial4.html#setting-up-loading-libraries-and-data",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Setting Up: Loading Libraries and Data",
    "text": "Setting Up: Loading Libraries and Data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nset.seed(1234)  # For reproducible examples\n\nFor this tutorial, we’ll work with both simulated data and a real-world example using country-level political and economic indicators.\n\n# Create sample data that mimics real political science variables\ncountries &lt;- data.frame(\n  country = paste(\"Country\", 1:100),\n  gdp_per_capita = rlnorm(100, meanlog = 8, sdlog = 1.5),  # Log-normal distribution\n  population = rlnorm(100, meanlog = 15, sdlog = 2),       # Highly skewed\n  democracy_score = rbeta(100, 2, 2) * 10,                # Bounded 0-10\n  election_turnout = rnorm(100, 65, 15),                   # Normal-ish\n  regime_type = sample(c(\"Democracy\", \"Hybrid\", \"Autocracy\"), 100,\n                      prob = c(0.4, 0.3, 0.3), replace = TRUE)\n)\n\nhead(countries)\n\n    country gdp_per_capita population democracy_score election_turnout\n1 Country 1      487.55416  7489758.2        6.349214         58.29560\n2 Country 2     4519.44222  1264975.2        6.907414         76.44765\n3 Country 3    15163.73142  3730254.4        5.522234         87.07578\n4 Country 4       88.36301  1196659.4        6.917598         71.65497\n5 Country 5     5674.21231   626561.9        5.874055         58.67417\n6 Country 6     6368.27453  4565225.9        7.069599         64.39998\n  regime_type\n1   Democracy\n2      Hybrid\n3   Autocracy\n4   Autocracy\n5   Democracy\n6   Democracy"
  },
  {
    "objectID": "tutorial4.html#part-1-understanding-distributions",
    "href": "tutorial4.html#part-1-understanding-distributions",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Part 1: Understanding Distributions",
    "text": "Part 1: Understanding Distributions\n\nWhat Do Distributions Tell Us?\nThe distribution of a variable shows us how values are spread across the range of possible outcomes. In political science, understanding distributions helps us:\n\nChoose appropriate statistical methods\nIdentify unusual cases or outliers\nMake valid comparisons across groups\nCommunicate findings effectively\n\n\n\nVisualizing Distributions\nLet’s examine the distribution of GDP per capita in our sample:\n\n# Basic histogram\ncountries %&gt;%\n  ggplot(aes(x = gdp_per_capita)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Distribution of GDP per Capita\",\n       x = \"GDP per Capita (USD)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhat do you notice? The distribution is heavily right-skewed—most countries cluster at lower GDP levels, with a few very wealthy countries creating a long right tail.\n\n\nTypes of Distributions in Political Science\nNormal Distribution: Symmetric, bell-shaped curve. Many statistical tests assume normality.\n\n# Election turnout - closer to normal\ncountries %&gt;%\n  ggplot(aes(x = election_turnout)) +\n  geom_histogram(bins = 15, fill = \"darkgreen\", alpha = 0.7) +\n  labs(title = \"Distribution of Election Turnout\",\n       x = \"Turnout (%)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nSkewed Distributions: Common with economic variables, population sizes, conflict casualties.\n\n# Population - highly right-skewed\ncountries %&gt;%\n  ggplot(aes(x = population)) +\n  geom_histogram(bins = 20, fill = \"coral\", alpha = 0.7) +\n  labs(title = \"Distribution of Population\",\n       x = \"Population\",\n       y = \"Count\") +\n  scale_x_continuous(labels = label_scientific())"
  },
  {
    "objectID": "tutorial4.html#part-2-the-power-of-logarithmic-transformations",
    "href": "tutorial4.html#part-2-the-power-of-logarithmic-transformations",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Part 2: The Power of Logarithmic Transformations",
    "text": "Part 2: The Power of Logarithmic Transformations\n\nWhy Log Transformations Matter\nLogarithmic transformations are crucial in political science because they:\n\nReduce skewness in right-skewed distributions\nStabilize variance across different scales\nMake relationships linear that are otherwise exponential\nAllow meaningful interpretation of percentage changes\n\n\n\nWhen to Use Log Transformations\nUse log transformations when:\n\nVariables span several orders of magnitude (GDP, population, military spending)\nYou observe exponential relationships\nYou want to interpret effects as percentage changes\nThe variable has a long right tail\n\n\n\nApplying Log Transformations\n\n# Add log-transformed variables\ncountries &lt;- countries %&gt;%\n  mutate(\n    log_gdp = log(gdp_per_capita),\n    log_population = log(population)\n  )\n\n\n\nComparing Original vs. Log-Transformed\n\n# Create side-by-side comparison\np1 &lt;- countries %&gt;%\n  ggplot(aes(x = gdp_per_capita)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Original GDP per Capita\", x = \"GDP per Capita\") +\n  theme_minimal()\n\np2 &lt;- countries %&gt;%\n  ggplot(aes(x = log_gdp)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Log GDP per Capita\", x = \"Log(GDP per Capita)\") +\n  theme_minimal()\n\n# Display plots side by side (you might need gridExtra package)\n# grid.arrange(p1, p2, ncol = 2)\n\nKey Insight: The log transformation converts the right-skewed distribution into something much closer to normal!\n\n\nInterpreting Log-Transformed Variables\nWhen you use log-transformed variables in regression:\n\nA 1-unit change in log(X) represents a 100% increase in X\nA 0.1-unit change in log(X) represents approximately a 10% increase in X\nThis makes economic interpretations much more intuitive\n\n\n# Example: How does log GDP relate to democracy scores?\ncountries %&gt;%\n  ggplot(aes(x = log_gdp, y = democracy_score)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(title = \"Relationship between Log GDP and Democracy\",\n       x = \"Log(GDP per Capita)\",\n       y = \"Democracy Score\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "tutorial4.html#part-3-recoding-variables",
    "href": "tutorial4.html#part-3-recoding-variables",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Part 3: Recoding Variables",
    "text": "Part 3: Recoding Variables\n\nWhy Recode Variables?\nRecoding involves changing how variables are categorized or valued. Common reasons:\n\nSimplifying analysis: Converting continuous variables to categories\nFixing data problems: Standardizing inconsistent coding\nCreating meaningful groups: Collapsing small categories\nHandling missing data: Deciding how to treat different types of missingness\n\n\n\nCreating Categorical Variables from Continuous Ones\n\n# Create GDP categories\ncountries &lt;- countries %&gt;%\n  mutate(\n    gdp_category = case_when(\n      gdp_per_capita &lt; 5000 ~ \"Low Income\",\n      gdp_per_capita &lt; 20000 ~ \"Middle Income\",\n      gdp_per_capita &gt;= 20000 ~ \"High Income\"\n    ),\n    # Alternative using quantiles\n    gdp_tertile = case_when(\n      gdp_per_capita &lt;= quantile(gdp_per_capita, 0.33) ~ \"Bottom Third\",\n      gdp_per_capita &lt;= quantile(gdp_per_capita, 0.67) ~ \"Middle Third\",\n      TRUE ~ \"Top Third\"\n    )\n  )\n\n# Check the distribution\ntable(countries$gdp_category)\n\n\n  High Income    Low Income Middle Income \n           11            72            17 \n\n\n\n\nRecoding Categorical Variables\n\n# Sometimes you need to collapse categories\ncountries &lt;- countries %&gt;%\n  mutate(\n    simple_regime = case_when(\n      regime_type == \"Democracy\" ~ \"Democratic\",\n      regime_type %in% c(\"Hybrid\", \"Autocracy\") ~ \"Non-Democratic\"\n    )\n  )\n\ntable(countries$simple_regime)\n\n\n    Democratic Non-Democratic \n            43             57 \n\n\n\n\nCreating Dummy Variables\nFor regression analysis, you often need to convert categorical variables into numeric dummy variables:\n\n# Create dummy variables for regime types\ncountries &lt;- countries %&gt;%\n  mutate(\n    is_democracy = ifelse(regime_type == \"Democracy\", 1, 0),\n    is_hybrid = ifelse(regime_type == \"Hybrid\", 1, 0),\n    is_autocracy = ifelse(regime_type == \"Autocracy\", 1, 0)\n  )\n\n# Check correlations (should be negative - if one is 1, others are 0)\ncor(countries[c(\"is_democracy\", \"is_hybrid\", \"is_autocracy\")])\n\n             is_democracy  is_hybrid is_autocracy\nis_democracy    1.0000000 -0.5416386   -0.5550941\nis_hybrid      -0.5416386  1.0000000   -0.3985498\nis_autocracy   -0.5550941 -0.3985498    1.0000000"
  },
  {
    "objectID": "tutorial4.html#part-4-handling-missing-data-and-outliers",
    "href": "tutorial4.html#part-4-handling-missing-data-and-outliers",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Part 4: Handling Missing Data and Outliers",
    "text": "Part 4: Handling Missing Data and Outliers\n\nIdentifying Missing Data Patterns\n\n# Introduce some missing data for demonstration\ncountries_with_missing &lt;- countries %&gt;%\n  mutate(\n    # Randomly assign some missing values\n    democracy_score = ifelse(runif(n()) &lt; 0.1, NA, democracy_score),\n    election_turnout = ifelse(runif(n()) &lt; 0.05, NA, election_turnout)\n  )\n\n# Check missing data patterns\nsummary(countries_with_missing)\n\n   country          gdp_per_capita        population        democracy_score \n Length:100         Min.   :    88.36   Min.   :1.081e+04   Min.   :0.9347  \n Class :character   1st Qu.:   778.29   1st Qu.:1.071e+06   1st Qu.:3.9636  \n Mode  :character   Median :  1674.76   Median :3.491e+06   Median :5.7426  \n                    Mean   :  8739.23   Mean   :3.223e+07   Mean   :5.3331  \n                    3rd Qu.:  6046.71   3rd Qu.:1.147e+07   3rd Qu.:6.8295  \n                    Max.   :136419.04   Max.   :1.439e+09   Max.   :9.1644  \n                                                            NA's   :11      \n election_turnout regime_type           log_gdp       log_population  \n Min.   :14.06    Length:100         Min.   : 4.481   Min.   : 9.288  \n 1st Qu.:55.15    Class :character   1st Qu.: 6.657   1st Qu.:13.881  \n Median :64.25    Mode  :character   Median : 7.423   Median :15.066  \n Mean   :63.95                       Mean   : 7.765   Mean   :15.082  \n 3rd Qu.:73.24                       3rd Qu.: 8.707   3rd Qu.:16.255  \n Max.   :98.78                       Max.   :11.823   Max.   :21.088  \n NA's   :8                                                            \n gdp_category       gdp_tertile        simple_regime       is_democracy \n Length:100         Length:100         Length:100         Min.   :0.00  \n Class :character   Class :character   Class :character   1st Qu.:0.00  \n Mode  :character   Mode  :character   Mode  :character   Median :0.00  \n                                                          Mean   :0.43  \n                                                          3rd Qu.:1.00  \n                                                          Max.   :1.00  \n                                                                        \n   is_hybrid     is_autocracy \n Min.   :0.00   Min.   :0.00  \n 1st Qu.:0.00   1st Qu.:0.00  \n Median :0.00   Median :0.00  \n Mean   :0.28   Mean   :0.29  \n 3rd Qu.:1.00   3rd Qu.:1.00  \n Max.   :1.00   Max.   :1.00  \n                              \n\n\n\n\nIdentifying Outliers\n\n# Box plot to identify outliers\ncountries %&gt;%\n  ggplot(aes(y = gdp_per_capita)) +\n  geom_boxplot() +\n  labs(title = \"GDP per Capita - Identifying Outliers\",\n       y = \"GDP per Capita\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Statistical approach: values beyond 1.5 * IQR\noutlier_threshold &lt;- quantile(countries$gdp_per_capita, 0.75) +\n                     1.5 * IQR(countries$gdp_per_capita)\n\ncountries %&gt;%\n  filter(gdp_per_capita &gt; outlier_threshold) %&gt;%\n  select(country, gdp_per_capita)\n\n       country gdp_per_capita\n1    Country 3       15163.73\n2   Country 20      111720.20\n3   Country 31       15575.37\n4   Country 41       26219.13\n5   Country 57       35303.11\n6   Country 59       33152.20\n7   Country 62      136419.04\n8   Country 66       42857.10\n9   Country 68       23196.13\n10  Country 69       21902.31\n11  Country 75       66529.87\n12  Country 93       38520.51\n13 Country 100       71802.58\n\n\n\n\nHandling Outliers\n\n# Option 1: Remove outliers (use cautiously!)\ncountries_no_outliers &lt;- countries %&gt;%\n  filter(gdp_per_capita &lt;= outlier_threshold)\n\n# Option 2: Winsorize (cap at certain percentiles)\ncountries_winsorized &lt;- countries %&gt;%\n  mutate(\n    gdp_winsorized = case_when(\n      gdp_per_capita &gt; quantile(gdp_per_capita, 0.95) ~ quantile(gdp_per_capita, 0.95),\n      gdp_per_capita &lt; quantile(gdp_per_capita, 0.05) ~ quantile(gdp_per_capita, 0.05),\n      TRUE ~ gdp_per_capita\n    )\n  )"
  },
  {
    "objectID": "tutorial4.html#part-5-best-practices-and-common-pitfalls",
    "href": "tutorial4.html#part-5-best-practices-and-common-pitfalls",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Part 5: Best Practices and Common Pitfalls",
    "text": "Part 5: Best Practices and Common Pitfalls\n\nDocumentation is Key\n\n# Always document your transformations\ncountries_final &lt;- countries %&gt;%\n  mutate(\n    # Log transformation for skewed economic variables\n    log_gdp_pc = log(gdp_per_capita),  # Natural log of GDP per capita\n    log_pop = log(population),         # Natural log of population\n\n    # Standardized democracy score (0-1 scale)\n    democracy_01 = democracy_score / 10,\n\n    # Binary regime classification\n    democratic = ifelse(regime_type == \"Democracy\", 1, 0)\n  ) %&gt;%\n  # Keep original variables for comparison\n  select(country, gdp_per_capita, log_gdp_pc, democracy_score, democracy_01,\n         regime_type, democratic, everything())\n\n\n\nCommon Mistakes to Avoid\n\nTaking logs of zero or negative values - Add a small constant if necessary\nOver-transforming - Not every skewed variable needs transformation\nLosing track of original scales - Keep both versions when possible\nMechanical outlier removal - Investigate outliers before removing them\n\n\n\nChecking Your Work\n\n# Always examine your transformations\nsummary(countries_final[c(\"gdp_per_capita\", \"log_gdp_pc\", \"democracy_score\", \"democracy_01\")])\n\n gdp_per_capita        log_gdp_pc     democracy_score   democracy_01    \n Min.   :    88.36   Min.   : 4.481   Min.   :0.9347   Min.   :0.09347  \n 1st Qu.:   778.29   1st Qu.: 6.657   1st Qu.:3.8713   1st Qu.:0.38713  \n Median :  1674.76   Median : 7.423   Median :5.7161   Median :0.57161  \n Mean   :  8739.23   Mean   : 7.765   Mean   :5.2812   Mean   :0.52812  \n 3rd Qu.:  6046.71   3rd Qu.: 8.707   3rd Qu.:6.8229   3rd Qu.:0.68229  \n Max.   :136419.04   Max.   :11.823   Max.   :9.1644   Max.   :0.91644  \n\n# Visualize relationships\ncountries_final %&gt;%\n  ggplot(aes(x = log_gdp_pc, y = democracy_01)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Log GDP vs. Standardized Democracy Score\",\n       x = \"Log(GDP per Capita)\",\n       y = \"Democracy Score (0-1 scale)\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "tutorial4.html#conclusion",
    "href": "tutorial4.html#conclusion",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Conclusion",
    "text": "Conclusion\nVariable transformations are fundamental tools in political science research. Key takeaways:\n\nUnderstand your data first - Always visualize distributions before transforming\nLog transformations are powerful for right-skewed economic/demographic variables\nThoughtful recoding can simplify analysis and improve interpretation\nDocument everything - Future you will thank present you\nKeep originals - Preserve untransformed variables for robustness checks\n\n\nNext Steps\nIn future tutorials, we’ll explore:\n\nAdvanced transformation techniques (Box-Cox, inverse hyperbolic sine)\nMultiple imputation for missing data\nRobust regression methods for outlier-prone data\nInteraction terms and polynomial transformations\n\n\n\nPractice Exercises\n\nTry log-transforming the population variable and compare the distribution\nCreate a three-category democracy variable (Low/Medium/High) using quantile cuts\nIdentify and investigate the countries with the highest democracy scores\nCreate a scatter plot matrix comparing original and transformed variables"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Newhouse",
    "section": "",
    "text": "I am a PhD candidate in Political Science at the University of Colorado Boulder, specializing in computational analysis of political violence and extremism. My research develops and applies cutting-edge methods—machine learning, natural language processing, and social network analysis—to understand how online communities translate into offline political action.\nKey Experience:\n\nInvestigative Consultant to the House Select Committee on January 6th\n\nDeputy Director of the Center on Terrorism, Extremism & Counterterrorism\nPrincipal Investigator on $1.38M in federal research grants\nResearch partnerships with major technology companies"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Alex Newhouse",
    "section": "",
    "text": "I am a PhD candidate in Political Science at the University of Colorado Boulder, specializing in computational analysis of political violence and extremism. My research develops and applies cutting-edge methods—machine learning, natural language processing, and social network analysis—to understand how online communities translate into offline political action.\nKey Experience:\n\nInvestigative Consultant to the House Select Committee on January 6th\n\nDeputy Director of the Center on Terrorism, Extremism & Counterterrorism\nPrincipal Investigator on $1.38M in federal research grants\nResearch partnerships with major technology companies"
  },
  {
    "objectID": "index.html#research-expertise",
    "href": "index.html#research-expertise",
    "title": "Alex Newhouse",
    "section": "Research Expertise",
    "text": "Research Expertise\n\n\nMethods:\n\nMachine Learning & NLP\nSocial Network Analysis\n\nCausal Inference\nData Science & Visualization\n\n\nSubstantive Areas:\n\nPolitical Violence & Extremism\nDigital Platform Research\nTechnology & Society\nComputational Social Science"
  },
  {
    "objectID": "index.html#featured-publications",
    "href": "index.html#featured-publications",
    "title": "Alex Newhouse",
    "section": "Featured Publications",
    "text": "Featured Publications\nKowert, R., Kilmer, E., and Newhouse, A. “Taking it to the Extreme: Prevalence and Nature of Extremist Sentiment in Games.” Frontiers in Psychology, 2024.\nNewhouse, A. “The Threat is the Network: The Multi-Node Structure of Neo-Fascist Accelerationism.” CTC Sentinel, 2021.\nMcGuffie, K. and Newhouse, A. “The Radicalization Risks of GPT-3 and Advanced Neural Language Models.” arXiv, 2020."
  },
  {
    "objectID": "index.html#research-impact",
    "href": "index.html#research-impact",
    "title": "Alex Newhouse",
    "section": "Research Impact",
    "text": "Research Impact\n$1.38M in grant funding (Principal Investigator) | 45+ students mentored | 276K article views | Featured in 8 major media outlets"
  },
  {
    "objectID": "index.html#media-recognition",
    "href": "index.html#media-recognition",
    "title": "Alex Newhouse",
    "section": "Media Recognition",
    "text": "Media Recognition\nResearch and expertise featured in: Washington Post • New York Times • NPR • BBC • Politico • Wired • Bloomberg • Meet the Press"
  },
  {
    "objectID": "index.html#current-projects",
    "href": "index.html#current-projects",
    "title": "Alex Newhouse",
    "section": "Current Projects",
    "text": "Current Projects\nDissertation Research: Developing computational frameworks to measure online-offline relationships in political violence\nCOVID-19 Visual Politics: Analyzing partisan differences in pandemic media coverage using computer vision and text analysis\nGaming & Extremism: Multi-platform investigation of recruitment and radicalization in digital gaming environments\n\nContact: alex.newhouse@colorado.edu"
  },
  {
    "objectID": "posts/2025-09-04-first-post/index.html",
    "href": "posts/2025-09-04-first-post/index.html",
    "title": "First post",
    "section": "",
    "text": "Testing blog!"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching & Mentorship",
    "section": "",
    "text": "My approach centers on active learning, methodological rigor, and practical application—helping students develop both theoretical understanding and technical skills they can apply to pressing societal challenges.\nI believe in making complex political science concepts accessible through hands-on learning and real-world applications, particularly in quantitative methods and comparative politics."
  },
  {
    "objectID": "teaching.html#teaching-philosophy",
    "href": "teaching.html#teaching-philosophy",
    "title": "Teaching & Mentorship",
    "section": "",
    "text": "My approach centers on active learning, methodological rigor, and practical application—helping students develop both theoretical understanding and technical skills they can apply to pressing societal challenges.\nI believe in making complex political science concepts accessible through hands-on learning and real-world applications, particularly in quantitative methods and comparative politics."
  },
  {
    "objectID": "teaching.html#teaching-impact",
    "href": "teaching.html#teaching-impact",
    "title": "Teaching & Mentorship",
    "section": "Teaching Impact",
    "text": "Teaching Impact\n45+ students mentored | 15 independent projects supervised | 7 courses taught | 3 institutions"
  },
  {
    "objectID": "teaching.html#courses-taught",
    "href": "teaching.html#courses-taught",
    "title": "Teaching & Mentorship",
    "section": "Courses Taught",
    "text": "Courses Taught\n\nUniversity of Colorado Boulder\nTeaching Assistant\n\nPSCI 2075: Quantitative Methods (Fall 2025, Spring 2024)\n\nComprehensive introduction to statistical analysis in political science, covering descriptive statistics, hypothesis testing, regression analysis, and research design. Emphasized practical application using R and real political datasets.\n\nPSCI 1101: Introduction to Western Political Thought (Spring 2025)\n\nSurvey of foundational political philosophy from ancient Greece through Enlightenment thinkers, connecting classical ideas to contemporary political challenges.\n\nPSCI 2223: Introduction to Comparative Politics (Fall 2024)\n\nExploration of political systems, institutions, and processes across different countries and regions, with emphasis on democratization, authoritarianism, and political development.\n\nPSCI 1824: Introduction to International Relations (Fall 2023)\n\nComprehensive overview of international relations theory and practice, covering security studies, international political economy, and global governance challenges.\n\n\n\n\nMiddlebury Institute of International Studies\nAdjunct Professor\n\nDigital Extremism (Spring 2023)\n\nSelf-designed course examining the intersection of technology and political violence. Covered online radicalization processes, platform dynamics, and content moderation strategies. Students conducted original research on extremist communities across digital platforms.\n\n\n\n\nMiddlebury College\nAdjunct Professor\n\nOnline Extremism (January 2021)\n\nIntensive winter term course on digital radicalization and online political violence. Integrated computational methods with political science theory, providing students hands-on experience with social media data analysis."
  },
  {
    "objectID": "teaching.html#teaching-resources",
    "href": "teaching.html#teaching-resources",
    "title": "Teaching & Mentorship",
    "section": "Teaching Resources",
    "text": "Teaching Resources\n\nQuantitative Methods Materials\n\nTutorial 1: Getting Started with Greek Notation and Descriptive Stats\nTutorial 2: An Introduction to R’s pipe operator\nTutorial 4: Variable Transformations in R - Understanding distributions, log transforms, and data recoding\nTutorial 5: Using AI Chatbots to Learn R Programming - Effective strategies for LLM-assisted coding and learning"
  },
  {
    "objectID": "teaching.html#student-research-mentorship",
    "href": "teaching.html#student-research-mentorship",
    "title": "Teaching & Mentorship",
    "section": "Student Research Mentorship",
    "text": "Student Research Mentorship\nI have supervised 45 undergraduate and graduate research assistants and 15 independent projects since 2019. My mentorship philosophy emphasizes methodological rigor, ethical research practices, and practical skill development.\n\nCurrent Projects\n\nApocalyptic Language Analysis: Four undergraduate RAs working on qualitative coding for social media research\n\n\n\nPast Student Projects\n\n\n\nItalian Neofascism & Political Violence\nMilitant Accelerationism & Coalition-Building\n\nFrench Far-Right on Encrypted Platforms\n\n\n\nAI Implications for Domestic Counterterrorism\nChristian Identity Movements\nGaming Platform Radicalization"
  },
  {
    "objectID": "teaching.html#research-teaching-integration",
    "href": "teaching.html#research-teaching-integration",
    "title": "Teaching & Mentorship",
    "section": "Research-Teaching Integration",
    "text": "Research-Teaching Integration\nMy courses integrate cutting-edge research methods with substantive political science questions. Students learn to:\n\nApply computational methods to real political phenomena using Python, R, and specialized software\nCritically evaluate digital media, online communities, and algorithmic systems\n\nNavigate ethical challenges of studying sensitive political topics\nConnect theoretical frameworks to empirical analysis through hands-on projects\nDevelop professional skills in data visualization, statistical analysis, and research communication\n\nSignature Approach: Each course includes substantial research components where students collect and analyze original data, preparing them for graduate study or professional research roles.\n\nInterested in collaboration or guest lectures? Contact alex.newhouse@colorado.edu"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My research program sits at the intersection of computational social science, political violence, and technology studies. I develop and apply cutting-edge computational methods—machine learning, natural language processing, and social network analysis—to understand how online communities translate into offline political action.\nCore Questions:\n\nHow do digital platforms facilitate political radicalization and mobilization?\nWhat role do emerging technologies play in extremist recruitment and organization?\n\nHow can computational methods help us understand and counter political violence?"
  },
  {
    "objectID": "research.html#research-overview",
    "href": "research.html#research-overview",
    "title": "Research",
    "section": "",
    "text": "My research program sits at the intersection of computational social science, political violence, and technology studies. I develop and apply cutting-edge computational methods—machine learning, natural language processing, and social network analysis—to understand how online communities translate into offline political action.\nCore Questions:\n\nHow do digital platforms facilitate political radicalization and mobilization?\nWhat role do emerging technologies play in extremist recruitment and organization?\n\nHow can computational methods help us understand and counter political violence?"
  },
  {
    "objectID": "research.html#citation-metrics",
    "href": "research.html#citation-metrics",
    "title": "Research",
    "section": "Citation Metrics",
    "text": "Citation Metrics\nGoogle Scholar Profile: 1,093 total citations | Research Areas: collective behavior, online communities, extremism, conspiracy theories, video games\nImpact Highlights:\n\nFeatured in 8+ major media outlets\n276K+ total article views\nResearch cited in policy reports and academic literature"
  },
  {
    "objectID": "research.html#featured-publications",
    "href": "research.html#featured-publications",
    "title": "Research",
    "section": "Featured Publications",
    "text": "Featured Publications\n\nPeer-Reviewed Articles\nKowert, R., Kilmer, E., and Newhouse, A. (2024). “Taking it to the Extreme: Prevalence and Nature of Extremist Sentiment in Games.” Frontiers in Psychology, 15:1410620.\nThis study addresses the gap in research knowledge around extremist sentiment in games by evaluating prevalence, location, nature, and impact through an online survey of 423 game players, revealing alarmingly high rates of extremist content exposure and normalization within gaming cultures.\n\nFunded by: Department of Homeland Security (DHS # EMW-2022-GR-00036)\nKey Finding: More than half of all game players report experiencing some form of hate, harassment or abuse within gaming spaces, with extremist rhetoric becoming culturally normalized\n\nNewhouse, A. (2021). “The Threat is the Network: The Multi-Node Structure of Neo-Fascist Accelerationism.” CTC Sentinel, 14(5).\nSince 2015, the Atomwaffen Division has received bulk academic and media attention in coverage of the neo-fascist accelerationist movement, but evidence reveals it was not the apex of a hierarchy but rather one node in a larger network of violent accelerationists built on membership fluidity, frequent communications, and a shared goal of social destruction.\n\nInnovation: Novel network-based framework for understanding extremist movements\nPolicy Impact: Enforcement against individuals and groups is necessary but not sufficient—focus on specific groups may not tackle the root of the issue\nCitations: Extensively cited in terrorism studies and cited in recent Studies in Conflict & Terrorism article\n\nMcGuffie, K. and Newhouse, A. (2020). “The Radicalization Risks of GPT-3 and Advanced Neural Language Models.” arXiv preprint, 2009.06807.\nWe show GPT-3’s strength in generating text that accurately emulates interactive, informational, and influential content that could be utilized for radicalizing individuals into violent far-right extremist ideologies and behaviors. While OpenAI’s preventative measures are strong, the possibility of unregulated copycat technology represents significant risk for large-scale online radicalization and recruitment.\n\nImpact: 28 citations and featured in major tech policy discussions\nMedia Coverage: Extensive coverage in The Register, Vice, and other tech outlets\nPolicy Influence: Already in 2020, Kris McGuffie and Alex Newhouse highlighted the potential for abuse of generative language models by assessing GPT-3, revealing significant risk for large-scale online radicalization and recruitment\n\n\n\nOther Major Publications\nNewhouse, A. (2020). “Far-right activists on social media telegraphed violence weeks in advance of the attack on the US Capitol.” The Conversation.\nNewhouse, A. (2020). “Parler is bringing together mainstream conservatives, anti-Semites and white supremacists as the social media platform attracts millions of Trump supporters.” The Conversation.\n\nImpact: 241,000 views for Parler analysis\n\nMultiple GNET Research Reports (2021-2022) on accelerationist movements and extremist narratives, including analysis of the Great Replacement theory and Boogaloo movement."
  },
  {
    "objectID": "research.html#research-areas-methods",
    "href": "research.html#research-areas-methods",
    "title": "Research",
    "section": "Research Areas & Methods",
    "text": "Research Areas & Methods\n\nSubstantive Focus Areas\nPolitical Violence & Extremism\n\nNeo-fascist accelerationism and network structures\nOnline-offline radicalization pathways\n\nGaming platforms as extremist recruitment venues\nAI and synthetic media threats\n\nTechnology & Society\n\nDigital platform governance and content moderation\nAI safety and misuse potential\nSocial media data analysis and behavioral patterns\nGaming culture and political socialization\n\n\n\nMethodological Expertise\nComputational Methods\n\nMachine Learning (PyTorch, Scikit-Learn, Hugging Face Transformers)\nNatural Language Processing and text-as-data approaches\nSocial Network Analysis (igraph, statnet)\nTime-series analysis and causal inference\n\nFeatured Technical Work:\n\nDistilBERT for Political Text Classification - Automated detection of extremist content using transformer models (94% F1-score)\n\nData & Platforms\n\nLarge-scale social media data collection and analysis\nGaming platform research and mixed-methods approaches\nSurvey research and experimental design\nQualitative coding and content analysis"
  },
  {
    "objectID": "research.html#current-research-projects",
    "href": "research.html#current-research-projects",
    "title": "Research",
    "section": "Current Research Projects",
    "text": "Current Research Projects\n\nDissertation Research\n“Measuring Online-Offline Relationships in Political Violence”\nDeveloping computational frameworks to understand how digital engagement translates into offline political action. Uses machine learning, natural language processing, social network analysis, and time-series causal inference to examine extremist community dynamics.\n\nCommittee: Andrew Q. Philips (co-chair), Jennifer Fitzgerald (co-chair), Alexandra Siegel\nExternal Advisor: Brian Keegan (Information Science)\nExpected Completion: May 2027\n\n\n\nCOVID-19 Media Framing Study\n“Partisan Visual Politics During a Pandemic” (with Andrew Q. Philips and Komal P. Kaur)\nInvestigating partisan differences in visual and textual framing of pandemic coverage across U.S. news outlets using computer vision and text analysis methods.\n\n\nGaming & Extremism Research\n“Multi-Platform Radicalization in Digital Gaming”\nComprehensive study of recruitment and radicalization processes within digital gaming environments, including direct partnerships with major gaming platforms for harm mitigation strategies.\n\n\nPolicy & Financial Markets\n“Political Messaging and Market Responses” (under review)\nAnalysis of how political communications affect financial market behavior and investor decision-making."
  },
  {
    "objectID": "research.html#research-impact-media-coverage",
    "href": "research.html#research-impact-media-coverage",
    "title": "Research",
    "section": "Research Impact & Media Coverage",
    "text": "Research Impact & Media Coverage\n\nAcademic Recognition\n\nResearch extensively cited in terrorism studies literature\nWork featured in CTC Sentinel, premier counterterrorism publication\nPublications in top-tier psychology and computer science venues\n\n\n\nPolicy & Industry Impact\n\nJanuary 6th Committee: Investigative consultant providing expertise on extremist movements\nTechnology Partnerships: Direct collaboration with gaming companies (Roblox, Spectrum Labs)\nFederal Funding: $1.38M in grants as Principal Investigator\n\n\n\nMedia & Public Engagement\nMajor Media Features: Washington Post • New York Times • NPR (Morning Edition, All Things Considered) • BBC • Politico • Wired • Bloomberg Radio • Meet the Press Now\nHigh-Impact Articles:\n\nParler analysis: 241,000 views\nJanuary 6th prediction: 35,400 views\n\nExpert Commentary Topics:\n\nAI safety and misuse potential\nGaming platform moderation\nExtremist recruitment strategies\n\nSocial media radicalization\nTechnology policy implications"
  },
  {
    "objectID": "research.html#funding-grants",
    "href": "research.html#funding-grants",
    "title": "Research",
    "section": "Funding & Grants",
    "text": "Funding & Grants\n\nAs Principal Investigator\n\nDepartment of Homeland Security (2022-2024): Gaming and extremism research - $350,000\nMultiple Federal Grants (2019-2024): Extremism and technology studies - $1.38M total\n\n\n\nAs Co-Investigator\n\nLogically Partnership (2022): Social media data infrastructure for extremism research\nVarious Industry Partnerships: Gaming platform safety research"
  },
  {
    "objectID": "research.html#future-research-directions",
    "href": "research.html#future-research-directions",
    "title": "Research",
    "section": "Future Research Directions",
    "text": "Future Research Directions\nEmerging Technologies & Political Violence\n\nAI-generated disinformation and radicalization\nVirtual/augmented reality environments as political spaces\nBlockchain and decentralized platform governance\n\nComparative Extremism Studies\n\nCross-national analysis of digital radicalization patterns\nComparative platform governance approaches\nInternational cooperation in countering online extremism\n\nMethodological Innovation\n\nAdvanced causal inference methods for social media data\nMulti-modal analysis combining text, visual, and network data\nReal-time detection and intervention systems\n\n\nFor collaboration inquiries, media requests, or access to datasets, contact alex.newhouse@colorado.edu.\nGoogle Scholar: Alex Newhouse • ORCID: Available upon request"
  },
  {
    "objectID": "tutorial2-pipe.html",
    "href": "tutorial2-pipe.html",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "",
    "text": "When you first start learning R, your code might look something like this:\n\nresult &lt;- function3(function2(function1(data, arg1), arg2), arg3)\n\nThis nested approach works, but it’s hard to read and understand. What if there was a way to make your code read more like a sentence, flowing from left to right? Enter the pipe operator.\nThe pipe operator allows you to chain functions together in a way that’s intuitive and readable. Instead of nesting functions inside each other, you can “pipe” the output of one function directly into the next function as input."
  },
  {
    "objectID": "tutorial2-pipe.html#introduction",
    "href": "tutorial2-pipe.html#introduction",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "",
    "text": "When you first start learning R, your code might look something like this:\n\nresult &lt;- function3(function2(function1(data, arg1), arg2), arg3)\n\nThis nested approach works, but it’s hard to read and understand. What if there was a way to make your code read more like a sentence, flowing from left to right? Enter the pipe operator.\nThe pipe operator allows you to chain functions together in a way that’s intuitive and readable. Instead of nesting functions inside each other, you can “pipe” the output of one function directly into the next function as input."
  },
  {
    "objectID": "tutorial2-pipe.html#what-is-the-pipe-operator",
    "href": "tutorial2-pipe.html#what-is-the-pipe-operator",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "What is the Pipe Operator?",
    "text": "What is the Pipe Operator?\nThe pipe operator takes the result from the expression on its left side and passes it as the first argument to the function on its right side. Think of it like a literal pipe in plumbing—data flows through it from one function to the next.\nIn R, there are two main pipe operators you’ll encounter:\n\nNative pipe |&gt; (introduced in R 4.1.0, 2021)\nMagrittr pipe %&gt;% (from the magrittr package, popularized by tidyverse)"
  },
  {
    "objectID": "tutorial2-pipe.html#the-native-pipe",
    "href": "tutorial2-pipe.html#the-native-pipe",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "The Native Pipe |>",
    "text": "The Native Pipe |&gt;\nThe native pipe |&gt; is built directly into R (no packages required). Here’s how it works:\n\nBasic Syntax\n\ndata |&gt; function1() |&gt; function2() |&gt; function3()\n\nThis is equivalent to:\n\nfunction3(function2(function1(data)))\n\n\n\nSimple Example\nLet’s say you want to: 1. Take a vector of numbers 2. Calculate the square root of each 3. Round to 2 decimal places 4. Calculate the mean\nWithout pipes:\n\nnumbers &lt;- c(4, 9, 16, 25, 36)\nresult &lt;- mean(round(sqrt(numbers), 2))\nresult\n\n[1] 4\n\n\nWith the native pipe:\n\nnumbers &lt;- c(4, 9, 16, 25, 36)\nresult &lt;- numbers |&gt; \n  sqrt() |&gt; \n  round(2) |&gt; \n  mean()\nresult\n\n[1] 4\n\n\nMuch more readable! You can follow the data flow from left to right, top to bottom."
  },
  {
    "objectID": "tutorial2-pipe.html#the-magrittr-pipe",
    "href": "tutorial2-pipe.html#the-magrittr-pipe",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "The Magrittr Pipe %>%",
    "text": "The Magrittr Pipe %&gt;%\nThe %&gt;% pipe comes from the magrittr package and is widely used in the tidyverse ecosystem (dplyr, ggplot2, etc.). It works very similarly to the native pipe but has some additional features.\n\nLoading the Package\n\nlibrary(magrittr)  # For standalone use\n# OR\nlibrary(dplyr)     # Automatically loads %&gt;%\n# OR  \nlibrary(tidyverse) # Loads entire tidyverse, including %&gt;%\n\n\n\nBasic Usage\nThe same example with %&gt;%:\n\nlibrary(magrittr)\nnumbers &lt;- c(4, 9, 16, 25, 36)\nresult &lt;- numbers %&gt;% \n  sqrt() %&gt;% \n  round(2) %&gt;% \n  mean()\nresult"
  },
  {
    "objectID": "tutorial2-pipe.html#practical-data-analysis-examples",
    "href": "tutorial2-pipe.html#practical-data-analysis-examples",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Practical Data Analysis Examples",
    "text": "Practical Data Analysis Examples\nLet’s look at more realistic examples using a dataset. We’ll use the built-in mtcars dataset.\n\nExample 1: Data Summarization\nTask: Find the average miles per gallon (mpg) for cars with more than 4 cylinders, rounded to 1 decimal place.\nWithout pipes:\n\nresult &lt;- round(mean(mtcars[mtcars$cyl &gt; 4, \"mpg\"]), 1)\nresult\n\n[1] 16.6\n\n\nWith native pipe:\n\nresult &lt;- mtcars |&gt; \n  subset(cyl &gt; 4) |&gt; \n  subset(select = mpg) |&gt; \n  unlist() |&gt; \n  mean() |&gt; \n  round(1)\nresult\n\n[1] 16.6\n\n\n\n\nExample 2: Using with dplyr\nIf you’re using dplyr (part of tidyverse), pipes become even more powerful:\n\nlibrary(dplyr)\n\n# Find the 3 most fuel-efficient cars by transmission type\nmtcars |&gt; \n  group_by(am) |&gt; \n  arrange(desc(mpg)) |&gt; \n  slice_head(n = 3) |&gt; \n  select(mpg, am, cyl)"
  },
  {
    "objectID": "tutorial2-pipe.html#key-differences-between-and",
    "href": "tutorial2-pipe.html#key-differences-between-and",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Key Differences Between |> and %>%",
    "text": "Key Differences Between |&gt; and %&gt;%\nWhile both pipes work similarly for basic operations, there are some important differences:\n\n1. Availability\n\n|&gt; is built into R 4.1.0+ (no packages needed)\n%&gt;% requires the magrittr package or tidyverse\n\n\n\n2. Placeholder Usage\nMagrittr pipe %&gt;% with placeholder:\n\n# When you need the piped value in a position other than first argument\ndata %&gt;% \n  lm(y ~ x, data = .)  # The dot (.) represents the piped data\n\nNative pipe |&gt; with placeholder:\n\n# R 4.2.0+ syntax\ndata |&gt; \n  lm(y ~ x, data = _)  # Underscore (_) as placeholder\n\n# Alternative for all R 4.1.0+ versions\ndata |&gt; \n  (\\(x) lm(y ~ z, data = x))()  # Anonymous function\n\n\n\n3. Performance\nThe native pipe |&gt; is slightly faster since it’s built into R’s core."
  },
  {
    "objectID": "tutorial2-pipe.html#when-to-use-which-pipe",
    "href": "tutorial2-pipe.html#when-to-use-which-pipe",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "When to Use Which Pipe?",
    "text": "When to Use Which Pipe?\nUse the native pipe |&gt; when:\n\nYou’re using R 4.1.0 or later\nYou want to minimize package dependencies\nYou’re doing straightforward piping (most common case)\nPerformance is critical\n\nUse the magrittr pipe %&gt;% when:\n\nYou’re working with tidyverse packages\nYou need advanced features like the dot placeholder\nYou’re working with legacy code that uses %&gt;%\nYou’re collaborating with others who use tidyverse"
  },
  {
    "objectID": "tutorial2-pipe.html#best-practices",
    "href": "tutorial2-pipe.html#best-practices",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Format for Readability\n\n# Good: One function per line, properly indented\ndata |&gt; \n  filter(condition) |&gt; \n  group_by(variable) |&gt; \n  summarise(mean_value = mean(value))\n\n# Avoid: Everything on one line (hard to read)\ndata |&gt; filter(condition) |&gt; group_by(variable) |&gt; summarise(mean_value = mean(value))\n\n\n\n2. Don’t Overuse Pipes\n\n# Sometimes simple assignment is clearer\nx &lt;- mean(data$variable)\n\n# Instead of\nx &lt;- data |&gt; pull(variable) |&gt; mean()\n\n\n\n3. Break Long Chains\nIf your pipe chain gets very long (&gt;10 steps), consider breaking it into smaller chunks with intermediate variables."
  },
  {
    "objectID": "tutorial2-pipe.html#common-errors-and-solutions",
    "href": "tutorial2-pipe.html#common-errors-and-solutions",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Common Errors and Solutions",
    "text": "Common Errors and Solutions\n\nError: Object not found\n\n# This won't work - data isn't defined\ndata |&gt; mean()\n\n# Solution: Make sure your starting object exists\nmy_data &lt;- c(1, 2, 3, 4, 5)\nmy_data |&gt; mean()"
  },
  {
    "objectID": "tutorial2-pipe.html#conclusion",
    "href": "tutorial2-pipe.html#conclusion",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Conclusion",
    "text": "Conclusion\nThe pipe operator is a powerful tool that makes R code more readable and intuitive. Whether you use the native pipe |&gt; or the magrittr pipe %&gt;%, the key is consistency within your projects.\nStart incorporating pipes into your workflow gradually—begin with simple chains and work your way up to more complex data manipulation tasks. Your future self (and your collaborators) will thank you for the cleaner, more readable code!"
  },
  {
    "objectID": "tutorial2-pipe.html#additional-resources",
    "href": "tutorial2-pipe.html#additional-resources",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nR for Data Science - Pipes chapter\nmagrittr documentation\nR 4.1.0 release notes"
  },
  {
    "objectID": "tutorial5.html",
    "href": "tutorial5.html",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "",
    "text": "Large Language Models (LLMs) like ChatGPT, Claude, and others have revolutionized how we approach coding and data analysis. When used properly, these tools can be powerful learning aids that help you understand R programming concepts, debug code, and solve analytical problems. However, blindly copying and pasting code without understanding will not help you learn and may lead to incorrect analyses.\nThis tutorial will teach you how to effectively collaborate with AI chatbots to enhance your learning while maintaining academic integrity and developing genuine programming skills."
  },
  {
    "objectID": "tutorial5.html#introduction",
    "href": "tutorial5.html#introduction",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "",
    "text": "Large Language Models (LLMs) like ChatGPT, Claude, and others have revolutionized how we approach coding and data analysis. When used properly, these tools can be powerful learning aids that help you understand R programming concepts, debug code, and solve analytical problems. However, blindly copying and pasting code without understanding will not help you learn and may lead to incorrect analyses.\nThis tutorial will teach you how to effectively collaborate with AI chatbots to enhance your learning while maintaining academic integrity and developing genuine programming skills."
  },
  {
    "objectID": "tutorial5.html#learning-objectives",
    "href": "tutorial5.html#learning-objectives",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this tutorial, you’ll be able to:\n\nFormulate effective prompts that get you helpful, educational responses\nCritically evaluate AI-generated code before using it\nLearn from AI explanations rather than just copying code\nDebug and modify AI-generated solutions\nRecognize when AI advice is wrong and how to verify it"
  },
  {
    "objectID": "tutorial5.html#understanding-what-llms-can-and-cannot-do",
    "href": "tutorial5.html#understanding-what-llms-can-and-cannot-do",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Understanding What LLMs Can and Cannot Do",
    "text": "Understanding What LLMs Can and Cannot Do\n\nWhat LLMs Excel At\n✅ Code Explanation - Breaking down complex R syntax - Explaining what specific functions do - Clarifying statistical concepts\n✅ Debugging Help - Identifying syntax errors - Suggesting fixes for common problems - Explaining error messages\n✅ Alternative Approaches - Showing different ways to solve the same problem - Suggesting more efficient code - Introducing new functions or packages\n✅ Learning Support - Providing step-by-step explanations - Creating practice examples - Answering conceptual questions\n\n\nWhat LLMs Struggle With\n❌ Domain-Specific Context - Understanding your specific assignment requirements - Knowing your dataset’s unique characteristics - Applying course-specific conventions\n❌ Current Information - Latest R package updates - Recent statistical methods - Current best practices (beyond training data)\n❌ Complex Statistical Reasoning - Choosing appropriate statistical tests - Interpreting results in context - Making methodological decisions\n❌ Academic Integrity - Understanding your course’s specific policies - Knowing what constitutes your own work - Balancing assistance with independent learning"
  },
  {
    "objectID": "tutorial5.html#crafting-effective-prompts",
    "href": "tutorial5.html#crafting-effective-prompts",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Crafting Effective Prompts",
    "text": "Crafting Effective Prompts\n\nThe CLEAR Framework\nUse this framework to structure your prompts:\nContext - Provide relevant background Level - Specify your experience level Example - Include sample data or code Action - Be specific about what you want Restriction - Set boundaries on the response\n\n\nExample: Poor vs. Good Prompts\n❌ Poor Prompt:\n\"Help me with regression in R\"\n✅ Good Prompt:\n\"I'm an undergraduate student learning linear regression in R. I have a dataset with variables for GDP per capita (gdp_pc) and democracy scores (democracy) for 50 countries, and I need to run a regression to see if GDP predicts democracy levels.\n\nHere's a sample of my data:\ncountry | gdp_pc | democracy\n--------|--------|----------\nUSA     | 65000  | 8.5\nGermany | 48000  | 9.1\n\nCan you show me how to:\n1. Run a linear regression with democracy as the outcome variable\n2. Interpret the coefficients\n3. Check model assumptions\n\nPlease explain each step so I can understand the logic, and use comments in the code.\""
  },
  {
    "objectID": "tutorial5.html#working-through-a-complete-example",
    "href": "tutorial5.html#working-through-a-complete-example",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Working Through a Complete Example",
    "text": "Working Through a Complete Example\nLet’s walk through a realistic scenario where you need AI help with a data analysis assignment.\n\nScenario: Analyzing Election Turnout\nAssignment: You need to analyze factors that predict voter turnout using a dataset with variables for income, education, and age.\n\n\nStep 1: Initial Problem Setup\nYour Prompt:\nI'm working on a political science assignment analyzing voter turnout. I have a dataset called 'voter_data' with these variables:\n- turnout: whether someone voted (1 = yes, 0 = no)\n- income: household income in thousands\n- education: years of education\n- age: respondent age\n\nI need to run a logistic regression since turnout is binary. Can you show me the basic code structure and explain why we use logistic regression for binary outcomes? Please include comments explaining each step.\nAI Response Analysis:\n\n\nCode\n# Load necessary library\nlibrary(dplyr)\n\n# Run logistic regression\nmodel &lt;- glm(turnout ~ income + education + age,\n             data = voter_data,\n             family = binomial)\n\n# View results\nsummary(model)\n\n\n✅ Good AI Response Elements: - Provides commented code - Uses appropriate function (glm with family = binomial) - Shows how to view results\n🤔 What to Verify: - Is the syntax correct for your version of R? - Do the variable names match your actual dataset? - Are there additional steps you might need?\n\n\nStep 2: Follow-Up Questions\nDon’t just copy the code! Ask follow-up questions to deepen your understanding:\nYour Follow-Up Prompts:\n1. \"Can you explain what the 'family = binomial' argument does and why it's necessary for binary outcomes?\"\n\n2. \"How do I interpret the coefficients in logistic regression? They seem different from regular regression.\"\n\n3. \"What should I look for in the summary output to determine if my model is good?\"\n\n\nStep 3: Adapting and Debugging\nWhen you try to run the AI’s code, you might encounter errors. Here’s how to handle them:\nYour Error Message:\nError in glm.fit: object 'voter_data' not found\nYour Debugging Prompt:\nI'm getting this error when trying to run the logistic regression code you provided: [paste error message]. I think it might be because I haven't loaded my dataset properly. Can you show me how to load a CSV file and check that it loaded correctly before running the regression?"
  },
  {
    "objectID": "tutorial5.html#critical-evaluation-of-ai-code",
    "href": "tutorial5.html#critical-evaluation-of-ai-code",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Critical Evaluation of AI Code",
    "text": "Critical Evaluation of AI Code\n\nAlways Ask Yourself These Questions\n\nDoes this code make sense for my problem?\n\nAre the variable names correct?\nIs the statistical method appropriate?\nDo the steps follow a logical order?\n\nCan I explain what each line does?\n\nIf not, ask the AI to explain further\nTest your understanding by modifying small parts\n\nDoes this follow best practices?\n\nAre there proper comments?\nIs the code readable and well-structured?\nAre packages loaded appropriately?\n\n\n\n\nExample: Spotting Problems\nAI-Generated Code:\n\n\nCode\n# This code has several issues - can you spot them?\nresult &lt;- lm(turnout ~ income + education, data = voter_data)\nplot(result)\nsummary(result)\n\n\nProblems to Identify: 1. Wrong function: Uses lm() instead of glm() for binary outcome 2. Missing family argument: No specification for logistic regression 3. Incomplete model: Missing the age variable from original request 4. Poor plotting: plot() on logistic regression isn’t very informative\nHow to Address:\n\"I notice this code uses lm() instead of glm(). Since my outcome variable is binary (0/1), shouldn't I be using logistic regression? Also, the original model was supposed to include age as a predictor. Can you correct this and explain the difference between linear and logistic regression for binary outcomes?\""
  },
  {
    "objectID": "tutorial5.html#learning-focused-interaction-strategies",
    "href": "tutorial5.html#learning-focused-interaction-strategies",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Learning-Focused Interaction Strategies",
    "text": "Learning-Focused Interaction Strategies\n\nStrategy 1: Request Step-by-Step Explanations\nInstead of just asking for code, ask for the reasoning:\n\"Before showing me the code, can you explain the logic of why we use logistic regression for binary outcomes? Then show me the code with comments explaining how each part implements that logic.\"\n\n\nStrategy 2: Ask for Alternative Approaches\n\"You showed me one way to check model assumptions for logistic regression. Are there other methods I could use? What are the pros and cons of each approach?\"\n\n\nStrategy 3: Request Practice Problems\n\"Can you create a similar but simpler example with made-up data so I can practice the concepts before applying them to my real dataset?\"\n\n\nStrategy 4: Seek Conceptual Clarification\n\"I understand the code, but I'm confused about interpreting odds ratios. Can you explain what an odds ratio of 1.5 means in practical terms for my voter turnout study?\""
  },
  {
    "objectID": "tutorial5.html#common-pitfalls-and-how-to-avoid-them",
    "href": "tutorial5.html#common-pitfalls-and-how-to-avoid-them",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Common Pitfalls and How to Avoid Them",
    "text": "Common Pitfalls and How to Avoid Them\n\nPitfall 1: The Copy-Paste Trap\nProblem: Copying code without understanding leads to: - Inability to modify code for your specific needs - Poor performance on exams or future assignments - Incorrect analyses when assumptions don’t hold\nSolution: Always ask “why” and “how” questions\n\"Why do we use this specific function instead of alternatives?\"\n\"How would I modify this code if my variable names were different?\"\n\"What would happen if I changed this parameter?\"\n\n\nPitfall 2: Assuming AI is Always Right\nProblem: LLMs can make mistakes, especially with: - Recent R package updates - Domain-specific best practices - Complex statistical procedures\nSolution: Always verify important claims\n\"Can you provide documentation or references for this approach?\"\n\"How can I verify that this method is appropriate for my data?\"\n\"What are the limitations or assumptions of this technique?\"\n\n\nPitfall 3: Over-Reliance on AI\nProblem: Using AI for every small problem prevents skill development\nSolution: Try first, then ask for help\n# Your workflow should be:\n1. Attempt the problem yourself\n2. Identify specific stuck points\n3. Ask targeted questions about those points\n4. Try to implement and understand the solution\n5. Ask follow-up questions if needed"
  },
  {
    "objectID": "tutorial5.html#academic-integrity-guidelines",
    "href": "tutorial5.html#academic-integrity-guidelines",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Academic Integrity Guidelines",
    "text": "Academic Integrity Guidelines\n\nWhat’s Generally Acceptable\n✅ Using AI to: - Explain error messages - Clarify concepts from lectures or readings - Suggest debugging approaches - Provide general coding guidance - Create practice examples for learning\n\n\nWhat’s Usually Not Acceptable\n❌ Using AI to: - Write entire assignment solutions - Complete take-home exams - Generate answers you can’t explain - Bypass learning objectives - Submit work without understanding\n\n\nWhen in Doubt, Ask Your Instructor\nEvery course has different policies. Always check with your instructor about: - What level of AI assistance is permitted - Whether you need to cite AI use - How to balance AI help with independent work - Course-specific expectations"
  },
  {
    "objectID": "tutorial5.html#practical-workflow-example",
    "href": "tutorial5.html#practical-workflow-example",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Practical Workflow Example",
    "text": "Practical Workflow Example\nHere’s a step-by-step workflow for using AI help effectively with a simple data task:\n\nStep 1: Try It Yourself First\n\n\nCode\n# Your initial attempt to calculate mean by group\ncountries %&gt;%\n  group_by(region)\n  mean(gdp)\n# Error: unexpected symbol\n\n\n\n\nStep 2: Ask Targeted Questions\n\"I'm trying to calculate the mean GDP for each region in my dataset but getting 'unexpected symbol' error. Here's my code: [paste code]. What am I doing wrong?\"\n\n\nStep 3: Implement and Test\n\n\nCode\n# AI suggested the correct syntax\ncountries %&gt;%\n  group_by(region) %&gt;%           # Added missing %&gt;%\n  summarize(mean_gdp = mean(gdp)) # Use summarize function\n\n\n\n\nStep 4: Seek Understanding\n\"Why do I need summarize() here? What's the difference between group_by() and summarize()?\"\n\n\nStep 5: Verify and Extend\n\"How can I also calculate the median and count of countries for each region in the same command?\""
  },
  {
    "objectID": "tutorial5.html#advanced-tips-for-better-learning",
    "href": "tutorial5.html#advanced-tips-for-better-learning",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Advanced Tips for Better Learning",
    "text": "Advanced Tips for Better Learning\n\nTip 1: Ask for Multiple Solutions\n\"Show me three different ways to visualize the relationship between income and voting probability, and explain when each approach is most useful.\"\n\n\nTip 2: Request Error Handling\n\"What are common errors students make when running logistic regression, and how can I avoid or fix them?\"\n\n\nTip 3: Seek Connections to Theory\n\"How does this statistical analysis connect to political science theories about voter participation?\"\n\n\nTip 4: Practice Explanation\n\"If I had to explain these results to someone who doesn't know statistics, how would I describe what the model tells us about voter behavior?\""
  },
  {
    "objectID": "tutorial5.html#building-your-programming-intuition",
    "href": "tutorial5.html#building-your-programming-intuition",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Building Your Programming Intuition",
    "text": "Building Your Programming Intuition\nThe goal isn’t to become dependent on AI, but to use it as a learning accelerator. Here’s how:\n\nDevelop Pattern Recognition\n\nNotice common code structures AI suggests\nLearn to recognize when similar patterns apply\nBuild your own library of template solutions\n\n\n\nBuild Debugging Skills\n\nAsk AI to explain error messages\nLearn common debugging strategies\nPractice identifying problems independently\n\n\n\nUnderstand the “Why” Behind the Code\n\nAlways ask for explanations of approach choices\nRequest information about alternatives\nLearn the reasoning behind coding decisions"
  },
  {
    "objectID": "tutorial5.html#conclusion",
    "href": "tutorial5.html#conclusion",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Conclusion",
    "text": "Conclusion\nAI chatbots can be powerful allies in learning R and statistics, but they work best when you approach them as teaching assistants rather than homework completion services. The key principles are:\n\nStart with your own effort - Try first, then ask for help\nAsk for understanding - Don’t just request code\nVerify and test - AI can make mistakes\nLearn actively - Engage with explanations and ask follow-ups\nMaintain integrity - Follow your course policies\n\nRemember: The goal is to become a competent data analyst who can think critically about statistical problems. AI should enhance your learning journey, not replace the learning process itself."
  },
  {
    "objectID": "tutorial5.html#quick-reference-effective-prompt-templates",
    "href": "tutorial5.html#quick-reference-effective-prompt-templates",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Quick Reference: Effective Prompt Templates",
    "text": "Quick Reference: Effective Prompt Templates\n\nFor Debugging\n\"I'm getting this error: [paste error]. Here's my code: [paste code]. I'm trying to [describe goal]. Can you help me understand what's wrong and how to fix it?\"\n\n\nFor Learning Concepts\n\"I'm learning about [concept] in my political science methods class. Can you explain [specific aspect] in simple terms and show how it applies to [your domain example]?\"\n\n\nFor Code Review\n\"Here's my code for [task]: [paste code]. Does this look correct? Are there ways I could improve it or make it more readable?\"\n\n\nFor Alternative Approaches\n\"I solved [problem] using [current approach]. Are there other ways to do this? What are the trade-offs between different methods?\"\n\nRemember: The best programmers aren’t those who never need help—they’re those who know how to find good help and learn from it effectively."
  },
  {
    "objectID": "tutorial3.html",
    "href": "tutorial3.html",
    "title": "Coding Tutorial: Grouping, Filtering, Mutating, Selecting, and Plotting",
    "section": "",
    "text": "Coding Tutorial: Grouping, Filtering, Mutating, Selecting, and Plotting\n\nload(\"~/Dropbox/PSCI2075_2025/PSCI_2075_v2.1.RData\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nnes |&gt; \n    summary()\n\n               follow       birthyr                     turnout12  \n Most of the time :610   Min.   :1921   Definitely voted     :912  \n Some of the Time :349   1st Qu.:1955   Probably did not vote:218  \n Only now and then:157   Median :1968   Not completely sure  : 48  \n Hardly at all    : 62   Mean   :1968                              \n                         3rd Qu.:1982                              \n                         Max.   :1997                              \n                                                                   \n          vote12                   meet    \n Mitt Romney :364   Extremely Likely :117  \n Barack Obama:505   Very Likely      :127  \n Someone Else: 68   Moderately likely:232  \n Not asked   :241   A little likely  :247  \n                    Not at all likely:455  \n                                           \n                                           \n                                march     \n Have done this in past 4 years    : 172  \n Have not done this in past 4 years:1006  \n                                          \n                                          \n                                          \n                                          \n                                          \n                           compromise     ftobama          ftblack      \n Compromies to get things done  :673   Min.   :  0.00   Min.   :  0.00  \n Sticks to his or her principles:505   1st Qu.:  5.00   1st Qu.: 50.00  \n                                       Median : 53.50   Median : 70.00  \n                                       Mean   : 48.76   Mean   : 66.97  \n                                       3rd Qu.: 87.00   3rd Qu.: 89.00  \n                                       Max.   :100.00   Max.   :100.00  \n                                                                        \n    ftwhite          fthisp           ftgay           fttrump      \n Min.   :  0.0   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.: 51.0   1st Qu.: 50.00   1st Qu.: 40.00   1st Qu.:  2.00  \n Median : 73.0   Median : 68.00   Median : 54.00   Median : 30.00  \n Mean   : 71.1   Mean   : 65.65   Mean   : 57.39   Mean   : 38.26  \n 3rd Qu.: 90.0   3rd Qu.: 88.00   3rd Qu.: 88.00   3rd Qu.: 72.00  \n Max.   :100.0   Max.   :100.00   Max.   :100.00   Max.   :100.00  \n                                                                   \n     fthrc          ftsanders        ftpolice          ftfem       \n Min.   :  0.00   Min.   :  0.0   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  3.00   1st Qu.: 19.0   1st Qu.: 50.00   1st Qu.: 29.00  \n Median : 45.50   Median : 51.0   Median : 70.00   Median : 50.00  \n Mean   : 43.11   Mean   : 50.5   Mean   : 66.22   Mean   : 51.55  \n 3rd Qu.: 76.00   3rd Qu.: 81.0   3rd Qu.: 90.00   3rd Qu.: 76.00  \n Max.   :100.00   Max.   :100.0   Max.   :100.00   Max.   :100.00  \n                                                                   \n    ftmuslim          ftsci        econnow\n Min.   :  0.00   Min.   :  0.00   1: 97  \n 1st Qu.: 18.00   1st Qu.: 55.25   2:287  \n Median : 50.00   Median : 80.00   3:406  \n Mean   : 45.44   Mean   : 72.92   4:246  \n 3rd Qu.: 69.00   3rd Qu.: 92.00   5:142  \n Max.   :100.00   Max.   :100.00          \n                                          \n                              lcself                  disc_b   \n Neither liberal nor conservative:327   A great deal     :218  \n Very liberal                    :173   A lot            :288  \n Somewhat liberal                :160   A moderate amount:356  \n Somewhat conservative           :158   A little         :247  \n Very conservative               :140   None at all      : 67  \n Closer to conservatives         :123   Skipped          :  2  \n (Other)                         : 97                          \n               disc_h                  disc_g                  disc_w   \n A great deal     :103   A great deal     :173   A great deal     : 75  \n A lot            :250   A lot            :293   A lot            :114  \n A moderate amount:440   A moderate amount:371   A moderate amount:232  \n A little         :300   A little         :246   A little         :384  \n None at all      : 84   None at all      : 93   None at all      :372  \n Skipped          :  1   Skipped          :  2   Skipped          :  1  \n                                                                        \n               disc_m                               disc_fed  \n A great deal     : 53   Treats both the same           :353  \n A lot            : 78   Treats whites much better      :199  \n A moderate amount:202   Treats whites a little better  :162  \n A little         :358   Treats blacks a little better  :152  \n None at all      :486   Treats whites moderately better:138  \n Skipped          :  1   Treats blacks moderately better: 87  \n                         (Other)                        : 87  \n                          disc_police                        immig_numb \n Treats both the same           :410   Increased a lot            : 83  \n Treats whites much better      :337   Increased a moderate amount:129  \n Treats whites a little better  :221   Increased a little         :145  \n Treats whites moderately better:181   Kept the same              :444  \n Treats blacks a little better  : 17   Decreased a little         :147  \n Treats blacks moderately better:  8   Decreased a moderate amount:108  \n (Other)                        :  4   Decreased a lot            :122  \n             terror_worry                healthspend \n Extremely worried :177   Increase a great deal:259  \n Very worried      :159   Incrase moderately   :237  \n Moderately worried:190   No Change            :236  \n Slightly worried  :149   Increase a little    :194  \n Not at all worried: 96   Decrease a great deal:132  \n Not asked         :407   Decrease a little    : 66  \n                          (Other)              : 54  \n                     finwell   \n A little worse          :243  \n A little better         :223  \n Moderately worse        :213  \n A great deal worse      :200  \n The same as 20 years ago:150  \n Moderately better       :105  \n (Other)                 : 44  \n                                              warmcause  \n Mostly by human activity                          :499  \n Mostly by natural causes                          :287  \n About equally by human activity and natural causes:392  \n                                                         \n                                                         \n                                                         \n                                                         \n                    freetrade            stopblack            stop_ever  \n Favor a great deal      : 98   Never         : 41   Has happened  :350  \n Favor moderately        :139   Rarely        : 75   Never happened:239  \n Favor a little          :183   Sometimes     :427   Skipped       :  1  \n Neither favor nor oppose:438   Somewhat often:331   Not asked     :588  \n Oppose a little         :108   Very often    :304                       \n Oppose moderately       :106                                            \n Opposed a great deal    :106                                            \n                  birthright_b        bo_muslim                  amer_ident \n Not asked              :576   Muslim      :391   Extremely important :506  \n Oppose a great deal    :161   Not a Muslim:781   Very important      :271  \n Neither favor or oppose:115   Skipped     :  6   Moderately important:213  \n Favor a great deal     : 92                      A little important  :109  \n Favor moderately       : 76                      Not important at all: 79  \n Oppose a little        : 61                                                \n (Other)                : 97                                                \n                 marstat                     employ   \n Married             :560   Full-time           :466  \n Separated           : 27   Retired             :194  \n Divorced            :123   Part-time           :141  \n Widowed             : 60   Permanently disabled:107  \n Single              :352   Unemployed          :101  \n Domestic Partnership: 56   Homemaker           : 91  \n                            (Other)             : 78  \n                   educ        gender          race             pid3    \n No HS               :100   Male  :567   White   :863   Democrat  :449  \n High school graduate:402   Female:611   Black   :131   Republican:276  \n Some college        :252                Hispanic:110   Indepent  :375  \n 2-year              :105                Mixed   : 29   Other     : 74  \n 4-year              :198                Asian   : 22   Not sure  :  4  \n Post-grad           :121                Other   : 15                   \n                                         (Other) :  8                   \n                pid7                   ideo5                    pew_churatd \n Strong Democrat  :295   Very liberal     :141   Never                :376  \n Independent      :201   Liberal          :207   Seldom               :267  \n Strong Republican:157   Moderate         :367   Once a week          :179  \n Weak Democrat    :141   Conservative     :257   A few times a year   :145  \n Weak Republican  :114   Very conservative:119   More than once a week: 95  \n (Other)          :256   Not sure         : 87   Once or twice a month: 93  \n NA's             : 14                           (Other)              : 23  \n     faminc          weight                                dpolice.new \n Min.   : 1.00   Min.   :0.1693   Treats whites much better      :337  \n 1st Qu.: 3.00   1st Qu.:0.3948   Treats whites moderately better:181  \n Median : 6.00   Median :0.8105   Treats whites a little better  :221  \n Mean   :16.76   Mean   :0.9941   Treats both the same           :410  \n 3rd Qu.: 9.00   3rd Qu.:1.2210   NA's                           : 29  \n Max.   :98.00   Max.   :7.0104                                        \n                                                                       \n\n\n\nnes |&gt; \n    summarize(mean_ftobama = mean(ftobama))\n\n# A tibble: 1 × 1\n  mean_ftobama\n         &lt;dbl&gt;\n1         48.8\n\n\n\nnes |&gt; \n    group_by(vote12) |&gt;\n    summarize(mean_ftobama = mean(ftobama))\n\n# A tibble: 4 × 2\n  vote12       mean_ftobama\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Mitt Romney          11.8\n2 Barack Obama         77.8\n3 Someone Else         23.9\n4 Not asked            50.7\n\n\n\nnes |&gt; \n    filter(vote12 == \"Barack Obama\") |&gt;\n    summarize(mean_ftobama = mean(ftobama))\n\n# A tibble: 1 × 1\n  mean_ftobama\n         &lt;dbl&gt;\n1         77.8\n\n\n\nnes |&gt; \n    mutate(new_var = \"this is a new variable\") |&gt; \n    glimpse()\n\nRows: 1,178\nColumns: 52\n$ follow       &lt;fct&gt; Most of the time, Some of the Time, Most of the time, Mos…\n$ birthyr      &lt;int&gt; 1960, 1957, 1963, 1980, 1974, 1958, 1978, 1951, 1973, 193…\n$ turnout12    &lt;fct&gt; Definitely voted, Probably did not vote, Definitely voted…\n$ vote12       &lt;fct&gt; Barack Obama, Not asked, Mitt Romney, Barack Obama, Mitt …\n$ meet         &lt;fct&gt; Extremely Likely, A little likely, Extremely Likely, Not …\n$ march        &lt;fct&gt; Have done this in past 4 years, Have not done this in pas…\n$ compromise   &lt;fct&gt; Compromies to get things done, Compromies to get things d…\n$ ftobama      &lt;int&gt; 100, 39, 1, 89, 1, 0, 73, 0, 12, 87, 32, 10, 70, 80, 4, 0…\n$ ftblack      &lt;int&gt; 100, 6, 50, 61, 61, 50, 100, 70, 50, 75, 53, 80, 88, 67, …\n$ ftwhite      &lt;int&gt; 100, 74, 50, 64, 58, 51, 70, 70, 50, 90, 52, 80, 90, 74, …\n$ fthisp       &lt;int&gt; 100, 6, 50, 61, 71, 51, 100, 69, 50, 51, 54, 80, 90, 57, …\n$ ftgay        &lt;int&gt; 96, 75, 16, 62, 55, 46, 100, 49, 5, 85, 52, 70, 86, 27, 9…\n$ fttrump      &lt;int&gt; 1, 28, 100, 0, 13, 61, 5, 85, 70, 5, 74, 95, 39, 82, 91, …\n$ fthrc        &lt;int&gt; 76, 52, 1, 69, 1, 1, 60, 0, 3, 87, 6, 10, 49, 3, 1, 0, 79…\n$ ftsanders    &lt;int&gt; 84, 13, 2, 71, 13, 11, 100, 40, 31, 97, 51, 10, 90, 41, 1…\n$ ftpolice     &lt;int&gt; 51, 65, 85, 56, 89, 55, 34, 85, 80, 60, 97, 90, 91, 99, 9…\n$ ftfem        &lt;int&gt; 62, 53, 2, 60, 31, 0, 100, 45, 21, 61, 34, 50, 80, 22, 54…\n$ ftmuslim     &lt;int&gt; 20, 22, 5, 61, 22, 11, 100, 40, 12, 72, 43, 50, 81, 53, 2…\n$ ftsci        &lt;int&gt; 100, 91, 61, 96, 40, 71, 100, 76, 50, 91, 98, 90, 91, 1, …\n$ econnow      &lt;fct&gt; 2, 4, 5, 2, 4, 4, 2, 3, 4, 2, 4, 3, 2, 5, 4, 4, 2, 3, 4, …\n$ lcself       &lt;fct&gt; Neither liberal nor conservative, Neither liberal nor con…\n$ disc_b       &lt;fct&gt; A little, None at all, None at all, A moderate amount, A …\n$ disc_h       &lt;fct&gt; A little, A moderate amount, None at all, A little, A lit…\n$ disc_g       &lt;fct&gt; A moderate amount, A moderate amount, None at all, A mode…\n$ disc_w       &lt;fct&gt; A little, A lot, A lot, None at all, A great deal, A litt…\n$ disc_m       &lt;fct&gt; A little, A little, A lot, None at all, A great deal, Non…\n$ disc_fed     &lt;fct&gt; Treats whites a little better, Treats whites a little bet…\n$ disc_police  &lt;fct&gt; Treats whites moderately better, Treats whites a little b…\n$ immig_numb   &lt;fct&gt; Increased a moderate amount, Increased a little, Decrease…\n$ terror_worry &lt;fct&gt; Not at all worried, Not asked, Extremely worried, Not at …\n$ healthspend  &lt;fct&gt; Increase a great deal, Incrase moderately, Decrease moder…\n$ finwell      &lt;fct&gt; Moderately worse, A little worse, A great deal worse, Mod…\n$ warmcause    &lt;fct&gt; Mostly by human activity, About equally by human activity…\n$ freetrade    &lt;fct&gt; Neither favor nor oppose, Neither favor nor oppose, Neith…\n$ stopblack    &lt;fct&gt; Somewhat often, Somewhat often, Sometimes, Somewhat often…\n$ stop_ever    &lt;fct&gt; Has happened, Not asked, Not asked, Not asked, Has happen…\n$ birthright_b &lt;fct&gt; Favor moderately, Not asked, Not asked, Not asked, Oppose…\n$ bo_muslim    &lt;fct&gt; Not a Muslim, Not a Muslim, Muslim, Not a Muslim, Not a M…\n$ amer_ident   &lt;fct&gt; A little important, Moderately important, Extremely impor…\n$ marstat      &lt;fct&gt; Married, Divorced, Single, Married, Married, Married, Sin…\n$ employ       &lt;fct&gt; Full-time, Full-time, Unemployed, Full-time, Full-time, F…\n$ educ         &lt;fct&gt; 4-year, Post-grad, 4-year, 4-year, High school graduate, …\n$ gender       &lt;fct&gt; Male, Female, Male, Male, Male, Male, Male, Male, Male, M…\n$ race         &lt;fct&gt; White, White, White, White, White, White, White, White, W…\n$ pid3         &lt;fct&gt; Democrat, Indepent, Republican, Democrat, Other, Indepent…\n$ pid7         &lt;fct&gt; Strong Democrat, Independent, Weak Republican, Strong Dem…\n$ ideo5        &lt;fct&gt; Not sure, Liberal, Conservative, Liberal, Conservative, C…\n$ pew_churatd  &lt;fct&gt; Never, A few times a year, Never, Never, Once a week, Onc…\n$ faminc       &lt;int&gt; 4, 8, 1, 12, 10, 7, 97, 10, 8, 10, 11, 5, 5, 6, 8, 97, 97…\n$ weight       &lt;dbl&gt; 0.9511600, 2.6701962, 1.4303896, 0.9139662, 0.2639346, 1.…\n$ dpolice.new  &lt;fct&gt; Treats whites moderately better, Treats whites a little b…\n$ new_var      &lt;chr&gt; \"this is a new variable\", \"this is a new variable\", \"this…\n\n\n\nnes |&gt; \n    mutate(ftobama_cat = case_when(ftobama &gt;= 50 ~ \"Generally Positive\",\n                                    ftobama &lt; 50 ~ \"Generally Negative\")) |&gt; \n    glimpse()\n\nRows: 1,178\nColumns: 52\n$ follow       &lt;fct&gt; Most of the time, Some of the Time, Most of the time, Mos…\n$ birthyr      &lt;int&gt; 1960, 1957, 1963, 1980, 1974, 1958, 1978, 1951, 1973, 193…\n$ turnout12    &lt;fct&gt; Definitely voted, Probably did not vote, Definitely voted…\n$ vote12       &lt;fct&gt; Barack Obama, Not asked, Mitt Romney, Barack Obama, Mitt …\n$ meet         &lt;fct&gt; Extremely Likely, A little likely, Extremely Likely, Not …\n$ march        &lt;fct&gt; Have done this in past 4 years, Have not done this in pas…\n$ compromise   &lt;fct&gt; Compromies to get things done, Compromies to get things d…\n$ ftobama      &lt;int&gt; 100, 39, 1, 89, 1, 0, 73, 0, 12, 87, 32, 10, 70, 80, 4, 0…\n$ ftblack      &lt;int&gt; 100, 6, 50, 61, 61, 50, 100, 70, 50, 75, 53, 80, 88, 67, …\n$ ftwhite      &lt;int&gt; 100, 74, 50, 64, 58, 51, 70, 70, 50, 90, 52, 80, 90, 74, …\n$ fthisp       &lt;int&gt; 100, 6, 50, 61, 71, 51, 100, 69, 50, 51, 54, 80, 90, 57, …\n$ ftgay        &lt;int&gt; 96, 75, 16, 62, 55, 46, 100, 49, 5, 85, 52, 70, 86, 27, 9…\n$ fttrump      &lt;int&gt; 1, 28, 100, 0, 13, 61, 5, 85, 70, 5, 74, 95, 39, 82, 91, …\n$ fthrc        &lt;int&gt; 76, 52, 1, 69, 1, 1, 60, 0, 3, 87, 6, 10, 49, 3, 1, 0, 79…\n$ ftsanders    &lt;int&gt; 84, 13, 2, 71, 13, 11, 100, 40, 31, 97, 51, 10, 90, 41, 1…\n$ ftpolice     &lt;int&gt; 51, 65, 85, 56, 89, 55, 34, 85, 80, 60, 97, 90, 91, 99, 9…\n$ ftfem        &lt;int&gt; 62, 53, 2, 60, 31, 0, 100, 45, 21, 61, 34, 50, 80, 22, 54…\n$ ftmuslim     &lt;int&gt; 20, 22, 5, 61, 22, 11, 100, 40, 12, 72, 43, 50, 81, 53, 2…\n$ ftsci        &lt;int&gt; 100, 91, 61, 96, 40, 71, 100, 76, 50, 91, 98, 90, 91, 1, …\n$ econnow      &lt;fct&gt; 2, 4, 5, 2, 4, 4, 2, 3, 4, 2, 4, 3, 2, 5, 4, 4, 2, 3, 4, …\n$ lcself       &lt;fct&gt; Neither liberal nor conservative, Neither liberal nor con…\n$ disc_b       &lt;fct&gt; A little, None at all, None at all, A moderate amount, A …\n$ disc_h       &lt;fct&gt; A little, A moderate amount, None at all, A little, A lit…\n$ disc_g       &lt;fct&gt; A moderate amount, A moderate amount, None at all, A mode…\n$ disc_w       &lt;fct&gt; A little, A lot, A lot, None at all, A great deal, A litt…\n$ disc_m       &lt;fct&gt; A little, A little, A lot, None at all, A great deal, Non…\n$ disc_fed     &lt;fct&gt; Treats whites a little better, Treats whites a little bet…\n$ disc_police  &lt;fct&gt; Treats whites moderately better, Treats whites a little b…\n$ immig_numb   &lt;fct&gt; Increased a moderate amount, Increased a little, Decrease…\n$ terror_worry &lt;fct&gt; Not at all worried, Not asked, Extremely worried, Not at …\n$ healthspend  &lt;fct&gt; Increase a great deal, Incrase moderately, Decrease moder…\n$ finwell      &lt;fct&gt; Moderately worse, A little worse, A great deal worse, Mod…\n$ warmcause    &lt;fct&gt; Mostly by human activity, About equally by human activity…\n$ freetrade    &lt;fct&gt; Neither favor nor oppose, Neither favor nor oppose, Neith…\n$ stopblack    &lt;fct&gt; Somewhat often, Somewhat often, Sometimes, Somewhat often…\n$ stop_ever    &lt;fct&gt; Has happened, Not asked, Not asked, Not asked, Has happen…\n$ birthright_b &lt;fct&gt; Favor moderately, Not asked, Not asked, Not asked, Oppose…\n$ bo_muslim    &lt;fct&gt; Not a Muslim, Not a Muslim, Muslim, Not a Muslim, Not a M…\n$ amer_ident   &lt;fct&gt; A little important, Moderately important, Extremely impor…\n$ marstat      &lt;fct&gt; Married, Divorced, Single, Married, Married, Married, Sin…\n$ employ       &lt;fct&gt; Full-time, Full-time, Unemployed, Full-time, Full-time, F…\n$ educ         &lt;fct&gt; 4-year, Post-grad, 4-year, 4-year, High school graduate, …\n$ gender       &lt;fct&gt; Male, Female, Male, Male, Male, Male, Male, Male, Male, M…\n$ race         &lt;fct&gt; White, White, White, White, White, White, White, White, W…\n$ pid3         &lt;fct&gt; Democrat, Indepent, Republican, Democrat, Other, Indepent…\n$ pid7         &lt;fct&gt; Strong Democrat, Independent, Weak Republican, Strong Dem…\n$ ideo5        &lt;fct&gt; Not sure, Liberal, Conservative, Liberal, Conservative, C…\n$ pew_churatd  &lt;fct&gt; Never, A few times a year, Never, Never, Once a week, Onc…\n$ faminc       &lt;int&gt; 4, 8, 1, 12, 10, 7, 97, 10, 8, 10, 11, 5, 5, 6, 8, 97, 97…\n$ weight       &lt;dbl&gt; 0.9511600, 2.6701962, 1.4303896, 0.9139662, 0.2639346, 1.…\n$ dpolice.new  &lt;fct&gt; Treats whites moderately better, Treats whites a little b…\n$ ftobama_cat  &lt;chr&gt; \"Generally Positive\", \"Generally Negative\", \"Generally Ne…\n\n\n\nnes &lt;- nes |&gt; \n    mutate(ftobama_cat = case_when(ftobama &gt;= 50 ~ \"Generally Positive\",\n                                    ftobama &lt; 50 ~ \"Generally Negative\"))\n\ng &lt;- nes |&gt; \n    ggplot(aes(x = ftobama_cat)) + \n    geom_bar() \n\ng + \n    theme_minimal() + \n    labs(\n        x = \"Feelings toward Obama (Categorical)\",\n        y = \"Frequency\", \n        title = \"Frequency of NES respondents who are generally negative\\n vs. generally positive toward Obama\"\n    )"
  },
  {
    "objectID": "tutorial1.html",
    "href": "tutorial1.html",
    "title": "Intro to Greek Notation and Univariate Descrpitive Statistics",
    "section": "",
    "text": "For most people, one of the most challenging parts about learning data analysis and quantitative methods is getting used to all of the complicated notation. This is perfectly understandable: math notation uses Greek (literally a different language than English) and combines it with math!\nWhile the learning curve is steep, you will eventually get more comfortable consuming Greek notation. In this tutorial, I’ll walk through some of the most common symbols you’ll encounter and illustrate them with the univariate descriptive statistics that you’ll need to know."
  },
  {
    "objectID": "tutorial1.html#what-is-going-on-with-all-those-greek-letters",
    "href": "tutorial1.html#what-is-going-on-with-all-those-greek-letters",
    "title": "Intro to Greek Notation and Univariate Descrpitive Statistics",
    "section": "",
    "text": "For most people, one of the most challenging parts about learning data analysis and quantitative methods is getting used to all of the complicated notation. This is perfectly understandable: math notation uses Greek (literally a different language than English) and combines it with math!\nWhile the learning curve is steep, you will eventually get more comfortable consuming Greek notation. In this tutorial, I’ll walk through some of the most common symbols you’ll encounter and illustrate them with the univariate descriptive statistics that you’ll need to know."
  },
  {
    "objectID": "tutorial1.html#mean-a.k.a-first-population-moment-a.k.a-expected-value",
    "href": "tutorial1.html#mean-a.k.a-first-population-moment-a.k.a-expected-value",
    "title": "Intro to Greek Notation and Univariate Descrpitive Statistics",
    "section": "Mean (a.k.a first population moment) (a.k.a expected value)",
    "text": "Mean (a.k.a first population moment) (a.k.a expected value)\nMean, which is one measure of the “average” or central tendency of a set of values, is calculated by summing all of your values and dividing by the total number of values that you have. This should be pretty familiar for you, you likely know intuitively that the average of \\(1 + 2 + 3 = 2\\).\nShown below is how this looks in code. I first generate 500 random values and store them as a column in a dataframe. Then, I print the mean of that column. Last, I create a histogram of the values and show the mean wtih a red line.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.0     ✔ stringr   1.5.1\n✔ ggplot2   3.5.2     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.1.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(1234)\n# simulate 500 values and store as a vector in a dataframe\ndf &lt;- data.frame(x = rnorm(500))\n\n# print the mean\nmean(df$x)\n\n[1] 0.001838821\n\n# graph the histogram and show the mean line (in red)\ndf %&gt;% \n    ggplot() + \n    geom_histogram(aes(x = x)) + \n    geom_vline(aes(xintercept = mean(x)), color = \"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nWe simulated data from the “standard normal” distribution (don’t worry about what that means yet), which by definition has a mean of 0–and you can see that we are finding a mean of close to zero! And, indeed, if we do the basic “add up all the values and divide by the number of values”, you’ll get the same value:\n\nsum_x = sum(df$x)\nsum_x / length(df$x)\n\n[1] 0.001838821\n\n\nWhat does this look like in Greek notation? Let’s take a look:\n\\[\\mathbb{E}[X] = \\frac{\\sum_{i=1}^n X_i}{n}\\]\nDespite the fact that you’re probably comfortable with the concept of mean, there are a lot of symbols here to describe that concept. It can be daunting! But we can break it down and show that it really isn’t that bad.\n\\(\\mathbb{E}[X]\\) is how we symbolize “Expected Value of X”; in this case, we’re using X to refer to our whole set of values (the vector containing all of the numbers that we want to find the mean of). Expected value, remember, just means “mean”.\n\\(\\sum_{i=1}^n X_i\\) symbolizes a summation. The sigma \\(\\sum\\) is the greek symbol for “add all of the elements together”. The \\(i\\) below the sigma indicates that we’re going to start at the first element of our data. The \\(n\\) above the sigma indicates that we’re going to use all of the data up to the last value. \\(X_i\\) refers to the \\(i\\)-th element of X, our vector holding all of the values that we want to average.\nTo put it together, we’re summing every value from \\(X_1\\) to \\(X_n\\), and then we’re dividing that by \\(n\\). In this case, our \\(X\\) vector is the data that we simulated, and we’re going to go from \\(X_1\\) to \\(X_{500}\\) (since we created 500 values in our original code)."
  },
  {
    "objectID": "tutorial1.html#variance",
    "href": "tutorial1.html#variance",
    "title": "Intro to Greek Notation and Univariate Descrpitive Statistics",
    "section": "Variance",
    "text": "Variance\nLet’s turn to another important descriptive statistic, variance. This measures the “spread”, or “dispersion”, of your data around the mean. Think how long the tails on the left and right of the mean are.\nFinding variance in code is easy:\n\nvar(df$x)\n\n[1] 1.07084\n\n\nWhat does a higher or lower variance actually mean? Let’s investigate. We’re going to create two new vectors/columns: a high variance one (var = 4) and a low variance one (var = .25). Note: the code we use to generate this asks for standard deviation, which is just the square root of the variance.\nAfter adding a “high_var” and a “low_var” column to our data frame, we then create a histogram showing the different columns on the same graph. Our high-variance data is shown in blue, our original data is shown in orange, and our low-variance data is shown in green. What do you notice?\n\ndf &lt;- df %&gt;% \n    mutate(high_var = rnorm(500, mean = 0, sd = 2)) %&gt;% \n    mutate(low_var = rnorm(500, mean = 0, sd = .5))\n\ndf %&gt;% \n    ggplot() + \n    geom_histogram(aes(x = high_var), fill = \"navy\", alpha = .5) +\n    geom_histogram(aes(x = low_var), fill = \"darkgreen\", alpha = .5) +\n    geom_histogram(aes(x = x), fill = \"orange\") + \n    geom_vline(aes(xintercept = mean(x)), color = \"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.\n\n\n\n\n\n\n\n\n\nVariance in Greek notation is pretty similar to that of the mean–in fact, mean and variance are linked mathematically, too! The variance equation is depicted by:\n\\[\\text{Var(X)} = \\sigma^2 = s^2 = \\frac{\\sum_{i=1}^n (X_i - \\mu)^2}{n-1}\\]\nLet’s walk through it piece by piece. First, \\(\\text{Var(X)}, \\sigma^2, \\text{and } s^2\\) are all different ways of saying “variance of X”. Second, now we have a slightly different summation than what’s in the equation for mean. \\(\\sum_{i=1}^n (X_i - \\mu)^2\\) is depicting how we are summing (\\(\\sum\\)) from our first element (\\(i=1\\)) to our \\(n\\)-th element for the value \\((X_i - \\mu)^2\\), where \\(\\mu\\) is simply the mean of \\(X\\). Intuitively, this is finding the sum of all of the distances (squared) between a single value \\(X_i\\) and the mean of all of the values. Last, we divide that summed value by \\(n-1\\).\nNote that variance includes mean as part of its equation. You can literally take the mean we calculated above and plug it into this function and find the right value! Let’s see how that looks:\n\nvar(df$x)\n\n[1] 1.07084\n\nmean_x &lt;- mean(df$x)\nsum_squared_distance &lt;- sum((df$x - mean_x)^2)\nvar_calc = sum_squared_distance/(length(df$x)-1)\nvar_calc\n\n[1] 1.07084\n\n\nSo the variance of a sample is a function of the mean of that sample."
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n\nThis is a personal academic website built with Quarto, a scientific publishing system. The site belongs to Alex Newhouse, a PhD candidate in Political Science specializing in computational analysis of political violence and extremism.\n\n\n\nQuarto Website Structure: - _quarto.yml - Main configuration file defining website structure, theme (lumen), and navbar - *.qmd - Quarto markdown files that generate the website pages - docs/ - Output directory where rendered HTML files are published (GitHub Pages) - posts/ - Directory containing blog post subdirectories - profpic.jpeg - Profile image used on the main page\nKey Pages: - index.qmd - Homepage with personal bio and research overview - cv.qmd - Academic CV - research.qmd - Research projects and publications - teaching.qmd - Teaching experience and tutorial links - posts.qmd - Blog listing page - tutorial*.qmd - Educational content for quantitative methods\n\n\n\nBuild and Preview:\nquarto render          # Build the entire website to docs/\nquarto preview         # Live preview with auto-reload during development\nquarto render &lt;file&gt;   # Render specific file\nPublishing: The site automatically publishes from the docs/ directory to GitHub Pages when changes are pushed to the main branch.\n\n\n\nAcademic Focus: Content should maintain an academic tone appropriate for a PhD candidate’s professional website. The site showcases research on political violence, extremism, and computational social science.\nTeaching Materials: Tutorial files (tutorial1.qmd, tutorial2-pipe.qmd, etc.) contain educational content for quantitative methods courses. These should be pedagogically sound and accessible to undergraduate students.\nBlog Posts: Located in posts/ subdirectories, focused on research, technology, and academia.\n\n\n\n\nUse lowercase with hyphens for multi-word filenames\nTutorial files: tutorial[number]-[topic].qmd\nBlog posts: posts/YYYY-MM-DD-[title]/index.qmd\n\n\n\n\n\nSearch functionality enabled\nRSS feed for blog posts\nResponsive design with trestles template on homepage\nSocial media links (LinkedIn, Bluesky, GitHub, Email)\nCategory filtering and sorting for blog posts"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This is a personal academic website built with Quarto, a scientific publishing system. The site belongs to Alex Newhouse, a PhD candidate in Political Science specializing in computational analysis of political violence and extremism."
  },
  {
    "objectID": "CLAUDE.html#architecture",
    "href": "CLAUDE.html#architecture",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto Website Structure: - _quarto.yml - Main configuration file defining website structure, theme (lumen), and navbar - *.qmd - Quarto markdown files that generate the website pages - docs/ - Output directory where rendered HTML files are published (GitHub Pages) - posts/ - Directory containing blog post subdirectories - profpic.jpeg - Profile image used on the main page\nKey Pages: - index.qmd - Homepage with personal bio and research overview - cv.qmd - Academic CV - research.qmd - Research projects and publications - teaching.qmd - Teaching experience and tutorial links - posts.qmd - Blog listing page - tutorial*.qmd - Educational content for quantitative methods"
  },
  {
    "objectID": "CLAUDE.html#development-commands",
    "href": "CLAUDE.html#development-commands",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Build and Preview:\nquarto render          # Build the entire website to docs/\nquarto preview         # Live preview with auto-reload during development\nquarto render &lt;file&gt;   # Render specific file\nPublishing: The site automatically publishes from the docs/ directory to GitHub Pages when changes are pushed to the main branch."
  },
  {
    "objectID": "CLAUDE.html#content-guidelines",
    "href": "CLAUDE.html#content-guidelines",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Academic Focus: Content should maintain an academic tone appropriate for a PhD candidate’s professional website. The site showcases research on political violence, extremism, and computational social science.\nTeaching Materials: Tutorial files (tutorial1.qmd, tutorial2-pipe.qmd, etc.) contain educational content for quantitative methods courses. These should be pedagogically sound and accessible to undergraduate students.\nBlog Posts: Located in posts/ subdirectories, focused on research, technology, and academia."
  },
  {
    "objectID": "CLAUDE.html#file-naming",
    "href": "CLAUDE.html#file-naming",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Use lowercase with hyphens for multi-word filenames\nTutorial files: tutorial[number]-[topic].qmd\nBlog posts: posts/YYYY-MM-DD-[title]/index.qmd"
  },
  {
    "objectID": "CLAUDE.html#website-features",
    "href": "CLAUDE.html#website-features",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Search functionality enabled\nRSS feed for blog posts\nResponsive design with trestles template on homepage\nSocial media links (LinkedIn, Bluesky, GitHub, Email)\nCategory filtering and sorting for blog posts"
  },
  {
    "objectID": "tutorial5.html#working-through-simple-examples",
    "href": "tutorial5.html#working-through-simple-examples",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Working Through Simple Examples",
    "text": "Working Through Simple Examples\nLet’s start with basic data analysis tasks that are common in undergraduate coursework. We’ll build complexity gradually.\n\nExample 1: Getting Basic Statistics\nScenario: You have a dataset of country information and need to calculate descriptive statistics.\nYour Prompt:\nI'm new to R and have a dataset called 'countries' with variables for population, GDP, and region. I need to calculate the mean, median, and standard deviation for population. Can you show me how to do this step by step with explanations?\nGood AI Response:\n\n\nCode\n# Calculate basic statistics for population\nmean(countries$population)          # Average population\nmedian(countries$population)        # Middle value when sorted\nsd(countries$population)           # Standard deviation (spread)\n\n# Get all statistics at once\nsummary(countries$population)      # Shows min, max, quartiles, mean\n\n\n✅ What Makes This Good: - Simple, clear code - Comments explain what each function does - Shows both individual functions and summary approach\n🤔 Follow-Up Questions to Ask:\n1. \"What does standard deviation tell me about the data?\"\n2. \"Why might median be different from mean?\"\n3. \"What if my data has missing values?\"\n\n\nExample 2: Data Filtering and Grouping\nYour Prompt:\nI want to find the average GDP for European countries only. My dataset has a 'region' column with values like \"Europe\", \"Asia\", etc. How do I filter the data and then calculate the mean? Please explain each step.\nGood AI Response:\n\n\nCode\nlibrary(dplyr)  # Load the dplyr package for data manipulation\n\n# Method 1: Filter then calculate mean\neuropean_countries &lt;- countries %&gt;%\n  filter(region == \"Europe\")        # Keep only European countries\n\nmean(european_countries$gdp)        # Calculate mean GDP\n\n# Method 2: Do it all in one step\neurope_avg_gdp &lt;- countries %&gt;%\n  filter(region == \"Europe\") %&gt;%    # Filter to Europe\n  summarize(avg_gdp = mean(gdp))    # Calculate mean\n\n# Method 3: Compare all regions\ncountries %&gt;%\n  group_by(region) %&gt;%              # Group by region\n  summarize(avg_gdp = mean(gdp))    # Calculate mean for each group\n\n\n✅ Learning Opportunities: - Shows multiple approaches (good for learning!) - Introduces the pipe operator %&gt;% - Demonstrates filtering and grouping concepts\n🤔 Questions to Deepen Understanding:\n1. \"What's the difference between filter() and select()?\"\n2. \"Can you explain how the pipe operator (%&gt;%) works?\"\n3. \"How would I filter for multiple conditions?\"\n\n\nExample 3: Creating New Variables\nYour Prompt:\nI need to create a new variable that categorizes countries as \"High\", \"Medium\", or \"Low\" GDP based on their GDP values. How do I create this new column? I'm a beginner so please explain the logic.\nGood AI Response:\n\n\nCode\nlibrary(dplyr)\n\n# Method 1: Using ifelse for simple categories\ncountries &lt;- countries %&gt;%\n  mutate(gdp_category = ifelse(gdp &gt; 50000, \"High\",\n                        ifelse(gdp &gt; 20000, \"Medium\", \"Low\")))\n\n# Method 2: Using case_when (cleaner for multiple categories)\ncountries &lt;- countries %&gt;%\n  mutate(gdp_category = case_when(\n    gdp &gt; 50000 ~ \"High\",\n    gdp &gt; 20000 ~ \"Medium\",\n    TRUE ~ \"Low\"                    # Everything else gets \"Low\"\n  ))\n\n# Check your work\ntable(countries$gdp_category)       # Count how many in each category\n\n\n✅ Good Teaching Elements: - Shows two methods for comparison - Explains the logic of nested conditions - Includes verification step\n🤔 Learning Questions:\n1. \"What does the TRUE in case_when() do?\"\n2. \"How do I choose the cutoff values for categories?\"\n3. \"What if I want to base categories on percentiles instead?\"\n\n\nExample 4: Simple Data Visualization\nYour Prompt:\nI want to create a histogram of GDP values to see the distribution. I'm using ggplot2 but I'm not sure about the syntax. Can you show me how and explain what each part does?\nGood AI Response:\n\n\nCode\nlibrary(ggplot2)\n\n# Basic histogram\nggplot(countries, aes(x = gdp)) +    # Set up plot with GDP on x-axis\n  geom_histogram() +                 # Add histogram layer\n  labs(title = \"Distribution of GDP\", # Add labels\n       x = \"GDP per Capita\",\n       y = \"Number of Countries\")\n\n# Improved version with better formatting\nggplot(countries, aes(x = gdp)) +\n  geom_histogram(bins = 20,          # Set number of bins\n                 fill = \"steelblue\",  # Color the bars\n                 alpha = 0.7) +       # Make bars semi-transparent\n  labs(title = \"Distribution of GDP per Capita\",\n       x = \"GDP per Capita (USD)\",\n       y = \"Number of Countries\") +\n  theme_minimal()                    # Clean theme\n\n\n✅ Educational Value: - Starts simple, then adds complexity - Explains each argument - Shows how to improve basic plots\n🤔 Extension Questions:\n1. \"How do I add a vertical line showing the mean?\"\n2. \"What if I want separate histograms for each region?\"\n3. \"How do I save this plot as an image file?\""
  },
  {
    "objectID": "tutorial5.html#advanced-example-regression-analysis",
    "href": "tutorial5.html#advanced-example-regression-analysis",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Advanced Example: Regression Analysis",
    "text": "Advanced Example: Regression Analysis\nOnce you’re comfortable with basic data manipulation, you might encounter more complex statistical tasks. Here’s how to approach regression analysis with AI help:\n\nScenario: Analyzing Election Turnout\nAssignment: You need to analyze factors that predict voter turnout using a dataset with variables for income, education, and age.\n\n\nStep 1: Initial Problem Setup\nYour Prompt:\nI'm working on a political science assignment analyzing voter turnout. I have a dataset called 'voter_data' with these variables:\n- turnout: whether someone voted (1 = yes, 0 = no)\n- income: household income in thousands\n- education: years of education\n- age: respondent age\n\nI need to run a logistic regression since turnout is binary. Can you show me the basic code structure and explain why we use logistic regression for binary outcomes? Please include comments explaining each step.\nAI Response Analysis:\n\n\nCode\n# Load necessary library\nlibrary(dplyr)\n\n# Run logistic regression\nmodel &lt;- glm(turnout ~ income + education + age,\n             data = voter_data,\n             family = binomial)\n\n# View results\nsummary(model)\n\n\n✅ Good AI Response Elements: - Provides commented code - Uses appropriate function (glm with family = binomial) - Shows how to view results\n🤔 What to Verify: - Is the syntax correct for your version of R? - Do the variable names match your actual dataset? - Are there additional steps you might need?\n\n\nStep 2: Follow-Up Questions\nDon’t just copy the code! Ask follow-up questions to deepen your understanding:\nYour Follow-Up Prompts:\n1. \"Can you explain what the 'family = binomial' argument does and why it's necessary for binary outcomes?\"\n\n2. \"How do I interpret the coefficients in logistic regression? They seem different from regular regression.\"\n\n3. \"What should I look for in the summary output to determine if my model is good?\"\n\n\nStep 3: Adapting and Debugging\nWhen you try to run the AI’s code, you might encounter errors. Here’s how to handle them:\nYour Error Message:\nError in glm.fit: object 'voter_data' not found\nYour Debugging Prompt:\nI'm getting this error when trying to run the logistic regression code you provided: [paste error message]. I think it might be because I haven't loaded my dataset properly. Can you show me how to load a CSV file and check that it loaded correctly before running the regression?"
  },
  {
    "objectID": "tutorial6.html",
    "href": "tutorial6.html",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "",
    "text": "If you’ve ever found yourself confused about why R can’t find your data file, or why your script works on your computer but not your classmate’s, or why you keep getting “file not found” errors even though you know the file is there—this tutorial is for you!\nUnderstanding how your computer organizes files and how R navigates through them is absolutely crucial for doing data analysis. It’s like learning to read a map before going on a road trip. Once you understand the basic concepts, you’ll spend less time wrestling with technical issues and more time focusing on your actual research."
  },
  {
    "objectID": "tutorial6.html#why-do-i-need-to-understand-file-systems",
    "href": "tutorial6.html#why-do-i-need-to-understand-file-systems",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "",
    "text": "If you’ve ever found yourself confused about why R can’t find your data file, or why your script works on your computer but not your classmate’s, or why you keep getting “file not found” errors even though you know the file is there—this tutorial is for you!\nUnderstanding how your computer organizes files and how R navigates through them is absolutely crucial for doing data analysis. It’s like learning to read a map before going on a road trip. Once you understand the basic concepts, you’ll spend less time wrestling with technical issues and more time focusing on your actual research."
  },
  {
    "objectID": "tutorial6.html#what-is-a-file-system",
    "href": "tutorial6.html#what-is-a-file-system",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "What is a file system?",
    "text": "What is a file system?\nA file system is simply how your computer organizes and stores files. Think of it like a giant filing cabinet with folders inside folders inside folders. Every file on your computer has an “address” that tells you exactly where to find it.\nLet’s start with some basics that apply to both Windows and Mac:\n\nFiles are individual documents (like my_data.csv or homework.R)\nFolders (also called directories) are containers that hold files and other folders\nPaths are the “addresses” that tell you exactly where a file or folder is located"
  },
  {
    "objectID": "tutorial6.html#operating-system-differences",
    "href": "tutorial6.html#operating-system-differences",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Operating System Differences",
    "text": "Operating System Differences\n\nWindowsMac\n\n\nOn Windows, your file system starts with drive letters like C:, D:, etc. The main drive is usually C:. Folders are separated by backslashes (\\).\nA typical Windows path might look like:\nC:\\Users\\YourName\\Documents\\POLS101\\homework1.R\nThis means: - Start at the C: drive - Go to the Users folder - Then to the folder with your username - Then to Documents - Then to POLS101 - Finally, find the file homework1.R\n\n\nOn Mac, the file system starts with a forward slash (/) and doesn’t use drive letters. Folders are separated by forward slashes (/).\nA typical Mac path might look like:\n/Users/YourName/Documents/POLS101/homework1.R\nThis means: - Start at the root directory (/) - Go to the Users folder - Then to the folder with your username - Then to Documents - Then to POLS101 - Finally, find the file homework1.R"
  },
  {
    "objectID": "tutorial6.html#what-is-a-working-directory",
    "href": "tutorial6.html#what-is-a-working-directory",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "What is a working directory?",
    "text": "What is a working directory?\nYour working directory is like your current location in the file system. It’s where R is “standing” right now. When you tell R to open a file, it looks for that file starting from your working directory.\nThink of it this way: if you’re in a library and someone says “go get the book on the third shelf,” you need to know which section of the library you’re currently in to know which third shelf they mean!\nLet’s see what your current working directory is:\n\n# This shows you where R is currently \"standing\"\ngetwd()\n\n[1] \"/Users/alexnewhouse/alexbnewhouse.github.io\"\n\n\nWhen you first open RStudio, your working directory is usually set to your home folder. But this might not be where your data files are stored!"
  },
  {
    "objectID": "tutorial6.html#common-file-system-mistakes-and-how-to-fix-them",
    "href": "tutorial6.html#common-file-system-mistakes-and-how-to-fix-them",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Common file system mistakes (and how to fix them)",
    "text": "Common file system mistakes (and how to fix them)\n\nMistake #1: “But I can see the file!”\nThe Problem: You can see your data file in your file explorer/finder, but R says it can’t find it.\nWhat’s happening: Just because you can see the file doesn’t mean R knows where to look for it. R only looks in your current working directory (and its subdirectories) by default.\nThe Solution: Either move your file to your working directory, or tell R exactly where to find your file.\n\n\nMistake #2: Hard-coding full paths\nThe Problem: Your code works on your computer but breaks on your classmate’s computer.\n\n# This will only work on YOUR computer\ndata &lt;- read.csv(\"C:/Users/YourName/Documents/POLS101/data.csv\")  # Windows\ndata &lt;- read.csv(\"/Users/YourName/Documents/POLS101/data.csv\")   # Mac\n\nWhat’s happening: You’ve written the specific path that exists on your computer, but your classmate has a different username and maybe even a different folder structure.\nThe Solution: Use relative paths (explained below) or set up your project properly.\n\n\nMistake #3: Not organizing your project files\nThe Problem: Your files are scattered all over your computer, making it hard to keep track of what goes with what project.\nThe Solution: Create a dedicated folder for each project and keep everything related to that project inside it."
  },
  {
    "objectID": "tutorial6.html#absolute-vs.-relative-paths",
    "href": "tutorial6.html#absolute-vs.-relative-paths",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Absolute vs. Relative Paths",
    "text": "Absolute vs. Relative Paths\nUnderstanding the difference between absolute and relative paths is crucial:\n\nAbsolute Paths\nAn absolute path gives the complete address from the very top of your file system:\n\nWindowsMac\n\n\nC:\\Users\\YourName\\Documents\\POLS101\\data\\survey_results.csv\n\n\n/Users/YourName/Documents/POLS101/data/survey_results.csv\n\n\n\n\n\nRelative Paths\nA relative path gives directions from your current location (working directory):\ndata/survey_results.csv\nThis says: “from where I am now, go into the ‘data’ folder and find ‘survey_results.csv’”"
  },
  {
    "objectID": "tutorial6.html#best-practices-for-organizing-your-r-projects",
    "href": "tutorial6.html#best-practices-for-organizing-your-r-projects",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Best practices for organizing your R projects",
    "text": "Best practices for organizing your R projects\n\n1. Create a project folder\nFor each research project or class, create a dedicated folder:\n\nWindowsMac\n\n\nC:\\Users\\YourName\\Documents\\POLS101\\\n\n\n/Users/YourName/Documents/POLS101/\n\n\n\n\n\n2. Use a consistent folder structure\nInside your project folder, create subfolders like:\nPOLS101/\n  ├── data/           # Store your datasets here\n  ├── scripts/        # Store your R code here\n  ├── output/         # Store graphs, tables, etc. here\n  └── documents/      # Store papers, notes, etc. here\n\n\n3. Use RStudio Projects\nRStudio has a fantastic feature called “Projects” that makes file management much easier. When you create an RStudio Project, it automatically sets your working directory to the project folder.\nHere’s how to create a new project:\n\nWindowsMac\n\n\n\nIn RStudio, go to File → New Project\nChoose “New Directory”\nChoose “New Project”\nGive it a name (like “POLS101”)\nChoose where to put it (like C:\\Users\\YourName\\Documents\\)\nClick “Create Project”\n\n\n\n\nIn RStudio, go to File → New Project\nChoose “New Directory”\nChoose “New Project”\nGive it a name (like “POLS101”)\nChoose where to put it (like /Users/YourName/Documents/)\nClick “Create Project”"
  },
  {
    "objectID": "tutorial6.html#working-with-files-in-r",
    "href": "tutorial6.html#working-with-files-in-r",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Working with files in R",
    "text": "Working with files in R\nNow let’s look at how to actually work with files in your R code:\n\nSetting your working directory\nIf you’re not using RStudio Projects, you might need to set your working directory manually:\n\n# Set working directory (adjust the path for your computer)\nsetwd(\"/Users/YourName/Documents/POLS101\")  # Mac\nsetwd(\"C:/Users/YourName/Documents/POLS101\")  # Windows (note: forward slashes work in R even on Windows!)\n\nPro tip: Notice that even on Windows, you can use forward slashes (/) in R code. This makes your code more portable between operating systems!\n\n\nLoading data with relative paths\nOnce your working directory is set correctly, you can use relative paths:\n\n# This will work for anyone who has the same folder structure\nlibrary(tidyverse)\n\n# Load data from the 'data' subfolder\nsurvey_data &lt;- read_csv(\"data/survey_results.csv\")\n\n# Create a plot and save it to the 'output' subfolder\nsurvey_data %&gt;% \n  ggplot(aes(x = age, y = income)) + \n  geom_point() + \n  ggsave(\"output/age_income_plot.png\")\n\n\n\nChecking if files exist\nBefore trying to load a file, you can check if R can find it:\n\n# Check if a file exists\nfile.exists(\"data/survey_results.csv\")\n\n# List all files in a directory\nlist.files(\"data/\")\n\n# List all CSV files in a directory\nlist.files(\"data/\", pattern = \"*.csv\")"
  },
  {
    "objectID": "tutorial6.html#practical-example-setting-up-a-research-project",
    "href": "tutorial6.html#practical-example-setting-up-a-research-project",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Practical example: Setting up a research project",
    "text": "Practical example: Setting up a research project\nLet’s walk through setting up a complete research project:\n\nStep 1: Create your project structure\n\nWindowsMac\n\n\nUsing File Explorer: 1. Navigate to C:\\Users\\YourName\\Documents\\ 2. Create a new folder called “Research_Project” 3. Inside that folder, create subfolders: data, scripts, output, documents\n\n\nUsing Finder: 1. Navigate to /Users/YourName/Documents/ 2. Create a new folder called “Research_Project” 3. Inside that folder, create subfolders: data, scripts, output, documents\n\n\n\n\n\nStep 2: Create an RStudio Project\n\nOpen RStudio\nFile → New Project → Existing Directory\nBrowse to your “Research_Project” folder\nClick “Create Project”\n\n\n\nStep 3: Test your setup\nCreate a new R script and try this code:\n\n# Check where you are\ngetwd()\n\n# Create some sample data\nlibrary(tidyverse)\nsample_data &lt;- data.frame(\n  id = 1:10,\n  score = rnorm(10, mean = 75, sd = 10)\n)\n\n# Save it to your data folder\nwrite_csv(sample_data, \"data/sample_data.csv\")\n\n# Load it back (to test that it works)\nloaded_data &lt;- read_csv(\"data/sample_data.csv\")\nprint(loaded_data)\n\n# Create a simple plot and save it\nloaded_data %&gt;% \n  ggplot(aes(x = id, y = score)) + \n  geom_point() + \n  geom_line() +\n  labs(title = \"Sample Data Plot\",\n       x = \"ID\",\n       y = \"Score\") +\n  ggsave(\"output/sample_plot.png\")\n\nprint(\"Success! Your project structure is working correctly.\")"
  },
  {
    "objectID": "tutorial6.html#troubleshooting-common-issues",
    "href": "tutorial6.html#troubleshooting-common-issues",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Troubleshooting common issues",
    "text": "Troubleshooting common issues\n\n“Cannot open file ‘filename’: No such file or directory”\nSolutions: 1. Check your working directory: getwd() 2. Check if the file exists: file.exists(\"your_filename\") 3. List files in your directory: list.files() 4. Make sure the file name is spelled correctly (including the extension!)\n\n\n“Permission denied” errors\n\nWindowsMac\n\n\n\nMake sure the file isn’t open in Excel or another program\nCheck that you have write permissions to the folder\nTry running RStudio as administrator (right-click → “Run as administrator”)\n\n\n\n\nMake sure the file isn’t open in Excel or another program\nCheck the file permissions in Finder (right-click → Get Info)\nYou might need to change the folder permissions\n\n\n\n\n\n\nPaths with spaces or special characters\nIf your folders or files have spaces in their names, you need to put quotes around the entire path:\n\n# Correct way to handle spaces in file names\ndata &lt;- read_csv(\"data/survey results 2024.csv\")\n\n# Or use underscores instead of spaces (recommended)\ndata &lt;- read_csv(\"data/survey_results_2024.csv\")"
  },
  {
    "objectID": "tutorial6.html#advanced-tips",
    "href": "tutorial6.html#advanced-tips",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Advanced tips",
    "text": "Advanced tips\n\nUsing the here package\nThe here package is fantastic for making your code work across different computers and operating systems:\n\n# Install if you don't have it\ninstall.packages(\"here\")\nlibrary(here)\n\n# The here() function automatically finds your project root\ndata &lt;- read_csv(here(\"data\", \"survey_results.csv\"))\n\n# This works the same on Windows, Mac, and Linux!\nggsave(here(\"output\", \"my_plot.png\"))\n\n\n\nEnvironment variables and shortcuts\nR has some built-in shortcuts for common locations:\n\n# Your home directory\npath.expand(\"~\")\n\n# Desktop (usually)\nfile.path(path.expand(\"~\"), \"Desktop\")\n\n# Check all your environment variables\nSys.getenv()"
  },
  {
    "objectID": "tutorial6.html#summary-and-key-takeaways",
    "href": "tutorial6.html#summary-and-key-takeaways",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Summary and key takeaways",
    "text": "Summary and key takeaways\n\nYour computer’s file system is like a filing cabinet - everything has a specific location\nWorking directory is where R is currently “standing” - check it with getwd()\nUse RStudio Projects - they make file management much easier\nOrganize your projects - create dedicated folders with consistent structure\nUse relative paths - they make your code work on different computers\nForward slashes work everywhere - even on Windows, use / in your R code\nCheck if files exist - use file.exists() and list.files() to debug issues\n\nThe most important thing is to be intentional about where you put your files and consistent in how you organize your projects. A little bit of organization at the beginning will save you hours of frustration later!\nRemember: everyone struggles with file paths and working directories when they’re learning R. Don’t get discouraged—with practice, it becomes second nature. And when in doubt, ask for help!"
  }
]