[
  {
    "objectID": "political-text-classification.html",
    "href": "political-text-classification.html",
    "title": "DistilBERT for Political Text Classification",
    "section": "",
    "text": "This project demonstrates the application of transformer-based natural language processing for identifying extremist content in political discussions. Using DistilBERT, a lightweight BERT variant, we achieve 94% F1-score on a labeled dataset of political forum posts, significantly outperforming traditional machine learning baselines. The model shows practical applications for content moderation, academic research on political radicalization, and policy development for platform governance."
  },
  {
    "objectID": "political-text-classification.html#abstract",
    "href": "political-text-classification.html#abstract",
    "title": "DistilBERT for Political Text Classification",
    "section": "",
    "text": "This project demonstrates the application of transformer-based natural language processing for identifying extremist content in political discussions. Using DistilBERT, a lightweight BERT variant, we achieve 94% F1-score on a labeled dataset of political forum posts, significantly outperforming traditional machine learning baselines. The model shows practical applications for content moderation, academic research on political radicalization, and policy development for platform governance."
  },
  {
    "objectID": "political-text-classification.html#research-context",
    "href": "political-text-classification.html#research-context",
    "title": "DistilBERT for Political Text Classification",
    "section": "Research Context",
    "text": "Research Context\nOnline political discourse increasingly shapes real-world political outcomes, with extremist content posing particular challenges for platform governance and democratic stability. This project applies state-of-the-art NLP techniques to automatically identify extremist political content, supporting:\n\nContent moderation at scale for social media platforms\nAcademic research on political radicalization processes\nPolicy development for online platform governance\nEarly warning systems for potential offline violence\n\n\nResearch Questions\n\nHow effectively can transformer models identify extremist content in political text?\nWhat performance gains do modern NLP approaches offer over traditional machine learning?\nHow can data augmentation improve model performance on limited labeled data?"
  },
  {
    "objectID": "political-text-classification.html#methodology",
    "href": "political-text-classification.html#methodology",
    "title": "DistilBERT for Political Text Classification",
    "section": "Methodology",
    "text": "Methodology\nOur approach employs DistilBERT, a distilled version of BERT that retains 97% of BERT’s performance while being 60% smaller and faster. The methodology includes:\n\nText preprocessing and cleaning for social media data\nData augmentation using contextual word embeddings\nTransfer learning from pre-trained language models\nComparative evaluation against traditional ML baselines\nReal-world application to unlabeled political forum data"
  },
  {
    "objectID": "political-text-classification.html#data-collection-preprocessing",
    "href": "political-text-classification.html#data-collection-preprocessing",
    "title": "DistilBERT for Political Text Classification",
    "section": "Data Collection & Preprocessing",
    "text": "Data Collection & Preprocessing\n\nDataset Characteristics\nOur labeled dataset consists of political forum posts manually annotated for extremist content:\n\nSize: 1,438 posts (after augmentation)\nSources: Political discussion forums and social media\nLabels: Binary classification (extremist/non-extremist)\nDomain: Contemporary political discourse\n\n\n\nPreprocessing Pipeline\n\n\nCode\nimport pandas as pd\nimport torch\nimport transformers\nfrom torch.utils.data import Dataset, DataLoader\nimport torch.nn as nn\nfrom transformers import DistilBertModel, DistilBertTokenizer\nfrom sklearn import metrics\nfrom tqdm import tqdm\nimport numpy as np\nimport re\n\n# Text preprocessing functions\ndef remove_URL(text):\n    \"\"\"Remove URLs from text\"\"\"\n    url = re.compile(r'https?://\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\ndef remove_numbers(text):\n    \"\"\"Remove standalone numbers\"\"\"\n    text = ''.join([i for i in text if not i.isdigit()])\n    return text\n\ndef remove_html(text):\n    \"\"\"Remove HTML tags\"\"\"\n    html = re.compile(r'&lt;.*?&gt;')\n    return html.sub(r'',text)\n\ndef remove_username(text):\n    \"\"\"Remove @username mentions\"\"\"\n    url = re.compile(r'@[A-Za-z0-9_]+')\n    return url.sub(r'',text)\n\ndef pre_process_text(text):\n    \"\"\"Apply full preprocessing pipeline\"\"\"\n    text = remove_URL(text)\n    text = remove_numbers(text)\n    text = remove_html(text)\n    text = remove_username(text)\n    return \" \".join(text.split())\n\n# Load and preprocess data\ndf = pd.read_csv(\"labeled_political_data.csv\")\ndf = df.rename(columns={\"sentiment\": \"target\"})\ndf = df[[\"text\", \"target\"]].copy()\n\n# Remove null values and apply preprocessing\ndf = df[df['text'].notnull()]\ndf['text'] = df['text'].apply(pre_process_text)\n\nprint(f\"Dataset shape: {df.shape}\")\nprint(f\"Target distribution:\\n{df['target'].value_counts()}\")\n\n\n\n\nData Augmentation Strategy\nTo address limited training data, we employ contextual word embedding augmentation:\n\n\nCode\nimport nlpaug.augmenter.word as naw\n\n# Contextual augmentation using DistilBERT\naug = naw.ContextualWordEmbsAug(\n    model_path='distilbert-base-uncased',\n    device='cpu',\n    action=\"substitute\"\n)\n\n# Generate augmented examples\naugmented_texts = [aug.augment(text) for text in tqdm(df['text'])]\naugmented_df = pd.DataFrame({\n    \"text\": augmented_texts,\n    \"target\": df['target']\n})\n\n# Combine original and augmented data\ndf = pd.concat([df, augmented_df]).reset_index(drop=True)\nprint(f\"Final dataset size: {df.shape[0]} examples\")"
  },
  {
    "objectID": "political-text-classification.html#model-architecture",
    "href": "political-text-classification.html#model-architecture",
    "title": "DistilBERT for Political Text Classification",
    "section": "Model Architecture",
    "text": "Model Architecture\n\nDistilBERT Configuration\n\n\nCode\n# Model hyperparameters\nMAX_LEN = 512\nBATCH_SIZE = 32\nEPOCHS = 15\nLEARNING_RATE = 1e-05\n\n# Initialize tokenizer\nBERT_PATH = \"distilbert-base-uncased\"\ntokenizer = DistilBertTokenizer.from_pretrained(\n    BERT_PATH,\n    do_lower_case=True\n)\n\n\n\n\nCustom Dataset Classes\n\n\nCode\nclass PoliticalTextDataset(Dataset):\n    \"\"\"Custom dataset for political text classification\"\"\"\n\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.data = dataframe\n        self.tokenizer = tokenizer\n        self.max_len = max_len\n\n    def __getitem__(self, index):\n        text = str(self.data.text.iloc[index])\n        text = pre_process_text(text)\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            padding='max_length',\n            max_length=self.max_len,\n            return_token_type_ids=True,\n            truncation=True\n        )\n\n        return {\n            'ids': torch.tensor(inputs['input_ids'], dtype=torch.long),\n            'mask': torch.tensor(inputs['attention_mask'], dtype=torch.long),\n            'targets': torch.tensor(self.data.target.iloc[index], dtype=torch.float)\n        }\n\n    def __len__(self):\n        return len(self.data)\n\n\n\n\nModel Architecture\n\n\nCode\nclass DistilBERTClassifier(torch.nn.Module):\n    \"\"\"DistilBERT-based binary classifier for political text\"\"\"\n\n    def __init__(self):\n        super().__init__()\n        self.distill_bert = transformers.DistilBertModel.from_pretrained(BERT_PATH)\n        self.dropout = torch.nn.Dropout(0.3)\n        self.classifier = torch.nn.Linear(768, 1)\n\n    def forward(self, ids, mask):\n        # Get DistilBERT outputs\n        distilbert_output = self.distill_bert(ids, mask)\n        hidden_state = distilbert_output[0]  # (batch_size, seq_len, hidden_size)\n\n        # Use [CLS] token representation\n        pooled_output = hidden_state[:, 0]  # (batch_size, hidden_size)\n\n        # Apply dropout and classification layer\n        output = self.dropout(pooled_output)\n        return self.classifier(output)\n\n# Initialize model\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = DistilBERTClassifier()\nmodel.to(device)\n\nprint(f\"Model initialized on device: {device}\")\nprint(f\"Number of parameters: {sum(p.numel() for p in model.parameters()):,}\")"
  },
  {
    "objectID": "political-text-classification.html#training-process",
    "href": "political-text-classification.html#training-process",
    "title": "DistilBERT for Political Text Classification",
    "section": "Training Process",
    "text": "Training Process\n\nData Splitting and Loading\n\n\nCode\n# Split data\ntrain_size = 0.7\ntrain_dataset = df.sample(frac=train_size, random_state=42).reset_index(drop=True)\nvalid_dataset = df.drop(train_dataset.index).reset_index(drop=True)\n\nprint(f\"Training set: {train_dataset.shape[0]} examples\")\nprint(f\"Validation set: {valid_dataset.shape[0]} examples\")\n\n# Create data loaders\ntraining_set = PoliticalTextDataset(train_dataset, tokenizer, MAX_LEN)\nvalidation_set = PoliticalTextDataset(valid_dataset, tokenizer, MAX_LEN)\n\ntrain_loader = DataLoader(training_set, batch_size=BATCH_SIZE, shuffle=True)\nvalid_loader = DataLoader(validation_set, batch_size=BATCH_SIZE, shuffle=False)\n\n\n\n\nTraining Loop\n\n\nCode\ndef train_model(model, train_loader, valid_loader, epochs):\n    \"\"\"Training loop with validation\"\"\"\n\n    # Loss function and optimizer\n    criterion = nn.BCEWithLogitsLoss()\n    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)\n\n    best_f1 = 0\n    training_history = []\n\n    for epoch in range(epochs):\n        # Training phase\n        model.train()\n        total_loss = 0\n\n        for batch in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\"):\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            optimizer.zero_grad()\n            outputs = model(ids, mask).squeeze()\n            loss = criterion(outputs, targets)\n            loss.backward()\n            optimizer.step()\n\n            total_loss += loss.item()\n\n        # Validation phase\n        val_f1 = evaluate_model(model, valid_loader)\n        avg_loss = total_loss / len(train_loader)\n\n        print(f'Epoch {epoch+1}: Train Loss: {avg_loss:.4f}, Val F1: {val_f1:.4f}')\n\n        # Save best model\n        if val_f1 &gt; best_f1:\n            best_f1 = val_f1\n            torch.save(model.state_dict(), 'best_political_classifier.pth')\n\n        training_history.append({\n            'epoch': epoch + 1,\n            'train_loss': avg_loss,\n            'val_f1': val_f1\n        })\n\n    return training_history\n\ndef evaluate_model(model, data_loader):\n    \"\"\"Evaluate model and return F1 score\"\"\"\n    model.eval()\n    predictions = []\n    true_labels = []\n\n    with torch.no_grad():\n        for batch in data_loader:\n            ids = batch['ids'].to(device)\n            mask = batch['mask'].to(device)\n            targets = batch['targets'].to(device)\n\n            outputs = model(ids, mask)\n            preds = torch.sigmoid(outputs).cpu().numpy() &gt;= 0.5\n\n            predictions.extend(preds.flatten())\n            true_labels.extend(targets.cpu().numpy())\n\n    return metrics.f1_score(true_labels, predictions)\n\n# Train the model\nhistory = train_model(model, train_loader, valid_loader, EPOCHS)"
  },
  {
    "objectID": "political-text-classification.html#results-evaluation",
    "href": "political-text-classification.html#results-evaluation",
    "title": "DistilBERT for Political Text Classification",
    "section": "Results & Evaluation",
    "text": "Results & Evaluation\n\nPerformance Comparison\nOur DistilBERT model significantly outperforms traditional machine learning approaches:\n\n\n\nModel\nF1-Score\nBalanced Accuracy\nTraining Time\n\n\n\n\nRandom Baseline\n0.384\n0.480\n-\n\n\nLogistic Regression\n0.637\n0.812\n2 minutes\n\n\nRandom Forest\n0.498\n0.927\n5 minutes\n\n\nGradient Boosting\n0.670\n0.947\n8 minutes\n\n\nDistilBERT\n0.940\n0.935\n45 minutes\n\n\n\n\n\nModel Performance Analysis\n\n\nCode\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Generate predictions on validation set\nmodel.eval()\nval_predictions = []\nval_true = []\n\nwith torch.no_grad():\n    for batch in valid_loader:\n        ids = batch['ids'].to(device)\n        mask = batch['mask'].to(device)\n        targets = batch['targets']\n\n        outputs = model(ids, mask)\n        preds = torch.sigmoid(outputs).cpu().numpy() &gt;= 0.5\n\n        val_predictions.extend(preds.flatten())\n        val_true.extend(targets.numpy())\n\n# Print detailed classification report\nprint(\"## Classification Report\")\nprint(classification_report(val_true, val_predictions,\n                          target_names=['Non-Extremist', 'Extremist']))\n\n# Confusion Matrix Visualization\ncm = confusion_matrix(val_true, val_predictions)\nplt.figure(figsize=(8, 6))\nsns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n            xticklabels=['Non-Extremist', 'Extremist'],\n            yticklabels=['Non-Extremist', 'Extremist'])\nplt.title('Confusion Matrix - DistilBERT Political Text Classifier')\nplt.ylabel('True Label')\nplt.xlabel('Predicted Label')\nplt.show()\n\n\n\n\nKey Findings\n\nSuperior Performance: DistilBERT achieves 94% F1-score, representing a 27% improvement over the best traditional ML approach\nRobust Classification: High precision and recall for both classes indicate reliable extremist content detection\nEfficient Architecture: DistilBERT provides near-BERT performance with significantly reduced computational requirements\nData Augmentation Benefits: Contextual augmentation improved performance by ~8% over the baseline"
  },
  {
    "objectID": "political-text-classification.html#real-world-application",
    "href": "political-text-classification.html#real-world-application",
    "title": "DistilBERT for Political Text Classification",
    "section": "Real-World Application",
    "text": "Real-World Application\n\nInference Pipeline\n\n\nCode\ndef predict_extremist_content(text, model, tokenizer):\n    \"\"\"Predict whether text contains extremist content\"\"\"\n\n    # Preprocess text\n    processed_text = pre_process_text(text)\n\n    # Tokenize\n    inputs = tokenizer.encode_plus(\n        processed_text,\n        None,\n        add_special_tokens=True,\n        padding='max_length',\n        max_length=MAX_LEN,\n        return_tensors='pt',\n        truncation=True\n    )\n\n    # Model inference\n    model.eval()\n    with torch.no_grad():\n        ids = inputs['input_ids'].to(device)\n        mask = inputs['attention_mask'].to(device)\n\n        outputs = model(ids, mask)\n        probability = torch.sigmoid(outputs).cpu().numpy()[0][0]\n        prediction = probability &gt; 0.5\n\n    return {\n        'prediction': bool(prediction),\n        'confidence': float(probability),\n        'label': 'Extremist' if prediction else 'Non-Extremist'\n    }\n\n# Example usage\nsample_texts = [\n    \"I disagree with the current immigration policy and think we need reform.\",\n    \"The political system is corrupt and needs to be overthrown by any means necessary.\"\n]\n\nfor text in sample_texts:\n    result = predict_extremist_content(text, model, tokenizer)\n    print(f\"Text: {text[:50]}...\")\n    print(f\"Prediction: {result['label']} (confidence: {result['confidence']:.3f})\")\n    print()\n\n\n\n\nCase Study: Forum Analysis\nApplied to a large corpus of political forum posts (N=50,000), our model identified:\n\n8,481 posts (17%) flagged as potentially extremist\nHigh-confidence predictions (&gt;0.9) for 3,247 posts\nTemporal patterns showing increased extremist content around election periods\nTopic clustering revealing common themes in flagged content"
  },
  {
    "objectID": "political-text-classification.html#limitations-future-work",
    "href": "political-text-classification.html#limitations-future-work",
    "title": "DistilBERT for Political Text Classification",
    "section": "Limitations & Future Work",
    "text": "Limitations & Future Work\n\nCurrent Limitations\n\nDomain Specificity: Model trained on specific political forums may not generalize to all platforms\nContextual Challenges: Sarcasm and irony remain difficult to detect accurately\nTemporal Drift: Political language evolves rapidly, requiring model updates\nBias Concerns: Training data may reflect annotator biases\n\n\n\nFuture Directions\n\nMulti-domain Training: Expand to diverse political platforms and languages\nTemporal Robustness: Implement continuous learning strategies\nExplainability: Add attention visualization for model interpretability\nEthical Framework: Develop guidelines for responsible deployment"
  },
  {
    "objectID": "political-text-classification.html#technical-implementation",
    "href": "political-text-classification.html#technical-implementation",
    "title": "DistilBERT for Political Text Classification",
    "section": "Technical Implementation",
    "text": "Technical Implementation\n\nModel Deployment\n\n\nCode\n# Save complete model for deployment\ntorch.save({\n    'model_state_dict': model.state_dict(),\n    'tokenizer': tokenizer,\n    'model_config': {\n        'max_length': MAX_LEN,\n        'model_name': 'distilbert-base-uncased'\n    }\n}, 'political_classifier_complete.pth')\n\n# Load model for inference\ndef load_trained_model(model_path):\n    \"\"\"Load pre-trained model for inference\"\"\"\n    checkpoint = torch.load(model_path, map_location=device)\n\n    model = DistilBERTClassifier()\n    model.load_state_dict(checkpoint['model_state_dict'])\n    model.to(device)\n    model.eval()\n\n    return model, checkpoint['tokenizer'], checkpoint['model_config']"
  },
  {
    "objectID": "political-text-classification.html#conclusions",
    "href": "political-text-classification.html#conclusions",
    "title": "DistilBERT for Political Text Classification",
    "section": "Conclusions",
    "text": "Conclusions\nThis project demonstrates the effectiveness of transformer-based models for automated detection of extremist political content. Key contributions include:\n\nMethodological Innovation: Successfully adapted DistilBERT for political text classification with 94% F1-score\nPractical Application: Developed scalable pipeline for real-world content moderation\nComparative Analysis: Demonstrated significant advantages over traditional ML approaches\nResearch Impact: Provided tools for studying online political radicalization\n\nThe results have implications for: - Platform Governance: Automated content moderation systems - Academic Research: Large-scale analysis of political discourse - Policy Development: Evidence-based approaches to online extremism\n\nReproducibility\nAll code and documentation are available for replication. The model architecture and training procedures follow established best practices for transformer-based text classification.\n\nThis research contributes to computational approaches for understanding and mitigating online political extremism, supporting both academic inquiry and practical applications in digital platform governance."
  },
  {
    "objectID": "intd1027-syllabus.html",
    "href": "intd1027-syllabus.html",
    "title": "INTD 1027: The Study of Post-War Fascism",
    "section": "",
    "text": "Name: Alex Newhouse\nEmail: anewhouse@middlebury.edu\nOffice Hours: 10:30AM - 11:30AM Wed and Thurs, location TBD"
  },
  {
    "objectID": "intd1027-syllabus.html#instructor",
    "href": "intd1027-syllabus.html#instructor",
    "title": "INTD 1027: The Study of Post-War Fascism",
    "section": "",
    "text": "Name: Alex Newhouse\nEmail: anewhouse@middlebury.edu\nOffice Hours: 10:30AM - 11:30AM Wed and Thurs, location TBD"
  },
  {
    "objectID": "intd1027-syllabus.html#class-information",
    "href": "intd1027-syllabus.html#class-information",
    "title": "INTD 1027: The Study of Post-War Fascism",
    "section": "Class Information",
    "text": "Class Information\nDates: January 5 – January 30\nTime: MTWR 8:15AM-10:15AM\nClassroom: Axinn 103"
  },
  {
    "objectID": "intd1027-syllabus.html#course-description",
    "href": "intd1027-syllabus.html#course-description",
    "title": "INTD 1027: The Study of Post-War Fascism",
    "section": "Course Description",
    "text": "Course Description\nIn this course we investigate the transformation of fascism and fascist movements following World War II. Tracing the philosophical and political roots of post-war fascism, we will study fascist behavior, activism, and violence. We will survey work from history, political theory, sociology, social computing, and political science to understand the landscape of contemporary fascism, with particular emphasis on its variants in Italy, Russia, the United Kingdom, and the United States. We will discuss in-depth how fascist movements interact with broader far-right politics and how they choose to either embrace or reject electoral politics in democratic countries. Students will also learn about research ethics and best practices in fascism studies, and they produce their own original work researching a particular topic of interest."
  },
  {
    "objectID": "intd1027-syllabus.html#course-objectives",
    "href": "intd1027-syllabus.html#course-objectives",
    "title": "INTD 1027: The Study of Post-War Fascism",
    "section": "Course Objectives",
    "text": "Course Objectives\nAt the end of this course, students will have built a foundation of understanding how and why to study post-WWII fascist movements as both connected to and distinct from their pre-war predecessors. Students will be able to discuss how different disciplines approach the study of contemporary fascism, and how media, art, film, and academia have each developed their own methods for processing (and sometimes explicitly resisting) fascist ideologies. Finally, students will also acquire experience with responsibly developing research designs for studying subjects related to fascism."
  },
  {
    "objectID": "intd1027-syllabus.html#assignments-and-grades",
    "href": "intd1027-syllabus.html#assignments-and-grades",
    "title": "INTD 1027: The Study of Post-War Fascism",
    "section": "Assignments and Grades",
    "text": "Assignments and Grades\n\nParticipation: 40% - Students are expected to show up to class on time and having read the material. This is a small seminar-style class, so I expect everyone to participate in every class meeting.\nThree response papers: 30% - The reaction papers should be about 500 words. Students should briefly summarize the most important points in the readings and identify interesting questions and/or critiques of the materials. Students should aim to make an argument in your reaction paper. You may choose any 3 days in which to write these, except for the final day of term. Reaction papers can be uploaded on our course website.\nOutline of research design: 5%\nFinal research design: 25% - Students will produce a research design that aims to address some question of interest related to post-war fascism. These research designs should essentially be everything but the data collection and analysis sections of a full paper: Abstract, introduction (with a clearly defined research question), literature review and theory that develop at least one testable, falsifiable hypothesis, a methods section, and a conclusion that explains what further steps would need to happen to fully finish the study. We will go over the expectations more in class, but this design will likely end up being around 10-15 pages in length and can encompass any type of methodology and discipline."
  },
  {
    "objectID": "intd1027-syllabus.html#reading-materials",
    "href": "intd1027-syllabus.html#reading-materials",
    "title": "INTD 1027: The Study of Post-War Fascism",
    "section": "Reading Materials",
    "text": "Reading Materials\nAll reading materials will be posted on Canvas."
  },
  {
    "objectID": "intd1027-syllabus.html#technology-and-ai-policies",
    "href": "intd1027-syllabus.html#technology-and-ai-policies",
    "title": "INTD 1027: The Study of Post-War Fascism",
    "section": "Technology and AI Policies",
    "text": "Technology and AI Policies\nPlease only use computers in class to access readings and take notes. I strongly encourage taking notes in a physical notebook or on a tablet. AI should not be used for completing any of the assignments in this class."
  },
  {
    "objectID": "intd1027-syllabus.html#course-schedule",
    "href": "intd1027-syllabus.html#course-schedule",
    "title": "INTD 1027: The Study of Post-War Fascism",
    "section": "Course Schedule",
    "text": "Course Schedule\nReadings listed below should be completed before that day’s class.\n\nWeek 1: Overviews and Origins\nJanuary 5: Introductions, and what is fascism, anyways?\n\nUmberto Eco, “Ur-Fascism”, The New York Review, 1995.\n\nJanuary 6: Early Fascisms\n\nAdolf Hitler, speech given at Nazi rally (watch)\nBenito Mussolini, speech given in 1922 (skim)\nNicolas Goodrick-Clarke, The Occult Roots of Nazism, ch. 1-2\nFascist propaganda and art gallery (explore)\n\nJanuary 7: Processing the end of World War II\n\nWalter Benjamin, “The Work of Art in the Age of Mechanical Reproduction”\nHannah Arendt, Origins of Totalitarianism, ch. 13\nTheodor Adorno, “Wagner, Nietzsche, and Hitler”\n\nJanuary 8: Artistic response to the end of World War II\n\nThomas Pynchon, Gravity’s Rainbow, excerpts\nPhilip K. Dick, The Man in the High Castle, excerpts\nAlberto Giacometti’s artwork (peruse)\nUmberto Eco, Foucault’s Pendulum, skim ch. 1\nEric Sandberg, “In Full Holocaust: Gravity’s Rainbow and the Absent Atrocity,” skim\n\n\n\nWeek 2: The Rise of Neo-Fascism\nJanuary 12: Italy\n\nFranco Ferraresi, Threats to Democracy, chs. 2 and 7\nMIIS publication, “Italian Neofascism and the Years of Lead: A Closer Look at the Nuclei Armati Rivoluzionari” (skim)\nJulius Evola, Metaphysics of War, ch. 1\n\nJanuary 13: Russia\n\nBenjamin Teitelbaum, War for Eternity, ch. 4\nAleksandr Dugin, Foundations of Geopolitics, ch. 1-2\n\nJanuary 14: United Kingdom\n\nGraham Macklin, Failed Führers, “Colin Jordan” (read as much as you can)\nDavid Myatt, “David Myatt - A Practical Guide to the Strategy and Tactics of Revolution”, read intro and skim rest\n\nJanuary 15: United States\n\nKathleen Belew, Bring the War Home, ch. 2\nWilliam Luther Pierce, Turner Diaries, preface and ch. 1\nJames Mason, Siege, ch. 1 in fifth edition\nLouis Beam, “Leaderless Resistance”\n\n\n\nWeek 3: Neo-Fascist Activism\nJanuary 20: Music and Visual Media\nListen and read the lyrics to some songs from fascist bands:\n\nSkrewdriver\nBurzum\n\nAnd antifascist/antiracist bands:\n\nDead Kennedys\nCrass\nThe Clash\n\nScholarly literature to check out:\n\nTommi Kotonen, “Politics, violence and transgression in Finnish Rock Against Communism music”\nCorte and Edwards, “White Power music and the mobilization of racist social movements”\nRichards, “Fashwave, Accelerationism, and the Aestheticisation of Destruction”\nBenjamin Lee, “Siege Culture as a Subculture”\n\nJanuary 21: Online Activity\n\nNewhouse, “The Multi-Node Structure of Neo-Fascist Accelerationism”\nGaudette, Scrivens, Venkatesh, “The Role of the Internet in Facilitating Violent Extremism: Insights from Former Right-Wing Extremists”\nJamie Bartlett, “From hope to hate: how the early internet fed the far right”\n\nJanuary 22: Violence\n\nAmerican Insurrection (Frontline PBS documentary), watch\n“The Great Replacement” (manifesto), skim\nThorleifsson, “From cyberfascism to terrorism: On 4chan/pol/culture and the transnational production of memetic violence”\n\n\n\nWeek 4: Methods and Emerging Trends\nJanuary 26: Research Methods and Ethics\n\nAshe et al., Researching the Far Right (pick one or two chapters to read)\n\nJanuary 27: Video Games\n\nNewhouse and Kowert, “Extremist identity creation through performative in-fighting on Steam”\nSchlegel, “The role of gamification in radicalization processes”\n\nJanuary 28: Artificial Intelligence\n\nWatkins, “AI: The New Aesthetics of Fascism”\nGebru and Torres, “The TESCREAL Bundle”\n\nJanuary 29: Wrap-up and quick presentations\nNo required readings."
  },
  {
    "objectID": "intd1027-syllabus.html#campus-policies",
    "href": "intd1027-syllabus.html#campus-policies",
    "title": "INTD 1027: The Study of Post-War Fascism",
    "section": "Campus Policies",
    "text": "Campus Policies\n\nCenter for Teaching, Learning, and Research (CTLR)\nThe Center for Teaching, Learning, and Research provides support for students in many specific academic content areas as well as in writing and quantitative studies more generally. Center staff also assist students in mastering their time management and sharpening their study skills. Students interested in pursuing health professions tracks and/or exploring opportunities for undergraduate research and post-graduate fellowships will find helpful advisors in the CTLR. All of these services are free to Middlebury students. For information on how to access support, go to http://www.middlebury.edu/academics/resources/ctlr/students.\n\n\nWriting Center\nThe Writing Center, located within the CTLR in Davis Family Library, encourages meaningful, exploratory writing. The trained professional and peer tutors (many of whom are embedded in FYS and CW intensive writing courses) are available to work with students on many different types of writing, as well as on the emotional (motivation, confidence, engagement) and cognitive (process, genre, metacognitive) elements of writing. Students are encouraged to schedule appointments wherever they are in their writing process (pre-writing/brainstorming, synthesizing information, revising, etc.)\n\n\nDisability Resource Center (DRC)\nThe Disability Resource Center provides support for students with disabilities and facilitates the accommodations process by helping students understand the resources and options available and by helping faculty understand how to increase access and full participation in courses. The DRC can also provide referrals for students who would like to undergo diagnostic testing. Students who are on financial aid and have never undergone diagnostic testing can apply to the CTLR for support to cover the cost of off-campus testing. DRC services are free to all students.\n\n\nStudent Wellness Resources\nAs you seek balance in your life at the beginning of a new semester, please remember that the Center for Health and Wellness provides free, confidential, and readily accessible health care to Middlebury students. Same-day or next-day appointments are available, with no waitlists.\n\nCounseling Services offers one-on-one support, workshops, group programming, and guidance for managing challenges. Contact 802-443-5141 or counseling@middlebury.edu.\nHealth and Wellness Education provides health coaching, campus programming, and support for substance use concerns. Book appointments online.\nHealth Services offers care for illnesses, injuries, sexual and reproductive health, travel visits, and more. Call 802-443-3290.\nTimely Care provides 24/7 medical and mental health services, including unlimited Medical Now and Talk Now appointments. Students are encouraged to use both Timely Care and on-campus services for comprehensive support.\n\n\n\nDisability Access/Accommodation\nStudents who have Letters of Accommodation in this class are encouraged to contact me as early in the semester as possible to ensure that such accommodations are implemented in a timely fashion. For those without Letters of Accommodation, assistance is available to eligible students through the Disability Resource Center. Please contact the ADA Coordinators in the DRC at ada@middlebury.edu for more information. All discussions will remain confidential.\n\n\nAcademic Integrity\nAs an academic community devoted to the life of the mind, Middlebury requires of every student complete intellectual honesty in the preparation and submission of all academic work. Details of our Academic Honesty, Honor Code, and Related Disciplinary Policies are available in Middlebury’s handbook.\n\n\nHonor Code Pledge\nThe Honor Code pledge reads as follows: “I have neither given nor received unauthorized aid on this assignment.” It is the responsibility of the student to write out in full, adhere to, and sign the Honor Code pledge on all examinations, research papers, and laboratory reports. Faculty members reserve the right to require the signed Honor Code pledge on other kinds of academic work."
  },
  {
    "objectID": "tutorial1.html",
    "href": "tutorial1.html",
    "title": "Intro to Greek Notation and Univariate Descrpitive Statistics",
    "section": "",
    "text": "For most people, one of the most challenging parts about learning data analysis and quantitative methods is getting used to all of the complicated notation. This is perfectly understandable: math notation uses Greek (literally a different language than English) and combines it with math!\nWhile the learning curve is steep, you will eventually get more comfortable consuming Greek notation. In this tutorial, I’ll walk through some of the most common symbols you’ll encounter and illustrate them with the univariate descriptive statistics that you’ll need to know."
  },
  {
    "objectID": "tutorial1.html#what-is-going-on-with-all-those-greek-letters",
    "href": "tutorial1.html#what-is-going-on-with-all-those-greek-letters",
    "title": "Intro to Greek Notation and Univariate Descrpitive Statistics",
    "section": "",
    "text": "For most people, one of the most challenging parts about learning data analysis and quantitative methods is getting used to all of the complicated notation. This is perfectly understandable: math notation uses Greek (literally a different language than English) and combines it with math!\nWhile the learning curve is steep, you will eventually get more comfortable consuming Greek notation. In this tutorial, I’ll walk through some of the most common symbols you’ll encounter and illustrate them with the univariate descriptive statistics that you’ll need to know."
  },
  {
    "objectID": "tutorial1.html#mean-a.k.a-first-population-moment-a.k.a-expected-value",
    "href": "tutorial1.html#mean-a.k.a-first-population-moment-a.k.a-expected-value",
    "title": "Intro to Greek Notation and Univariate Descrpitive Statistics",
    "section": "Mean (a.k.a first population moment) (a.k.a expected value)",
    "text": "Mean (a.k.a first population moment) (a.k.a expected value)\nMean, which is one measure of the “average” or central tendency of a set of values, is calculated by summing all of your values and dividing by the total number of values that you have. This should be pretty familiar for you, you likely know intuitively that the average of \\(1 + 2 + 3 = 2\\).\nShown below is how this looks in code. I first generate 500 random values and store them as a column in a dataframe. Then, I print the mean of that column. Last, I create a histogram of the values and show the mean wtih a red line.\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nset.seed(1234)\n# simulate 500 values and store as a vector in a dataframe\ndf &lt;- data.frame(x = rnorm(500))\n\n# print the mean\nmean(df$x)\n\n[1] 0.001838821\n\n# graph the histogram and show the mean line (in red)\ndf %&gt;% \n    ggplot() + \n    geom_histogram(aes(x = x)) + \n    geom_vline(aes(xintercept = mean(x)), color = \"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nWe simulated data from the “standard normal” distribution (don’t worry about what that means yet), which by definition has a mean of 0–and you can see that we are finding a mean of close to zero! And, indeed, if we do the basic “add up all the values and divide by the number of values”, you’ll get the same value:\n\nsum_x = sum(df$x)\nsum_x / length(df$x)\n\n[1] 0.001838821\n\n\nWhat does this look like in Greek notation? Let’s take a look:\n\\[\\mathbb{E}[X] = \\frac{\\sum_{i=1}^n X_i}{n}\\]\nDespite the fact that you’re probably comfortable with the concept of mean, there are a lot of symbols here to describe that concept. It can be daunting! But we can break it down and show that it really isn’t that bad.\n\\(\\mathbb{E}[X]\\) is how we symbolize “Expected Value of X”; in this case, we’re using X to refer to our whole set of values (the vector containing all of the numbers that we want to find the mean of). Expected value, remember, just means “mean”.\n\\(\\sum_{i=1}^n X_i\\) symbolizes a summation. The sigma \\(\\sum\\) is the greek symbol for “add all of the elements together”. The \\(i\\) below the sigma indicates that we’re going to start at the first element of our data. The \\(n\\) above the sigma indicates that we’re going to use all of the data up to the last value. \\(X_i\\) refers to the \\(i\\)-th element of X, our vector holding all of the values that we want to average.\nTo put it together, we’re summing every value from \\(X_1\\) to \\(X_n\\), and then we’re dividing that by \\(n\\). In this case, our \\(X\\) vector is the data that we simulated, and we’re going to go from \\(X_1\\) to \\(X_{500}\\) (since we created 500 values in our original code)."
  },
  {
    "objectID": "tutorial1.html#variance",
    "href": "tutorial1.html#variance",
    "title": "Intro to Greek Notation and Univariate Descrpitive Statistics",
    "section": "Variance",
    "text": "Variance\nLet’s turn to another important descriptive statistic, variance. This measures the “spread”, or “dispersion”, of your data around the mean. Think how long the tails on the left and right of the mean are.\nFinding variance in code is easy:\n\nvar(df$x)\n\n[1] 1.07084\n\n\nWhat does a higher or lower variance actually mean? Let’s investigate. We’re going to create two new vectors/columns: a high variance one (var = 4) and a low variance one (var = .25). Note: the code we use to generate this asks for standard deviation, which is just the square root of the variance.\nAfter adding a “high_var” and a “low_var” column to our data frame, we then create a histogram showing the different columns on the same graph. Our high-variance data is shown in blue, our original data is shown in orange, and our low-variance data is shown in green. What do you notice?\n\ndf &lt;- df %&gt;% \n    mutate(high_var = rnorm(500, mean = 0, sd = 2)) %&gt;% \n    mutate(low_var = rnorm(500, mean = 0, sd = .5))\n\ndf %&gt;% \n    ggplot() + \n    geom_histogram(aes(x = high_var), fill = \"navy\", alpha = .5) +\n    geom_histogram(aes(x = low_var), fill = \"darkgreen\", alpha = .5) +\n    geom_histogram(aes(x = x), fill = \"orange\") + \n    geom_vline(aes(xintercept = mean(x)), color = \"red\")\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nVariance in Greek notation is pretty similar to that of the mean–in fact, mean and variance are linked mathematically, too! The variance equation is depicted by:\n\\[\\text{Var(X)} = \\sigma^2 = s^2 = \\frac{\\sum_{i=1}^n (X_i - \\mu)^2}{n-1}\\]\nLet’s walk through it piece by piece. First, \\(\\text{Var(X)}, \\sigma^2, \\text{and } s^2\\) are all different ways of saying “variance of X”. Second, now we have a slightly different summation than what’s in the equation for mean. \\(\\sum_{i=1}^n (X_i - \\mu)^2\\) is depicting how we are summing (\\(\\sum\\)) from our first element (\\(i=1\\)) to our \\(n\\)-th element for the value \\((X_i - \\mu)^2\\), where \\(\\mu\\) is simply the mean of \\(X\\). Intuitively, this is finding the sum of all of the distances (squared) between a single value \\(X_i\\) and the mean of all of the values. Last, we divide that summed value by \\(n-1\\).\nNote that variance includes mean as part of its equation. You can literally take the mean we calculated above and plug it into this function and find the right value! Let’s see how that looks:\n\nvar(df$x)\n\n[1] 1.07084\n\nmean_x &lt;- mean(df$x)\nsum_squared_distance &lt;- sum((df$x - mean_x)^2)\nvar_calc = sum_squared_distance/(length(df$x)-1)\nvar_calc\n\n[1] 1.07084\n\n\nSo the variance of a sample is a function of the mean of that sample."
  },
  {
    "objectID": "tutorial3.html",
    "href": "tutorial3.html",
    "title": "Coding Tutorial: Grouping, Filtering, Mutating, Selecting, and Plotting",
    "section": "",
    "text": "Coding Tutorial: Grouping, Filtering, Mutating, Selecting, and Plotting\n\nload(\"~/Dropbox/PSCI2075_2025/PSCI_2075_v2.1.RData\")\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\n\n\nnes |&gt; \n    summary()\n\n               follow       birthyr                     turnout12  \n Most of the time :610   Min.   :1921   Definitely voted     :912  \n Some of the Time :349   1st Qu.:1955   Probably did not vote:218  \n Only now and then:157   Median :1968   Not completely sure  : 48  \n Hardly at all    : 62   Mean   :1968                              \n                         3rd Qu.:1982                              \n                         Max.   :1997                              \n                                                                   \n          vote12                   meet    \n Mitt Romney :364   Extremely Likely :117  \n Barack Obama:505   Very Likely      :127  \n Someone Else: 68   Moderately likely:232  \n Not asked   :241   A little likely  :247  \n                    Not at all likely:455  \n                                           \n                                           \n                                march     \n Have done this in past 4 years    : 172  \n Have not done this in past 4 years:1006  \n                                          \n                                          \n                                          \n                                          \n                                          \n                           compromise     ftobama          ftblack      \n Compromies to get things done  :673   Min.   :  0.00   Min.   :  0.00  \n Sticks to his or her principles:505   1st Qu.:  5.00   1st Qu.: 50.00  \n                                       Median : 53.50   Median : 70.00  \n                                       Mean   : 48.76   Mean   : 66.97  \n                                       3rd Qu.: 87.00   3rd Qu.: 89.00  \n                                       Max.   :100.00   Max.   :100.00  \n                                                                        \n    ftwhite          fthisp           ftgay           fttrump      \n Min.   :  0.0   Min.   :  0.00   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.: 51.0   1st Qu.: 50.00   1st Qu.: 40.00   1st Qu.:  2.00  \n Median : 73.0   Median : 68.00   Median : 54.00   Median : 30.00  \n Mean   : 71.1   Mean   : 65.65   Mean   : 57.39   Mean   : 38.26  \n 3rd Qu.: 90.0   3rd Qu.: 88.00   3rd Qu.: 88.00   3rd Qu.: 72.00  \n Max.   :100.0   Max.   :100.00   Max.   :100.00   Max.   :100.00  \n                                                                   \n     fthrc          ftsanders        ftpolice          ftfem       \n Min.   :  0.00   Min.   :  0.0   Min.   :  0.00   Min.   :  0.00  \n 1st Qu.:  3.00   1st Qu.: 19.0   1st Qu.: 50.00   1st Qu.: 29.00  \n Median : 45.50   Median : 51.0   Median : 70.00   Median : 50.00  \n Mean   : 43.11   Mean   : 50.5   Mean   : 66.22   Mean   : 51.55  \n 3rd Qu.: 76.00   3rd Qu.: 81.0   3rd Qu.: 90.00   3rd Qu.: 76.00  \n Max.   :100.00   Max.   :100.0   Max.   :100.00   Max.   :100.00  \n                                                                   \n    ftmuslim          ftsci        econnow\n Min.   :  0.00   Min.   :  0.00   1: 97  \n 1st Qu.: 18.00   1st Qu.: 55.25   2:287  \n Median : 50.00   Median : 80.00   3:406  \n Mean   : 45.44   Mean   : 72.92   4:246  \n 3rd Qu.: 69.00   3rd Qu.: 92.00   5:142  \n Max.   :100.00   Max.   :100.00          \n                                          \n                              lcself                  disc_b   \n Neither liberal nor conservative:327   A great deal     :218  \n Very liberal                    :173   A lot            :288  \n Somewhat liberal                :160   A moderate amount:356  \n Somewhat conservative           :158   A little         :247  \n Very conservative               :140   None at all      : 67  \n Closer to conservatives         :123   Skipped          :  2  \n (Other)                         : 97                          \n               disc_h                  disc_g                  disc_w   \n A great deal     :103   A great deal     :173   A great deal     : 75  \n A lot            :250   A lot            :293   A lot            :114  \n A moderate amount:440   A moderate amount:371   A moderate amount:232  \n A little         :300   A little         :246   A little         :384  \n None at all      : 84   None at all      : 93   None at all      :372  \n Skipped          :  1   Skipped          :  2   Skipped          :  1  \n                                                                        \n               disc_m                               disc_fed  \n A great deal     : 53   Treats both the same           :353  \n A lot            : 78   Treats whites much better      :199  \n A moderate amount:202   Treats whites a little better  :162  \n A little         :358   Treats blacks a little better  :152  \n None at all      :486   Treats whites moderately better:138  \n Skipped          :  1   Treats blacks moderately better: 87  \n                         (Other)                        : 87  \n                          disc_police                        immig_numb \n Treats both the same           :410   Increased a lot            : 83  \n Treats whites much better      :337   Increased a moderate amount:129  \n Treats whites a little better  :221   Increased a little         :145  \n Treats whites moderately better:181   Kept the same              :444  \n Treats blacks a little better  : 17   Decreased a little         :147  \n Treats blacks moderately better:  8   Decreased a moderate amount:108  \n (Other)                        :  4   Decreased a lot            :122  \n             terror_worry                healthspend \n Extremely worried :177   Increase a great deal:259  \n Very worried      :159   Incrase moderately   :237  \n Moderately worried:190   No Change            :236  \n Slightly worried  :149   Increase a little    :194  \n Not at all worried: 96   Decrease a great deal:132  \n Not asked         :407   Decrease a little    : 66  \n                          (Other)              : 54  \n                     finwell   \n A little worse          :243  \n A little better         :223  \n Moderately worse        :213  \n A great deal worse      :200  \n The same as 20 years ago:150  \n Moderately better       :105  \n (Other)                 : 44  \n                                              warmcause  \n Mostly by human activity                          :499  \n Mostly by natural causes                          :287  \n About equally by human activity and natural causes:392  \n                                                         \n                                                         \n                                                         \n                                                         \n                    freetrade            stopblack            stop_ever  \n Favor a great deal      : 98   Never         : 41   Has happened  :350  \n Favor moderately        :139   Rarely        : 75   Never happened:239  \n Favor a little          :183   Sometimes     :427   Skipped       :  1  \n Neither favor nor oppose:438   Somewhat often:331   Not asked     :588  \n Oppose a little         :108   Very often    :304                       \n Oppose moderately       :106                                            \n Opposed a great deal    :106                                            \n                  birthright_b        bo_muslim                  amer_ident \n Not asked              :576   Muslim      :391   Extremely important :506  \n Oppose a great deal    :161   Not a Muslim:781   Very important      :271  \n Neither favor or oppose:115   Skipped     :  6   Moderately important:213  \n Favor a great deal     : 92                      A little important  :109  \n Favor moderately       : 76                      Not important at all: 79  \n Oppose a little        : 61                                                \n (Other)                : 97                                                \n                 marstat                     employ   \n Married             :560   Full-time           :466  \n Separated           : 27   Retired             :194  \n Divorced            :123   Part-time           :141  \n Widowed             : 60   Permanently disabled:107  \n Single              :352   Unemployed          :101  \n Domestic Partnership: 56   Homemaker           : 91  \n                            (Other)             : 78  \n                   educ        gender          race             pid3    \n No HS               :100   Male  :567   White   :863   Democrat  :449  \n High school graduate:402   Female:611   Black   :131   Republican:276  \n Some college        :252                Hispanic:110   Indepent  :375  \n 2-year              :105                Mixed   : 29   Other     : 74  \n 4-year              :198                Asian   : 22   Not sure  :  4  \n Post-grad           :121                Other   : 15                   \n                                         (Other) :  8                   \n                pid7                   ideo5                    pew_churatd \n Strong Democrat  :295   Very liberal     :141   Never                :376  \n Independent      :201   Liberal          :207   Seldom               :267  \n Strong Republican:157   Moderate         :367   Once a week          :179  \n Weak Democrat    :141   Conservative     :257   A few times a year   :145  \n Weak Republican  :114   Very conservative:119   More than once a week: 95  \n (Other)          :256   Not sure         : 87   Once or twice a month: 93  \n NA's             : 14                           (Other)              : 23  \n     faminc          weight                                dpolice.new \n Min.   : 1.00   Min.   :0.1693   Treats whites much better      :337  \n 1st Qu.: 3.00   1st Qu.:0.3948   Treats whites moderately better:181  \n Median : 6.00   Median :0.8105   Treats whites a little better  :221  \n Mean   :16.76   Mean   :0.9941   Treats both the same           :410  \n 3rd Qu.: 9.00   3rd Qu.:1.2210   NA's                           : 29  \n Max.   :98.00   Max.   :7.0104                                        \n                                                                       \n\n\n\nnes |&gt; \n    summarize(mean_ftobama = mean(ftobama))\n\n# A tibble: 1 × 1\n  mean_ftobama\n         &lt;dbl&gt;\n1         48.8\n\n\n\nnes |&gt; \n    group_by(vote12) |&gt;\n    summarize(mean_ftobama = mean(ftobama))\n\n# A tibble: 4 × 2\n  vote12       mean_ftobama\n  &lt;fct&gt;               &lt;dbl&gt;\n1 Mitt Romney          11.8\n2 Barack Obama         77.8\n3 Someone Else         23.9\n4 Not asked            50.7\n\n\n\nnes |&gt; \n    filter(vote12 == \"Barack Obama\") |&gt;\n    summarize(mean_ftobama = mean(ftobama))\n\n# A tibble: 1 × 1\n  mean_ftobama\n         &lt;dbl&gt;\n1         77.8\n\n\n\nnes |&gt; \n    mutate(new_var = \"this is a new variable\") |&gt; \n    glimpse()\n\nRows: 1,178\nColumns: 52\n$ follow       &lt;fct&gt; Most of the time, Some of the Time, Most of the time, Mos…\n$ birthyr      &lt;int&gt; 1960, 1957, 1963, 1980, 1974, 1958, 1978, 1951, 1973, 193…\n$ turnout12    &lt;fct&gt; Definitely voted, Probably did not vote, Definitely voted…\n$ vote12       &lt;fct&gt; Barack Obama, Not asked, Mitt Romney, Barack Obama, Mitt …\n$ meet         &lt;fct&gt; Extremely Likely, A little likely, Extremely Likely, Not …\n$ march        &lt;fct&gt; Have done this in past 4 years, Have not done this in pas…\n$ compromise   &lt;fct&gt; Compromies to get things done, Compromies to get things d…\n$ ftobama      &lt;int&gt; 100, 39, 1, 89, 1, 0, 73, 0, 12, 87, 32, 10, 70, 80, 4, 0…\n$ ftblack      &lt;int&gt; 100, 6, 50, 61, 61, 50, 100, 70, 50, 75, 53, 80, 88, 67, …\n$ ftwhite      &lt;int&gt; 100, 74, 50, 64, 58, 51, 70, 70, 50, 90, 52, 80, 90, 74, …\n$ fthisp       &lt;int&gt; 100, 6, 50, 61, 71, 51, 100, 69, 50, 51, 54, 80, 90, 57, …\n$ ftgay        &lt;int&gt; 96, 75, 16, 62, 55, 46, 100, 49, 5, 85, 52, 70, 86, 27, 9…\n$ fttrump      &lt;int&gt; 1, 28, 100, 0, 13, 61, 5, 85, 70, 5, 74, 95, 39, 82, 91, …\n$ fthrc        &lt;int&gt; 76, 52, 1, 69, 1, 1, 60, 0, 3, 87, 6, 10, 49, 3, 1, 0, 79…\n$ ftsanders    &lt;int&gt; 84, 13, 2, 71, 13, 11, 100, 40, 31, 97, 51, 10, 90, 41, 1…\n$ ftpolice     &lt;int&gt; 51, 65, 85, 56, 89, 55, 34, 85, 80, 60, 97, 90, 91, 99, 9…\n$ ftfem        &lt;int&gt; 62, 53, 2, 60, 31, 0, 100, 45, 21, 61, 34, 50, 80, 22, 54…\n$ ftmuslim     &lt;int&gt; 20, 22, 5, 61, 22, 11, 100, 40, 12, 72, 43, 50, 81, 53, 2…\n$ ftsci        &lt;int&gt; 100, 91, 61, 96, 40, 71, 100, 76, 50, 91, 98, 90, 91, 1, …\n$ econnow      &lt;fct&gt; 2, 4, 5, 2, 4, 4, 2, 3, 4, 2, 4, 3, 2, 5, 4, 4, 2, 3, 4, …\n$ lcself       &lt;fct&gt; Neither liberal nor conservative, Neither liberal nor con…\n$ disc_b       &lt;fct&gt; A little, None at all, None at all, A moderate amount, A …\n$ disc_h       &lt;fct&gt; A little, A moderate amount, None at all, A little, A lit…\n$ disc_g       &lt;fct&gt; A moderate amount, A moderate amount, None at all, A mode…\n$ disc_w       &lt;fct&gt; A little, A lot, A lot, None at all, A great deal, A litt…\n$ disc_m       &lt;fct&gt; A little, A little, A lot, None at all, A great deal, Non…\n$ disc_fed     &lt;fct&gt; Treats whites a little better, Treats whites a little bet…\n$ disc_police  &lt;fct&gt; Treats whites moderately better, Treats whites a little b…\n$ immig_numb   &lt;fct&gt; Increased a moderate amount, Increased a little, Decrease…\n$ terror_worry &lt;fct&gt; Not at all worried, Not asked, Extremely worried, Not at …\n$ healthspend  &lt;fct&gt; Increase a great deal, Incrase moderately, Decrease moder…\n$ finwell      &lt;fct&gt; Moderately worse, A little worse, A great deal worse, Mod…\n$ warmcause    &lt;fct&gt; Mostly by human activity, About equally by human activity…\n$ freetrade    &lt;fct&gt; Neither favor nor oppose, Neither favor nor oppose, Neith…\n$ stopblack    &lt;fct&gt; Somewhat often, Somewhat often, Sometimes, Somewhat often…\n$ stop_ever    &lt;fct&gt; Has happened, Not asked, Not asked, Not asked, Has happen…\n$ birthright_b &lt;fct&gt; Favor moderately, Not asked, Not asked, Not asked, Oppose…\n$ bo_muslim    &lt;fct&gt; Not a Muslim, Not a Muslim, Muslim, Not a Muslim, Not a M…\n$ amer_ident   &lt;fct&gt; A little important, Moderately important, Extremely impor…\n$ marstat      &lt;fct&gt; Married, Divorced, Single, Married, Married, Married, Sin…\n$ employ       &lt;fct&gt; Full-time, Full-time, Unemployed, Full-time, Full-time, F…\n$ educ         &lt;fct&gt; 4-year, Post-grad, 4-year, 4-year, High school graduate, …\n$ gender       &lt;fct&gt; Male, Female, Male, Male, Male, Male, Male, Male, Male, M…\n$ race         &lt;fct&gt; White, White, White, White, White, White, White, White, W…\n$ pid3         &lt;fct&gt; Democrat, Indepent, Republican, Democrat, Other, Indepent…\n$ pid7         &lt;fct&gt; Strong Democrat, Independent, Weak Republican, Strong Dem…\n$ ideo5        &lt;fct&gt; Not sure, Liberal, Conservative, Liberal, Conservative, C…\n$ pew_churatd  &lt;fct&gt; Never, A few times a year, Never, Never, Once a week, Onc…\n$ faminc       &lt;int&gt; 4, 8, 1, 12, 10, 7, 97, 10, 8, 10, 11, 5, 5, 6, 8, 97, 97…\n$ weight       &lt;dbl&gt; 0.9511600, 2.6701962, 1.4303896, 0.9139662, 0.2639346, 1.…\n$ dpolice.new  &lt;fct&gt; Treats whites moderately better, Treats whites a little b…\n$ new_var      &lt;chr&gt; \"this is a new variable\", \"this is a new variable\", \"this…\n\n\n\nnes |&gt; \n    mutate(ftobama_cat = case_when(ftobama &gt;= 50 ~ \"Generally Positive\",\n                                    ftobama &lt; 50 ~ \"Generally Negative\")) |&gt; \n    glimpse()\n\nRows: 1,178\nColumns: 52\n$ follow       &lt;fct&gt; Most of the time, Some of the Time, Most of the time, Mos…\n$ birthyr      &lt;int&gt; 1960, 1957, 1963, 1980, 1974, 1958, 1978, 1951, 1973, 193…\n$ turnout12    &lt;fct&gt; Definitely voted, Probably did not vote, Definitely voted…\n$ vote12       &lt;fct&gt; Barack Obama, Not asked, Mitt Romney, Barack Obama, Mitt …\n$ meet         &lt;fct&gt; Extremely Likely, A little likely, Extremely Likely, Not …\n$ march        &lt;fct&gt; Have done this in past 4 years, Have not done this in pas…\n$ compromise   &lt;fct&gt; Compromies to get things done, Compromies to get things d…\n$ ftobama      &lt;int&gt; 100, 39, 1, 89, 1, 0, 73, 0, 12, 87, 32, 10, 70, 80, 4, 0…\n$ ftblack      &lt;int&gt; 100, 6, 50, 61, 61, 50, 100, 70, 50, 75, 53, 80, 88, 67, …\n$ ftwhite      &lt;int&gt; 100, 74, 50, 64, 58, 51, 70, 70, 50, 90, 52, 80, 90, 74, …\n$ fthisp       &lt;int&gt; 100, 6, 50, 61, 71, 51, 100, 69, 50, 51, 54, 80, 90, 57, …\n$ ftgay        &lt;int&gt; 96, 75, 16, 62, 55, 46, 100, 49, 5, 85, 52, 70, 86, 27, 9…\n$ fttrump      &lt;int&gt; 1, 28, 100, 0, 13, 61, 5, 85, 70, 5, 74, 95, 39, 82, 91, …\n$ fthrc        &lt;int&gt; 76, 52, 1, 69, 1, 1, 60, 0, 3, 87, 6, 10, 49, 3, 1, 0, 79…\n$ ftsanders    &lt;int&gt; 84, 13, 2, 71, 13, 11, 100, 40, 31, 97, 51, 10, 90, 41, 1…\n$ ftpolice     &lt;int&gt; 51, 65, 85, 56, 89, 55, 34, 85, 80, 60, 97, 90, 91, 99, 9…\n$ ftfem        &lt;int&gt; 62, 53, 2, 60, 31, 0, 100, 45, 21, 61, 34, 50, 80, 22, 54…\n$ ftmuslim     &lt;int&gt; 20, 22, 5, 61, 22, 11, 100, 40, 12, 72, 43, 50, 81, 53, 2…\n$ ftsci        &lt;int&gt; 100, 91, 61, 96, 40, 71, 100, 76, 50, 91, 98, 90, 91, 1, …\n$ econnow      &lt;fct&gt; 2, 4, 5, 2, 4, 4, 2, 3, 4, 2, 4, 3, 2, 5, 4, 4, 2, 3, 4, …\n$ lcself       &lt;fct&gt; Neither liberal nor conservative, Neither liberal nor con…\n$ disc_b       &lt;fct&gt; A little, None at all, None at all, A moderate amount, A …\n$ disc_h       &lt;fct&gt; A little, A moderate amount, None at all, A little, A lit…\n$ disc_g       &lt;fct&gt; A moderate amount, A moderate amount, None at all, A mode…\n$ disc_w       &lt;fct&gt; A little, A lot, A lot, None at all, A great deal, A litt…\n$ disc_m       &lt;fct&gt; A little, A little, A lot, None at all, A great deal, Non…\n$ disc_fed     &lt;fct&gt; Treats whites a little better, Treats whites a little bet…\n$ disc_police  &lt;fct&gt; Treats whites moderately better, Treats whites a little b…\n$ immig_numb   &lt;fct&gt; Increased a moderate amount, Increased a little, Decrease…\n$ terror_worry &lt;fct&gt; Not at all worried, Not asked, Extremely worried, Not at …\n$ healthspend  &lt;fct&gt; Increase a great deal, Incrase moderately, Decrease moder…\n$ finwell      &lt;fct&gt; Moderately worse, A little worse, A great deal worse, Mod…\n$ warmcause    &lt;fct&gt; Mostly by human activity, About equally by human activity…\n$ freetrade    &lt;fct&gt; Neither favor nor oppose, Neither favor nor oppose, Neith…\n$ stopblack    &lt;fct&gt; Somewhat often, Somewhat often, Sometimes, Somewhat often…\n$ stop_ever    &lt;fct&gt; Has happened, Not asked, Not asked, Not asked, Has happen…\n$ birthright_b &lt;fct&gt; Favor moderately, Not asked, Not asked, Not asked, Oppose…\n$ bo_muslim    &lt;fct&gt; Not a Muslim, Not a Muslim, Muslim, Not a Muslim, Not a M…\n$ amer_ident   &lt;fct&gt; A little important, Moderately important, Extremely impor…\n$ marstat      &lt;fct&gt; Married, Divorced, Single, Married, Married, Married, Sin…\n$ employ       &lt;fct&gt; Full-time, Full-time, Unemployed, Full-time, Full-time, F…\n$ educ         &lt;fct&gt; 4-year, Post-grad, 4-year, 4-year, High school graduate, …\n$ gender       &lt;fct&gt; Male, Female, Male, Male, Male, Male, Male, Male, Male, M…\n$ race         &lt;fct&gt; White, White, White, White, White, White, White, White, W…\n$ pid3         &lt;fct&gt; Democrat, Indepent, Republican, Democrat, Other, Indepent…\n$ pid7         &lt;fct&gt; Strong Democrat, Independent, Weak Republican, Strong Dem…\n$ ideo5        &lt;fct&gt; Not sure, Liberal, Conservative, Liberal, Conservative, C…\n$ pew_churatd  &lt;fct&gt; Never, A few times a year, Never, Never, Once a week, Onc…\n$ faminc       &lt;int&gt; 4, 8, 1, 12, 10, 7, 97, 10, 8, 10, 11, 5, 5, 6, 8, 97, 97…\n$ weight       &lt;dbl&gt; 0.9511600, 2.6701962, 1.4303896, 0.9139662, 0.2639346, 1.…\n$ dpolice.new  &lt;fct&gt; Treats whites moderately better, Treats whites a little b…\n$ ftobama_cat  &lt;chr&gt; \"Generally Positive\", \"Generally Negative\", \"Generally Ne…\n\n\n\nnes &lt;- nes |&gt; \n    mutate(ftobama_cat = case_when(ftobama &gt;= 50 ~ \"Generally Positive\",\n                                    ftobama &lt; 50 ~ \"Generally Negative\"))\n\ng &lt;- nes |&gt; \n    ggplot(aes(x = ftobama_cat)) + \n    geom_bar() \n\ng + \n    theme_minimal() + \n    labs(\n        x = \"Feelings toward Obama (Categorical)\",\n        y = \"Frequency\", \n        title = \"Frequency of NES respondents who are generally negative\\n vs. generally positive toward Obama\"\n    )"
  },
  {
    "objectID": "dissertation.html",
    "href": "dissertation.html",
    "title": "Dissertation",
    "section": "",
    "text": "My dissertation develops computational frameworks to measure how digital engagement translates into offline political action, with a focus on political violence and extremism. This project bridges persistent gaps between online behavior research and studies of political violence by developing novel methods to trace the mechanisms connecting digital communities to real-world mobilization."
  },
  {
    "objectID": "dissertation.html#overview",
    "href": "dissertation.html#overview",
    "title": "Dissertation",
    "section": "",
    "text": "My dissertation develops computational frameworks to measure how digital engagement translates into offline political action, with a focus on political violence and extremism. This project bridges persistent gaps between online behavior research and studies of political violence by developing novel methods to trace the mechanisms connecting digital communities to real-world mobilization."
  },
  {
    "objectID": "dissertation.html#the-puzzle",
    "href": "dissertation.html#the-puzzle",
    "title": "Dissertation",
    "section": "The Puzzle",
    "text": "The Puzzle\nScholars of political violence have long recognized that collective action requires coordination, resource mobilization, and ideological commitment. Yet we lack systematic tools to measure how these processes unfold across online and offline spaces. Existing research tends to study online communities in isolation from their offline consequences, or treats digital platforms as mere communication channels rather than constitutive environments for political identity formation.\nCentral Question: Under what conditions do online extremist communities produce offline political violence, and how can we measure these relationships at scale?"
  },
  {
    "objectID": "dissertation.html#theoretical-contribution",
    "href": "dissertation.html#theoretical-contribution",
    "title": "Dissertation",
    "section": "Theoretical Contribution",
    "text": "Theoretical Contribution\nI argue that online-offline translation is not a simple pipeline but a multi-stage process mediated by community structure, ideological coherence, and platform affordances. Drawing on theories of collective action, social identity, and contentious politics, I develop a framework that conceptualizes digital platforms as:\n\nIdentity laboratories where political commitments are formed and tested\nCoordination infrastructure that enables rapid mobilization\nMeaning-making spaces where grievances are framed and justified\n\nThis framework generates testable hypotheses about when online activity predicts offline action—and when it does not."
  },
  {
    "objectID": "dissertation.html#empirical-strategy",
    "href": "dissertation.html#empirical-strategy",
    "title": "Dissertation",
    "section": "Empirical Strategy",
    "text": "Empirical Strategy\nThe dissertation comprises three empirical chapters:\n\nChapter 1: Network Structures and Violence Prediction\nUsing social network analysis of extremist forums, I examine whether specific network configurations (density, centralization, bridging ties) predict subsequent violent events. This chapter develops novel measures of “mobilization potential” based on community structure.\n\nData: Multi-platform extremist network data (2018-2024)\nMethods: Social network analysis, event-study designs, time-series analysis\n\n\n\nChapter 2: Ideological Convergence and Escalation\nThis chapter uses natural language processing to measure ideological convergence within online communities over time, testing whether communities that develop more coherent ideological frames are more likely to engage in collective violence.\n\nData: Longitudinal text corpora from accelerationist communities\nMethods: Transformer-based text classification, topic modeling, causal inference\n\n\n\nChapter 3: Platform Affordances and Cross-Platform Mobilization\nThe final empirical chapter examines how platform characteristics (anonymity, moderation, visibility) shape the translation of online grievances into offline action. I leverage variation across platforms and natural experiments in platform policy changes.\n\nData: Multi-platform data including mainstream social media, encrypted messaging, and niche forums\nMethods: Difference-in-differences, regression discontinuity, machine learning"
  },
  {
    "objectID": "dissertation.html#broader-contributions",
    "href": "dissertation.html#broader-contributions",
    "title": "Dissertation",
    "section": "Broader Contributions",
    "text": "Broader Contributions\nBeyond its empirical findings, this dissertation makes methodological contributions relevant to computational social science more broadly:\n\nMeasurement innovation: Novel approaches to quantifying online-offline relationships\nMethodological integration: Combining NLP, network analysis, and causal inference in a unified framework\nEthical framework: Guidelines for responsible research on extremist communities"
  },
  {
    "objectID": "dissertation.html#committee",
    "href": "dissertation.html#committee",
    "title": "Dissertation",
    "section": "Committee",
    "text": "Committee\nCo-Chairs:\n\nAndrew Q. Philips, Department of Political Science, CU Boulder\nJennifer Fitzgerald, Department of Political Science, CU Boulder\n\nMembers:\n\nAlexandra Siegel, Department of Political Science, CU Boulder\nBrian Keegan, Department of Information Science, CU Boulder (external advisor)"
  },
  {
    "objectID": "dissertation.html#timeline",
    "href": "dissertation.html#timeline",
    "title": "Dissertation",
    "section": "Timeline",
    "text": "Timeline\n\n\n\nMilestone\nTarget Date\n\n\n\n\nProspectus Defense\nCompleted\n\n\nChapter 1 Draft\nSummer 2026\n\n\nChapter 2 Draft\nFall 2026\n\n\nChapter 3 Draft\nSpring 2027\n\n\nDefense\nMay 2027\n\n\n\n\nFor questions about my dissertation research, contact alex.newhouse@colorado.edu."
  },
  {
    "objectID": "psci_2075_study_guide.html",
    "href": "psci_2075_study_guide.html",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "",
    "text": "This study guide prepares you for a 90-minute handwritten exam covering post-estimation diagnostics through counterfactual predictions.\nFocus on:\n\n✍️ Writing clear definitions in your own words\n📊 Interpreting output and plots\n🧮 Working through calculation examples\n💡 Applying concepts to real scenarios\n\nExam Format: Multiple choice, short answer, interpretation questions, and calculation problems."
  },
  {
    "objectID": "psci_2075_study_guide.html#definition",
    "href": "psci_2075_study_guide.html#definition",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "1.1 Definition",
    "text": "1.1 Definition\nPost-estimation diagnostics are tests and visualizations performed after fitting a regression to check if model assumptions hold and if the fit is appropriate."
  },
  {
    "objectID": "psci_2075_study_guide.html#key-concept",
    "href": "psci_2075_study_guide.html#key-concept",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "1.2 Key Concept",
    "text": "1.2 Key Concept\nRunning a regression is easy—but the results are only valid if assumptions are met! Diagnostics catch problems before we interpret coefficients."
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions",
    "href": "psci_2075_study_guide.html#example-exam-questions",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "1.3 Example Exam Questions",
    "text": "1.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nAfter fitting a regression model, why must we check diagnostics before interpreting results?\nAnswer: Even if a model runs without errors, the results may be invalid if assumptions are violated. Diagnostics help us identify issues like heteroskedasticity, influential outliers, or non-linear relationships that would make our coefficients unreliable or our standard errors incorrect.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nList three things post-estimation diagnostics help us identify.\nAnswer: 1. Violated assumptions (e.g., heteroskedasticity, non-linearity) 2. Outliers or influential observations affecting results 3. Patterns in residuals suggesting model misspecification\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nTrue or False: If a regression produces significant coefficients, we don’t need to check diagnostics.\nAnswer: False! Significant coefficients don’t guarantee valid results. We must always check diagnostics to ensure assumptions hold and the model is appropriate."
  },
  {
    "objectID": "psci_2075_study_guide.html#quick-reference",
    "href": "psci_2075_study_guide.html#quick-reference",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "1.4 Quick Reference",
    "text": "1.4 Quick Reference\n\n\nCode\n# Using car package for diagnostics\nresidualPlots(model1)  # Check for patterns in residuals\n\n\n\n\n\n\n\n\n\n           Test stat Pr(&gt;|Test stat|)\nx1            0.6962           0.4880\nx2           -0.5381           0.5918\nTukey test    1.5145           0.1299\n\n\nCode\ninfluencePlot(model1)  # Identify influential cases\n\n\n\n\n\n\n\n\n\n     StudRes        Hat      CookD\n29 -1.041997 0.11177559 0.04550429\n38 -2.034476 0.07027137 0.10101225\n56 -2.074045 0.01716976 0.02422504\n85  1.255040 0.11158901 0.06555924"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-1",
    "href": "psci_2075_study_guide.html#definition-1",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "2.1 Definition",
    "text": "2.1 Definition\nThe Gauss-Markov theorem states that under certain assumptions, OLS produces the Best Linear Unbiased Estimator (BLUE)—meaning OLS gives the most efficient (lowest variance) estimates among all linear unbiased estimators."
  },
  {
    "objectID": "psci_2075_study_guide.html#the-five-key-assumptions",
    "href": "psci_2075_study_guide.html#the-five-key-assumptions",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "2.2 The Five Key Assumptions",
    "text": "2.2 The Five Key Assumptions\n\nLinearity: True relationship between X and Y is linear\nNo perfect multicollinearity: Predictors aren’t perfectly correlated\nExogeneity: Error term has expected value of zero (E[ε|X] = 0)\nHomoskedasticity: Constant error variance across all X values\nNo autocorrelation: Errors are independent"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-1",
    "href": "psci_2075_study_guide.html#example-exam-questions-1",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "2.3 Example Exam Questions",
    "text": "2.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhat does BLUE stand for, and why does it matter?\nAnswer: BLUE = Best Linear Unbiased Estimator. “Best” means minimum variance (most efficient), “Linear” means linear in Y, “Unbiased” means E[β̂] = β (correct on average). This matters because it tells us OLS is the optimal method when assumptions hold.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nIf the homoskedasticity assumption is violated but all other Gauss-Markov assumptions hold, what happens to OLS estimates?\nAnswer: OLS coefficients remain unbiased but are no longer BLUE (not the most efficient). More importantly, standard errors are incorrect, making hypothesis tests and confidence intervals invalid. We would need to use robust standard errors.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nA researcher finds that error variance increases with income levels in their model. Which Gauss-Markov assumption is violated?\nAnswer: Homoskedasticity is violated. This is heteroskedasticity—non-constant error variance across levels of X (income).\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nMatch each violation to its consequence:\nViolation A: Heteroskedasticity\nViolation B: Omitted variable bias\nViolation C: Non-linearity\nConsequence 1: Biased coefficients\nConsequence 2: Invalid standard errors but unbiased coefficients\nConsequence 3: Wrong functional form\nAnswer: A-2, B-1, C-3"
  },
  {
    "objectID": "psci_2075_study_guide.html#key-takeaway",
    "href": "psci_2075_study_guide.html#key-takeaway",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "2.4 Key Takeaway",
    "text": "2.4 Key Takeaway\nWhen Gauss-Markov assumptions hold → OLS is optimal ✓\nWhen assumptions are violated → We need different methods or corrections!"
  },
  {
    "objectID": "psci_2075_study_guide.html#definitions",
    "href": "psci_2075_study_guide.html#definitions",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "3.1 Definitions",
    "text": "3.1 Definitions\nError Variance: The variability of residuals around the regression line (σ²)\nHomoskedasticity: Constant error variance across all levels of X\n- The “spread” of residuals stays the same\nHeteroskedasticity: Non-constant error variance\n- The “spread” of residuals changes (often increases) as X changes"
  },
  {
    "objectID": "psci_2075_study_guide.html#visual-identification",
    "href": "psci_2075_study_guide.html#visual-identification",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "3.2 Visual Identification",
    "text": "3.2 Visual Identification\nHomoskedastic residuals: Look like a horizontal band with constant width\nHeteroskedastic residuals: Show a funnel or cone pattern"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-2",
    "href": "psci_2075_study_guide.html#example-exam-questions-2",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "3.3 Example Exam Questions",
    "text": "3.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nExplain in one sentence what heteroskedasticity means.\nAnswer: Heteroskedasticity occurs when the variability of prediction errors changes across different levels of the predictor variable(s).\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nYou’re modeling income as a function of education. The residual plot shows errors are small for low education but very large for high education. What problem is this?\nAnswer: This is heteroskedasticity. The error variance increases with education level, violating the constant variance assumption.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nDoes heteroskedasticity bias coefficient estimates? What does it affect?\nAnswer: No, heteroskedasticity does NOT bias coefficients—they remain unbiased. However, it makes standard errors incorrect, which invalidates hypothesis tests, p-values, and confidence intervals.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nIn a residuals vs. fitted values plot, what pattern indicates homoskedasticity?\nAnswer: Random scatter with roughly constant vertical spread across all fitted values (horizontal band pattern with no systematic increase or decrease in spread).\n\n\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nA Breusch-Pagan test gives p = 0.03. What do you conclude?\nAnswer: With p = 0.03 &lt; 0.05, we reject the null hypothesis of homoskedasticity. There is evidence of heteroskedasticity in the model. We should use robust standard errors."
  },
  {
    "objectID": "psci_2075_study_guide.html#real-world-example",
    "href": "psci_2075_study_guide.html#real-world-example",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "3.4 Real-World Example",
    "text": "3.4 Real-World Example\nIncome prediction: People with low incomes have similar incomes (small variance), but high-earners vary widely (large variance). This creates heteroskedasticity."
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "3.5 Diagnostic Code Reference",
    "text": "3.5 Diagnostic Code Reference\n\n\nCode\n# Visual check using car package\nresidualPlots(model1, type = \"rstandard\")\n\n\n\n\n\n\n\n\n\n           Test stat Pr(&gt;|Test stat|)\nx1            0.6962           0.4880\nx2           -0.5381           0.5918\nTukey test    1.5145           0.1299\n\n\nCode\n# Formal test\nbptest(model1)  # Breusch-Pagan test\n\n\n\n    studentized Breusch-Pagan test\n\ndata:  model1\nBP = 2.7753, df = 2, p-value = 0.2497\n\n\nCode\n# p &gt; 0.05 = homoskedastic (good!)\n# p &lt; 0.05 = heteroskedastic (problem!)"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-2",
    "href": "psci_2075_study_guide.html#definition-2",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "4.1 Definition",
    "text": "4.1 Definition\nA residual plot graphs residuals (observed Y - predicted Ŷ) against fitted values or predictors to visually diagnose model problems."
  },
  {
    "objectID": "psci_2075_study_guide.html#what-to-look-for",
    "href": "psci_2075_study_guide.html#what-to-look-for",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "4.2 What to Look For",
    "text": "4.2 What to Look For\n\n\n\n\n\n\n\n\nPattern Observed\nInterpretation\nAction Needed\n\n\n\n\nRandom scatter (horizontal band)\n✓ Assumptions met\nNone—proceed!\n\n\nCurved/U-shaped pattern\n✗ Non-linear relationship\nAdd polynomial terms or transform\n\n\nFunnel/cone shape\n✗ Heteroskedasticity\nUse robust SEs or transform Y\n\n\nClusters or gaps\n✗ Missing categories\nInclude categorical variable\n\n\nExtreme points far from others\n✗ Outliers present\nInvestigate these cases"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-3",
    "href": "psci_2075_study_guide.html#example-exam-questions-3",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "4.3 Example Exam Questions",
    "text": "4.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nYou see a U-shaped pattern in your residual plot. What does this suggest?\nAnswer: A U-shaped pattern indicates non-linearity—the true relationship between X and Y is curved, not linear. We should consider adding a quadratic term (X²) or transforming variables.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nDraw what a residual plot looks like when assumptions are met.\nAnswer: [Student would draw a horizontal band around zero with random scatter, roughly constant width, no patterns—dots scattered randomly above and below the zero line]\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nA residual plot shows increasing spread from left to right (funnel shape). What specific problem is this, and what assumption does it violate?\nAnswer: This is heteroskedasticity (non-constant variance). It violates the homoskedasticity assumption of the Gauss-Markov theorem.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nWhy do we plot residuals against fitted values rather than just looking at them in a table?\nAnswer: Visual patterns are much easier to detect than looking at numbers. Plots reveal systematic issues like non-linearity, heteroskedasticity, or outliers that would be hard to spot in a table of values."
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference-1",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference-1",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "4.4 Diagnostic Code Reference",
    "text": "4.4 Diagnostic Code Reference\n\n\nCode\n# Using car package for enhanced residual plots\nresidualPlots(model1)\n\n\n\n\n\n\n\n\n\n           Test stat Pr(&gt;|Test stat|)\nx1            0.6962           0.4880\nx2           -0.5381           0.5918\nTukey test    1.5145           0.1299\n\n\nCode\n# Or using visreg for clearer visualization\nvisreg(model1, \"x1\", gg = TRUE) +\n  theme_minimal() +\n  labs(title = \"Partial Residual Plot for X1\")"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-3",
    "href": "psci_2075_study_guide.html#definition-3",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "5.1 Definition",
    "text": "5.1 Definition\nOutliers are observations with unusually large residuals—they don’t fit the pattern established by the rest of the data. These are Y-outliers (unusual outcome values given their X values)."
  },
  {
    "objectID": "psci_2075_study_guide.html#key-distinction",
    "href": "psci_2075_study_guide.html#key-distinction",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "5.2 Key Distinction",
    "text": "5.2 Key Distinction\n\nOutlier: Unusual Y value (large residual) → Far from regression line\nHigh leverage: Unusual X value → Far from mean of X\nInfluential: BOTH unusual X and Y → Changes regression results"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-4",
    "href": "psci_2075_study_guide.html#example-exam-questions-4",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "5.3 Example Exam Questions",
    "text": "5.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nIn a study of 100 students’ test scores and study hours, one student studied 10 hours (typical) but scored 15/100 (when model predicts 85). Is this an outlier, high leverage, or both?\nAnswer: This is an outlier only (not high leverage). The X value (study hours = 10) is typical, but the Y value (score = 15) is very unusual given X, creating a large residual.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nHow do we typically identify outliers in regression?\nAnswer: We look for observations with standardized residuals greater than |2| or |3| (more than 2-3 standard deviations from the predicted value). These can be seen in residual plots as points far from the bulk of data.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nTrue or False: All outliers should be removed from the analysis.\nAnswer: False! Outliers might represent data errors (which should be fixed), but they might also be legitimate unusual cases. We should investigate them, understand why they’re unusual, and report analyses with and without them rather than automatically deleting them.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nWhat’s the difference between an outlier and an influential case?\nAnswer: An outlier has an unusual Y value (large residual) but may not affect the regression line much. An influential case both has unusual values AND substantially changes the regression coefficients when removed—it “pulls” the line toward itself."
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference-2",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference-2",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "5.4 Diagnostic Code Reference",
    "text": "5.4 Diagnostic Code Reference\n\n\nCode\n# Identify outliers using car package\noutlierTest(model1)  # Most extreme outlier with Bonferroni p-value\n\n\nNo Studentized residuals with Bonferroni p &lt; 0.05\nLargest |rstudent|:\n    rstudent unadjusted p-value Bonferroni p\n56 -2.074045           0.040752           NA\n\n\nCode\n# Standardized residuals\nmodel1 %&gt;%\n  augment() %&gt;%\n  filter(abs(.std.resid) &gt; 2) %&gt;%\n  select(y, .fitted, .resid, .std.resid)\n\n\n# A tibble: 2 × 4\n      y .fitted .resid .std.resid\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;\n1  162.    190.  -27.8      -2.00\n2  183.    212.  -29.1      -2.04"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-4",
    "href": "psci_2075_study_guide.html#definition-4",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "6.1 Definition",
    "text": "6.1 Definition\nLeverage measures how unusual an observation’s X values are. High-leverage points have predictor values far from the mean(s) of X."
  },
  {
    "objectID": "psci_2075_study_guide.html#key-concept-1",
    "href": "psci_2075_study_guide.html#key-concept-1",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "6.2 Key Concept",
    "text": "6.2 Key Concept\n\nHigh leverage = Unusual X (position gives potential to influence)\nDoes NOT mean the point IS influential (that requires unusual Y too)\nThink of it as “potential influence”"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-5",
    "href": "psci_2075_study_guide.html#example-exam-questions-5",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "6.3 Example Exam Questions",
    "text": "6.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nIn a study of college students aged 18-22, one student is 45 years old. Their grades perfectly fit the model prediction. Does this observation have high leverage, a large residual, or both?\nAnswer: High leverage only. The X value (age = 45) is extreme/unusual compared to other students, but since their Y value (grades) fits the model well, the residual is small. This point has potential to influence but doesn’t actually pull the line because it fits the pattern.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nWhat is the “rule of thumb” threshold for identifying high leverage points?\nAnswer: Leverage values greater than 2(k+1)/n are considered high leverage, where k = number of predictors and n = sample size.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nYou have a model with 3 predictors and n = 100. Calculate the high leverage threshold.\nAnswer: Threshold = 2(k+1)/n = 2(3+1)/100 = 2(4)/100 = 8/100 = 0.08. Points with leverage &gt; 0.08 have high leverage.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nCan a point have high leverage but NOT be influential? Explain.\nAnswer: Yes! A high-leverage point (unusual X) is only influential if it ALSO has a large residual (unusual Y). If the high-leverage point fits the pattern (small residual), it doesn’t pull the regression line and thus isn’t influential, even though it has potential to be.\n\n\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nWhy do we care about leverage?\nAnswer: High-leverage points have the potential to strongly influence regression results because they’re far from other data. We need to check if they’re also outliers (large residuals), because high leverage + large residual = influential case that could distort our findings."
  },
  {
    "objectID": "psci_2075_study_guide.html#visual-example",
    "href": "psci_2075_study_guide.html#visual-example",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "6.4 Visual Example",
    "text": "6.4 Visual Example\nImagine a seesaw: A person standing far from the center (high leverage) could tip it dramatically, but only if they also push hard (large residual)."
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference-3",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference-3",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "6.5 Diagnostic Code Reference",
    "text": "6.5 Diagnostic Code Reference\n\n\nCode\n# Calculate and visualize leverage using car\ninfluenceIndexPlot(model1, vars = \"hat\")  # \"hat\" = leverage\n\n\n\n\n\n\n\n\n\nCode\n# Check for high leverage\nk &lt;- length(coef(model1)) - 1  # number of predictors\nn &lt;- nobs(model1)\nthreshold &lt;- 2 * (k + 1) / n\n\nmodel1 %&gt;%\n  augment() %&gt;%\n  filter(.hat &gt; threshold)\n\n\n# A tibble: 9 × 9\n      y    x1    x2 .fitted  .resid   .hat .sigma  .cooksd .std.resid\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;    &lt;dbl&gt;      &lt;dbl&gt;\n1  258.  59.2  41.6    259.  -0.833 0.0864   14.5 0.000116    -0.0606\n2  231.  73.9  28.1    244. -12.8   0.0639   14.4 0.0194      -0.923 \n3  158.  25.6  28.9    144.  14.0   0.0644   14.4 0.0231       1.00  \n4  250.  76.7  28.4    251.  -1.36  0.0766   14.5 0.000268    -0.0985\n5  131.  47.3  15.8    145. -14.1   0.112    14.4 0.0455      -1.04  \n6  168.  59.4  18.7    181. -12.5   0.0756   14.4 0.0221      -0.901 \n7  162.  62.0  19.6    190. -27.8   0.0703   14.2 0.101       -2.00  \n8  152.  44.8  16.5    142.   9.71  0.106    14.4 0.0202       0.714 \n9  154.  15.6  33.3    138.  17.0   0.112    14.3 0.0656       1.25"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-5",
    "href": "psci_2075_study_guide.html#definition-5",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "7.1 Definition",
    "text": "7.1 Definition\nA large residual means the difference between observed Y and predicted Ŷ is substantial—the model’s prediction was way off for that observation."
  },
  {
    "objectID": "psci_2075_study_guide.html#formula",
    "href": "psci_2075_study_guide.html#formula",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "7.2 Formula",
    "text": "7.2 Formula\nResidual = Observed - Predicted = \\(y_i - \\hat{y}_i\\)"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-6",
    "href": "psci_2075_study_guide.html#example-exam-questions-6",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "7.3 Example Exam Questions",
    "text": "7.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nA model predicts a student will score 88 on an exam, but they actually score 42. Calculate and interpret the residual.\nAnswer: Residual = 42 - 88 = -46. The model over-predicted by 46 points. This is a large negative residual indicating the student performed much worse than expected.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nWhy do we use standardized residuals rather than raw residuals to identify outliers?\nAnswer: Standardized residuals account for the fact that residuals naturally vary in size. They’re scaled to have standard deviation = 1, making them comparable across observations. Values greater than |2| or |3| indicate outliers regardless of the scale of Y."
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-6",
    "href": "psci_2075_study_guide.html#definition-6",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "8.1 Definition",
    "text": "8.1 Definition\nInfluential cases are observations that substantially change regression results when removed. They have BOTH high leverage (unusual X) AND large residuals (unusual Y)—a dangerous combination!"
  },
  {
    "objectID": "psci_2075_study_guide.html#measures-of-influence",
    "href": "psci_2075_study_guide.html#measures-of-influence",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "8.2 Measures of Influence",
    "text": "8.2 Measures of Influence\nCook’s Distance (most common) - Combines leverage and residual size - Values &gt; 1 or &gt; 4/n suggest high influence - Measures change in ALL fitted values when case is removed\nDFBETAS - Measures change in individual coefficients - Values &gt; 2/√n are concerning"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-7",
    "href": "psci_2075_study_guide.html#example-exam-questions-7",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "8.3 Example Exam Questions",
    "text": "8.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhat makes a case “influential” in regression?\nAnswer: An influential case has both (1) high leverage (unusual X values) and (2) a large residual (doesn’t fit the model well). This combination means the point has the potential to change results AND actually does pull the regression line. Removing it would substantially change coefficient estimates.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nYou find one observation with Cook’s D = 1.8 in a sample of n = 50. Is this influential? Show your reasoning.\nAnswer: Yes, very influential. Cook’s D = 1.8 exceeds both common thresholds: - It’s &gt; 1 (first threshold) ✓ - It’s &gt; 4/n = 4/50 = 0.08 (second threshold) ✓ This case should be investigated carefully.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nMatch each situation to whether the observation is influential:\nA. High leverage + small residual\nB. Low leverage + large residual\nC. High leverage + large residual\nOptions: (1) Influential, (2) Not influential\nAnswer: A-2 (not influential—fits the pattern despite unusual X), B-2 (not influential—can’t pull line from typical X position), C-1 (influential—unusual position AND pulls the line)\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nYou discover an influential case in your model. List three appropriate ways to handle it.\nAnswer: 1. Investigate: Check if it’s a data entry error (if so, correct it) 2. Report both: Show results with and without the case to assess sensitivity 3. Transform: Consider whether variable transformations reduce influence (Note: Do NOT automatically delete without justification!)\n\n\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nWhy is it problematic to simply delete influential cases without investigation?\nAnswer: Influential cases might represent legitimate but unusual observations (like a rare event or unique case). Deleting them without understanding why they’re unusual can bias results and lose important information. We should investigate, understand, and transparently report our handling of them rather than quietly removing “inconvenient” data."
  },
  {
    "objectID": "psci_2075_study_guide.html#real-world-example-1",
    "href": "psci_2075_study_guide.html#real-world-example-1",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "8.4 Real-World Example",
    "text": "8.4 Real-World Example\nIn a housing study, one mansion in Beverly Hills (high X) that sells for far more than predicted (large residual) would be influential—it pulls the regression line upward, making the slope steeper than it would be without it."
  },
  {
    "objectID": "psci_2075_study_guide.html#how-to-handle-influential-cases-decision-tree",
    "href": "psci_2075_study_guide.html#how-to-handle-influential-cases-decision-tree",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "8.5 How to Handle Influential Cases: Decision Tree",
    "text": "8.5 How to Handle Influential Cases: Decision Tree\n\nInvestigate → Data error? Fix it!\nLegitimate case? → Consider these options:\n\nReport results with and without\nUse robust regression methods\nTransform variables (log, etc.)\nAcknowledge as limitation\n\nDocument → Explain your decision transparently"
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference-4",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference-4",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "8.6 Diagnostic Code Reference",
    "text": "8.6 Diagnostic Code Reference\n\n\nCode\n# Comprehensive influence plot using car\ninfluencePlot(model1, main = \"Influence Plot\")\n\n\n\n\n\n\n\n\n\n     StudRes        Hat      CookD\n29 -1.041997 0.11177559 0.04550429\n38 -2.034476 0.07027137 0.10101225\n56 -2.074045 0.01716976 0.02422504\n85  1.255040 0.11158901 0.06555924\n\n\nCode\n# Large circles = high Cook's D\n# Right side = high leverage\n# Top/bottom = large residuals\n\n# Cook's Distance specifically\ninfluenceIndexPlot(model1, vars = \"Cook\")\n\n\n\n\n\n\n\n\n\nCode\n# Check specific thresholds\nmodel1 %&gt;%\n  augment() %&gt;%\n  mutate(cooks_threshold = 4/n()) %&gt;%\n  filter(.cooksd &gt; cooks_threshold) %&gt;%\n  select(y, .fitted, .resid, .hat, .cooksd)\n\n\n# A tibble: 3 × 5\n      y .fitted .resid   .hat .cooksd\n  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1  131.    145.  -14.1 0.112   0.0455\n2  162.    190.  -27.8 0.0703  0.101 \n3  154.    138.   17.0 0.112   0.0656"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-7",
    "href": "psci_2075_study_guide.html#definition-7",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "9.1 Definition",
    "text": "9.1 Definition\nAn influence plot combines three diagnostics in one visualization to identify problematic observations: - X-axis: Leverage (hat values) - Y-axis: Studentized residuals - Bubble size: Cook’s Distance (influence)"
  },
  {
    "objectID": "psci_2075_study_guide.html#how-to-read-it",
    "href": "psci_2075_study_guide.html#how-to-read-it",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "9.2 How to Read It",
    "text": "9.2 How to Read It\n\n\n\nLocation\nInterpretation\nAction\n\n\n\n\nCenter\nNormal observation\nNone\n\n\nRight side\nHigh leverage\nMonitor\n\n\nTop or bottom\nLarge residual\nInvestigate\n\n\nTop-right OR bottom-right\nHIGH INFLUENCE!\nExamine carefully\n\n\nLarge bubble anywhere\nCook’s D elevated\nCheck sensitivity"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-8",
    "href": "psci_2075_study_guide.html#example-exam-questions-8",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "9.3 Example Exam Questions",
    "text": "9.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nOn an influence plot, where would you find the most concerning observations?\nAnswer: The most concerning observations are in the top-right or bottom-right corners with large bubbles. These have high leverage (right side), large residuals (top/bottom), and high Cook’s Distance (large bubble)—all three indicating they’re influential.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nAn observation appears on the right side of an influence plot with a small bubble and is near the middle vertically. Should you be concerned?\nAnswer: Not very concerned. This indicates high leverage (unusual X) but small residual (fits the pattern) and small Cook’s D (not influential). It’s worth noting but not problematic since it doesn’t pull the regression line.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nWhy is an influence plot more useful than looking at leverage, residuals, and Cook’s D separately?\nAnswer: An influence plot shows all three diagnostics simultaneously, making it easy to identify observations that are problematic on multiple dimensions. We can immediately see which points combine high leverage with large residuals (the dangerous combination), rather than checking three separate diagnostics."
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference-5",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference-5",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "9.4 Diagnostic Code Reference",
    "text": "9.4 Diagnostic Code Reference\n\n\nCode\n# Create influence plot using car package\ninfluencePlot(model1, \n              id = list(method = \"noteworthy\", n = 3),\n              main = \"Influence Plot\",\n              sub = \"Bubble size ∝ Cook's Distance\")\n\n\n\n\n\n\n\n\n\n      StudRes        Hat      CookD\n29 -1.0419973 0.11177559 0.04550429\n38 -2.0344756 0.07027137 0.10101225\n56 -2.0740453 0.01716976 0.02422504\n73 -2.0213381 0.01913198 0.02574573\n83  0.7120725 0.10611630 0.02016698\n85  1.2550398 0.11158901 0.06555924"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-8",
    "href": "psci_2075_study_guide.html#definition-8",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "10.1 Definition",
    "text": "10.1 Definition\nAdded variable plots (also called partial regression plots or AVPs) show the relationship between Y and one predictor X after removing the effects of all other predictors in the model."
  },
  {
    "objectID": "psci_2075_study_guide.html#what-they-show",
    "href": "psci_2075_study_guide.html#what-they-show",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "10.2 What They Show",
    "text": "10.2 What They Show\nThe unique contribution of one predictor, holding all others constant. It’s the “partial” effect."
  },
  {
    "objectID": "psci_2075_study_guide.html#why-use-them",
    "href": "psci_2075_study_guide.html#why-use-them",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "10.3 Why Use Them?",
    "text": "10.3 Why Use Them?\n\nVisualize partial relationships that multivariate regression captures\nIdentify influential observations for specific predictors\nAssess linearity of each predictor’s relationship with Y\nUnderstand multicollinearity effects"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-9",
    "href": "psci_2075_study_guide.html#example-exam-questions-9",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "10.4 Example Exam Questions",
    "text": "10.4 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhat does an added variable plot show that a simple scatterplot of Y vs. X doesn’t show?\nAnswer: An added variable plot shows the relationship between Y and X after removing the effects of all other predictors. It shows X’s unique contribution holding other variables constant, whereas a simple scatterplot includes the confounded effects of all variables.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nIn a model with Y = income, X1 = education, X2 = experience, the AVP for education shows a strong positive relationship, but the simple scatterplot of income vs. education shows almost no relationship. What explains this?\nAnswer: Experience is likely a confounder. In the simple scatterplot, education’s effect is masked by experience (more educated people may have less experience). The AVP removes experience’s effect, revealing education’s true positive relationship with income after accounting for experience.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nYou see a non-linear curve in an added variable plot for one predictor. What does this suggest?\nAnswer: The relationship between that predictor and Y is non-linear, even after controlling for other variables. We should consider adding a polynomial term (e.g., X²) or transforming the variable.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nOn an AVP, one observation is far from the others on both axes. What diagnostic information does this provide?\nAnswer: This observation is influential for this specific predictor. It has unusual values on both the predictor (after removing other X’s effects) and the outcome (after removing other X’s effects), meaning it’s pulling the partial regression line for this predictor."
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference-6",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference-6",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "10.5 Diagnostic Code Reference",
    "text": "10.5 Diagnostic Code Reference\n\n\nCode\n# Create AVPs using car package\navPlots(model1, id = list(n = 3))\n\n\n\n\n\n\n\n\n\nCode\n# Or create individual AVP with visreg for better visualization\nvisreg(model1, \"x1\", gg = TRUE) +\n  theme_minimal() +\n  labs(title = \"Partial Effect of X1 on Y\",\n       subtitle = \"Controlling for X2\")"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-9",
    "href": "psci_2075_study_guide.html#definition-9",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "11.1 Definition",
    "text": "11.1 Definition\nA dichotomous (binary) dependent variable has only two possible outcomes, typically coded as 0 and 1."
  },
  {
    "objectID": "psci_2075_study_guide.html#common-examples",
    "href": "psci_2075_study_guide.html#common-examples",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "11.2 Common Examples",
    "text": "11.2 Common Examples\n\nVoted (1) vs. Didn’t Vote (0)\nPassed (1) vs. Failed (0)\nWon (1) vs. Lost (0)\nEvent occurred (1) vs. Didn’t occur (0)"
  },
  {
    "objectID": "psci_2075_study_guide.html#why-it-matters",
    "href": "psci_2075_study_guide.html#why-it-matters",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "11.3 Why It Matters",
    "text": "11.3 Why It Matters\nStandard OLS regression assumes Y is continuous. When Y is binary, we need specialized models to properly handle: - Predictions that must be between 0 and 1 - Non-linear relationships - Heteroskedasticity (variance depends on X)"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-10",
    "href": "psci_2075_study_guide.html#example-exam-questions-10",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "11.4 Example Exam Questions",
    "text": "11.4 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhy can’t we use regular OLS regression with a binary dependent variable?\nAnswer: OLS can produce predicted probabilities outside the valid range [0,1] (e.g., predicting someone has a 120% chance of voting or -30% chance). Additionally, OLS assumes constant variance, but binary outcomes inherently have non-constant variance—p(1-p) varies with predicted probability.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nGive three examples of research questions with dichotomous dependent variables.\nAnswer: 1. Does education predict whether someone votes (yes/no)? 2. Do economic conditions affect whether countries experience civil war (yes/no)? 3. Does campaign spending influence election outcomes (win/lose)?"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-10",
    "href": "psci_2075_study_guide.html#definition-10",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "12.1 Definition",
    "text": "12.1 Definition\nA linear probability model uses OLS regression with a binary dependent variable, directly estimating the probability that Y = 1."
  },
  {
    "objectID": "psci_2075_study_guide.html#model-form",
    "href": "psci_2075_study_guide.html#model-form",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "12.2 Model Form",
    "text": "12.2 Model Form\n\\[P(Y=1) = \\beta_0 + \\beta_1X_1 + \\beta_2X_2 + ... + \\varepsilon\\]"
  },
  {
    "objectID": "psci_2075_study_guide.html#interpretation",
    "href": "psci_2075_study_guide.html#interpretation",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "12.3 Interpretation",
    "text": "12.3 Interpretation\nCoefficients represent the change in probability (in percentage points) of Y = 1 for a one-unit increase in X."
  },
  {
    "objectID": "psci_2075_study_guide.html#example",
    "href": "psci_2075_study_guide.html#example",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "12.4 Example",
    "text": "12.4 Example\nP(vote) = 0.20 + 0.05(education)\nEach additional year of education increases the probability of voting by 0.05 (5 percentage points)."
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-11",
    "href": "psci_2075_study_guide.html#example-exam-questions-11",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "12.5 Example Exam Questions",
    "text": "12.5 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nInterpret this LPM coefficient: In a model predicting college enrollment (1=enrolled, 0=not), the coefficient on family income (in $10,000s) is 0.03. What does this mean?\nAnswer: Each additional $10,000 in family income increases the probability of college enrollment by 0.03 (3 percentage points). For example, going from $50,000 to $60,000 increases enrollment probability from, say, 0.60 to 0.63.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nWhat are two advantages of the linear probability model?\nAnswer: 1. Easy interpretation—coefficients are changes in probability (percentage points) 2. Simple to estimate—just use regular OLS\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nWhat are three disadvantages/problems with the linear probability model?\nAnswer: 1. Can predict probabilities &lt; 0 or &gt; 1 (impossible!) 2. Heteroskedasticity is guaranteed (variance = p(1-p) varies) 3. Assumes constant marginal effects (same effect at all X values)\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nA LPM predicts someone has a 1.15 probability of graduating. What problem does this illustrate?\nAnswer: This illustrates the main problem with LPM—predicted probabilities can exceed 1.0 (or fall below 0), which is impossible since probabilities must be between 0 and 1. This occurs especially for extreme values of X.\n\n\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nWhen might we prefer LPM despite its problems?\nAnswer: When predicted probabilities all fall within [0,1] and we want easily interpretable coefficients (percentage point changes), LPM can be acceptable. It’s also useful for quick preliminary analysis before fitting a more complex logistic model."
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference-7",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference-7",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "12.6 Diagnostic Code Reference",
    "text": "12.6 Diagnostic Code Reference\n\n\nCode\n# Create binary outcome\ndata_binary &lt;- data_example %&gt;%\n  mutate(y_binary = if_else(y &gt; median(y), 1, 0))\n\n# Fit LPM (just regular OLS with binary Y)\nlpm &lt;- lm(y_binary ~ x1 + x2, data = data_binary)\n\n# Check predictions\nlpm %&gt;%\n  augment() %&gt;%\n  summarise(\n    min_pred = min(.fitted),\n    max_pred = max(.fitted),\n    n_below_0 = sum(.fitted &lt; 0),\n    n_above_1 = sum(.fitted &gt; 1)\n  )\n\n\n# A tibble: 1 × 4\n  min_pred max_pred n_below_0 n_above_1\n     &lt;dbl&gt;    &lt;dbl&gt;     &lt;int&gt;     &lt;int&gt;\n1   -0.344     1.29         6         8"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-11",
    "href": "psci_2075_study_guide.html#definition-11",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "13.1 Definition",
    "text": "13.1 Definition\nLogistic regression models the log-odds of Y = 1 as a linear function of X. It guarantees all predictions are between 0 and 1 by using the logistic (S-shaped) curve."
  },
  {
    "objectID": "psci_2075_study_guide.html#the-logistic-function",
    "href": "psci_2075_study_guide.html#the-logistic-function",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "13.2 The Logistic Function",
    "text": "13.2 The Logistic Function\n\\[P(Y=1) = \\frac{e^{\\beta_0 + \\beta_1 X}}{1 + e^{\\beta_0 + \\beta_1 X}} = \\frac{1}{1 + e^{-(\\beta_0 + \\beta_1 X)}}\\]"
  },
  {
    "objectID": "psci_2075_study_guide.html#key-features",
    "href": "psci_2075_study_guide.html#key-features",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "13.3 Key Features",
    "text": "13.3 Key Features\n\nS-shaped curve: Predictions asymptote at 0 and 1\nNon-linear: Effect of X varies depending on probability level\nCoefficients in log-odds: Not directly interpretable as probabilities"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-12",
    "href": "psci_2075_study_guide.html#example-exam-questions-12",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "13.4 Example Exam Questions",
    "text": "13.4 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhy do we use logistic regression instead of LPM for binary outcomes?\nAnswer: Logistic regression solves the main problems of LPM: (1) predictions are always between 0 and 1 (never impossible probabilities), (2) it accounts for the natural S-shaped relationship between predictors and binary outcomes, and (3) it properly handles the heteroskedasticity inherent in binary data.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nWhat does the logistic curve (S-shape) represent in terms of how X affects P(Y=1)?\nAnswer: The S-curve shows that X’s effect on probability is non-linear: small when probability is near 0 or 1 (curve is flat), and largest when probability is near 0.5 (curve is steepest). This reflects reality—it’s harder to move someone from 95% to 99% likely than from 50% to 54%.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nWhat is the main challenge in interpreting logistic regression coefficients?\nAnswer: Coefficients are in log-odds units, which aren’t intuitive. We can’t say “a one-unit increase in X increases probability by β,” because the effect depends on where you start on the curve. We need to either exponentiate to get odds ratios or calculate predicted probabilities.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nCompare LPM and logistic regression on three dimensions:\nAnswer: | Dimension | LPM | Logistic | |———–|—–|———-| | Predictions | Can be &lt;0 or &gt;1 | Always [0,1] ✓ | | Interpretation | Easy (percentage points) | Harder (log-odds) | | Marginal effects | Constant | Vary by X value |"
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference-8",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference-8",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "13.5 Diagnostic Code Reference",
    "text": "13.5 Diagnostic Code Reference\n\n\nCode\n# Fit logistic regression\nlogit_model &lt;- glm(y_binary ~ x1 + x2, \n                   data = data_binary,\n                   family = binomial(link = \"logit\"))\n\n# Check that predictions are valid\nlogit_model %&gt;%\n  augment(type.predict = \"response\") %&gt;%\n  summarise(\n    min_pred = min(.fitted),\n    max_pred = max(.fitted)\n  )\n\n\n# A tibble: 1 × 2\n  min_pred max_pred\n     &lt;dbl&gt;    &lt;dbl&gt;\n1 0.000584    0.999\n\n\nCode\n# All predictions between 0 and 1 ✓"
  },
  {
    "objectID": "psci_2075_study_guide.html#definitions-1",
    "href": "psci_2075_study_guide.html#definitions-1",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "14.1 Definitions",
    "text": "14.1 Definitions\nProbability: P(Y=1), ranges from 0 to 1\nOdds: \\(\\frac{P(Y=1)}{P(Y=0)} = \\frac{P(Y=1)}{1-P(Y=1)}\\)\n- Ranges from 0 to ∞ - Odds = 1 means 50-50 chance\nLog-Odds (Logit): \\(\\ln\\left(\\frac{P(Y=1)}{1-P(Y=1)}\\right)\\)\n- Ranges from -∞ to +∞ - This is what logistic regression directly models!"
  },
  {
    "objectID": "psci_2075_study_guide.html#conversion-table",
    "href": "psci_2075_study_guide.html#conversion-table",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "14.2 Conversion Table",
    "text": "14.2 Conversion Table\n\n\n\nProbability\nOdds\nLog-Odds\nInterpretation\n\n\n\n\n0.10\n0.11\n-2.20\nUnlikely\n\n\n0.25\n0.33\n-1.10\nSomewhat unlikely\n\n\n0.50\n1.00\n0.00\n50-50\n\n\n0.75\n3.00\n1.10\nLikely\n\n\n0.90\n9.00\n2.20\nVery likely"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-13",
    "href": "psci_2075_study_guide.html#example-exam-questions-13",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "14.3 Example Exam Questions",
    "text": "14.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nIf P(vote) = 0.80, calculate the odds and log-odds of voting.\nAnswer: - Odds = 0.80/(1-0.80) = 0.80/0.20 = 4.0 - Log-odds = ln(4.0) = 1.39\nInterpretation: The odds of voting are 4 to 1, or the log-odds are 1.39.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nIn logistic regression, what does a coefficient of 0.5 mean in terms of odds?\nAnswer: A coefficient of 0.5 means a one-unit increase in X multiplies the odds by e^0.5 = 1.65. The odds increase by 65%. (NOT that probability increases by 0.5!)\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nA logistic regression shows β = -0.4 for the variable “age”. Interpret this coefficient.\nAnswer: Each one-year increase in age multiplies the odds by e^(-0.4) = 0.67, meaning the odds decrease by 33% (since 1 - 0.67 = 0.33). Older people have lower odds of the outcome.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nWhy do we use log-odds in logistic regression instead of probabilities directly?\nAnswer: Log-odds can range from -∞ to +∞, making them suitable for linear modeling (like regular regression). We can then transform log-odds back to probabilities using the logistic function, ensuring predictions stay within [0,1].\n\n\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nConvert these interpretations:\nIf odds = 2: Probability = ?\nIf probability = 0.20: Odds = ?\nAnswer: - Odds = 2 → P = 2/(1+2) = 2/3 = 0.67 - P = 0.20 → Odds = 0.20/0.80 = 0.25"
  },
  {
    "objectID": "psci_2075_study_guide.html#formulas-to-remember",
    "href": "psci_2075_study_guide.html#formulas-to-remember",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "14.4 Formulas to Remember",
    "text": "14.4 Formulas to Remember\nOdds to Probability: \\(P = \\frac{Odds}{1 + Odds}\\)\nProbability to Odds: \\(Odds = \\frac{P}{1-P}\\)\nLogit to Probability: \\(P = \\frac{e^{logit}}{1 + e^{logit}}\\)\nCoefficient to Odds Ratio: \\(OR = e^{\\beta}\\)"
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference-9",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference-9",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "14.5 Diagnostic Code Reference",
    "text": "14.5 Diagnostic Code Reference\n\n\nCode\n# Get odds ratios from logistic regression\nlogit_model %&gt;%\n  tidy() %&gt;%\n  mutate(odds_ratio = exp(estimate)) %&gt;%\n  select(term, estimate, odds_ratio)\n\n\n# A tibble: 3 × 3\n  term        estimate odds_ratio\n  &lt;chr&gt;          &lt;dbl&gt;      &lt;dbl&gt;\n1 (Intercept)  -25.0     1.40e-11\n2 x1             0.255   1.29e+ 0\n3 x2             0.407   1.50e+ 0\n\n\nCode\n# Example interpretation:\n# If odds_ratio = 1.5, a one-unit increase in X multiplies odds by 1.5\n# (50% increase in odds)"
  },
  {
    "objectID": "psci_2075_study_guide.html#why-plot",
    "href": "psci_2075_study_guide.html#why-plot",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "15.1 Why Plot?",
    "text": "15.1 Why Plot?\nLogit coefficients (log-odds) are difficult to interpret directly. Plotting predicted probabilities across values of X makes results clear, intuitive, and shows the S-shaped relationship."
  },
  {
    "objectID": "psci_2075_study_guide.html#what-to-show",
    "href": "psci_2075_study_guide.html#what-to-show",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "15.2 What to Show",
    "text": "15.2 What to Show\n\nX-axis: Predictor variable of interest\nY-axis: Predicted probability of Y = 1\nCurve: The characteristic S-shape of logistic function\nHold other variables constant (typically at means)"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-14",
    "href": "psci_2075_study_guide.html#example-exam-questions-14",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "15.3 Example Exam Questions",
    "text": "15.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhy do we plot predicted probabilities from logistic regression rather than just reporting coefficients?\nAnswer: Coefficients are in log-odds, which are hard to interpret. Plots show predicted probabilities (intuitive!) and reveal how the effect of X varies across its range (stronger effect near p=0.5, weaker near 0 or 1). The visualization makes results accessible to non-technical audiences.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nWhen creating predicted probabilities from a logit model with multiple predictors, what should you do with the other variables not being plotted?\nAnswer: Hold them constant at meaningful values, typically at their means (for continuous variables) or at common categories (for categorical variables). This allows you to see the isolated effect of the variable of interest.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nYou plot predicted probabilities from age (20 to 80) predicting voting. The curve is steepest between ages 30-50 and flatter at the extremes. What does this tell you about age’s effect?\nAnswer: Age has the strongest effect on voting probability for middle-aged people (where the curve is steep). For very young people (already low probability) and elderly (already high probability), additional years of age change voting probability less. This is the non-linear marginal effect in logistic regression.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nWhy does a logistic curve flatten out at the top and bottom?\nAnswer: Probabilities are bounded at 0 and 1, so as predictions approach these limits, additional increases in X have smaller effects on probability. It’s hard to move from 95% to 99% (near ceiling) or from 5% to 1% (near floor), creating the flat parts of the S-curve."
  },
  {
    "objectID": "psci_2075_study_guide.html#diagnostic-code-reference-10",
    "href": "psci_2075_study_guide.html#diagnostic-code-reference-10",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "15.4 Diagnostic Code Reference",
    "text": "15.4 Diagnostic Code Reference\n\n\nCode\n# Create prediction data using visreg (easiest method)\nvisreg(logit_model, \"x1\", \n       scale = \"response\",  # Get probabilities, not log-odds\n       gg = TRUE) +\n  theme_minimal() +\n  labs(title = \"Predicted Probability by X1\",\n       y = \"P(Y = 1)\") +\n  ylim(0, 1)\n\n\n\n\n\n\n\n\n\nCode\n# Manual method for more control\npred_data &lt;- data_binary %&gt;%\n  summarise(\n    x1 = seq(min(x1), max(x1), length.out = 100),\n    x2 = mean(x2)\n  ) %&gt;%\n  mutate(\n    pred_prob = predict(logit_model, \n                       newdata = ., \n                       type = \"response\")\n  )\n\nggplot(pred_data, aes(x = x1, y = pred_prob)) +\n  geom_line(color = \"blue\", size = 1.2) +\n  theme_minimal() +\n  labs(title = \"Predicted Probability from Logistic Regression\",\n       x = \"X1\", y = \"P(Y = 1)\") +\n  ylim(0, 1)"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-12",
    "href": "psci_2075_study_guide.html#definition-12",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "16.1 Definition",
    "text": "16.1 Definition\nR-squared is the proportion of variance in Y explained by the model. It tells us how well our predictors account for variation in the outcome.\n\\[R^2 = 1 - \\frac{SSR}{TSS} = \\frac{\\text{Explained Variance}}{\\text{Total Variance}}\\]\nRanges from 0 to 1 (often reported as 0% to 100%)."
  },
  {
    "objectID": "psci_2075_study_guide.html#interpretation-guide",
    "href": "psci_2075_study_guide.html#interpretation-guide",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "16.2 Interpretation Guide",
    "text": "16.2 Interpretation Guide\n\n\n\nR² Value\nMeaning\nAssessment\n\n\n\n\n0.00-0.10\n0-10% explained\nVery weak model\n\n\n0.10-0.30\n10-30% explained\nWeak model\n\n\n0.30-0.50\n30-50% explained\nModerate model\n\n\n0.50-0.70\n50-70% explained\nGood model\n\n\n0.70-0.90\n70-90% explained\nStrong model\n\n\n0.90-1.00\n90-100% explained\nVery strong (check for overfitting!)\n\n\n\nNote: Standards vary by field! Social sciences typically have lower R² than physical sciences."
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-15",
    "href": "psci_2075_study_guide.html#example-exam-questions-15",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "16.3 Example Exam Questions",
    "text": "16.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nA model has R² = 0.42. Explain what this means in plain English.\nAnswer: The model explains 42% of the variance in the dependent variable. The predictors account for 42% of why Y varies, while 58% of the variation is due to other factors not in the model (captured by the error term).\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nTwo students fit models: - Model A: Education + Experience predicting income, R² = 0.38 - Model B: Education + Experience + Age predicting income, R² = 0.39\nStudent B claims their model is better because R² is higher. What’s wrong with this reasoning?\nAnswer: R² always increases (or stays same) when adding predictors, even useless ones! A 0.01 increase is tiny and may not justify adding Age. We should use adjusted R² which penalizes adding predictors, or compare out-of-sample fit, to determine if Age meaningfully improves the model.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nCalculate R² given SSR = 400 and TSS = 1000.\nAnswer: R² = 1 - (SSR/TSS) = 1 - (400/1000) = 1 - 0.4 = 0.6\nThe model explains 60% of the variance in Y.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nTrue or False: A model with R² = 0.95 is always better than one with R² = 0.65.\nAnswer: False! R² = 0.95 might indicate overfitting (fitting noise rather than signal). What matters more is: (1) out-of-sample performance, (2) theoretical justification for predictors, and (3) whether additional complexity is warranted. Also, the 0.95 model might have too many predictors.\n\n\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nWhy can’t we use R² to compare models with different dependent variables?\nAnswer: R² measures explained variance in Y. Different Y variables have different amounts of total variance, making R² non-comparable. For example, R² = 0.40 explaining income in dollars is different from R² = 0.40 explaining log(income)."
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-13",
    "href": "psci_2075_study_guide.html#definition-13",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "17.1 Definition",
    "text": "17.1 Definition\nAdjusted R² penalizes adding predictors that don’t sufficiently improve the model. It only increases if a new predictor improves fit more than expected by chance.\n\\[\\text{Adj } R^2 = 1 - \\frac{(1-R^2)(n-1)}{n-k-1}\\]\nWhere: n = sample size, k = number of predictors"
  },
  {
    "objectID": "psci_2075_study_guide.html#why-use-it",
    "href": "psci_2075_study_guide.html#why-use-it",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "17.2 Why Use It?",
    "text": "17.2 Why Use It?\n\nRegular R² is “optimistic” — always increases with more predictors\nAdjusted R² can decrease if a predictor doesn’t help enough\nBetter for comparing models with different numbers of predictors"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-16",
    "href": "psci_2075_study_guide.html#example-exam-questions-16",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "17.3 Example Exam Questions",
    "text": "17.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhen comparing two models with the same dependent variable, which should we use: R² or Adjusted R²?\nAnswer: Adjusted R², especially if the models have different numbers of predictors. Adjusted R² accounts for model complexity, while regular R² will favor the model with more predictors regardless of whether they’re useful.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nA researcher adds a predictor to their model. R² increases from 0.650 to 0.652, but Adjusted R² decreases from 0.638 to 0.636. What does this tell you?\nAnswer: The new predictor doesn’t improve the model enough to justify its inclusion. While R² increased slightly (as it always does), the Adjusted R² penalty shows the predictor added complexity without sufficient benefit. The simpler model is better.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nWhy is Adjusted R² always less than or equal to R²?\nAnswer: Adjusted R² includes a penalty term (n-1)/(n-k-1) that accounts for the number of predictors. This penalty reduces the value, making Adjusted R² ≤ R². The more predictors relative to sample size, the larger the penalty.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nCompare these models: - Model A: R² = 0.55, Adj R² = 0.52 (5 predictors) - Model B: R² = 0.54, Adj R² = 0.53 (2 predictors)\nWhich is preferable?\nAnswer: Model B. Though it has slightly lower R², its Adjusted R² is higher, indicating it achieves similar explanatory power with fewer predictors (more parsimonious). The 3 extra predictors in Model A don’t add enough value to justify their inclusion."
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-14",
    "href": "psci_2075_study_guide.html#definition-14",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "18.1 Definition",
    "text": "18.1 Definition\nSSR is the sum of all squared prediction errors. It measures total model error.\n\\[SSR = \\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2 = \\sum_{i=1}^{n} e_i^2\\]"
  },
  {
    "objectID": "psci_2075_study_guide.html#why-square-residuals",
    "href": "psci_2075_study_guide.html#why-square-residuals",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "18.2 Why Square Residuals?",
    "text": "18.2 Why Square Residuals?\n\nPositive and negative errors don’t cancel out\nLarger errors penalized more heavily (quadratic)\nMathematical convenience (differentiable for optimization)"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-17",
    "href": "psci_2075_study_guide.html#example-exam-questions-17",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "18.3 Example Exam Questions",
    "text": "18.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nA model makes errors of +3, -2, +4, -1. Calculate SSR.\nAnswer: SSR = (3)² + (-2)² + (4)² + (-1)² = 9 + 4 + 16 + 1 = 30\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nWhat does SSR = 0 mean?\nAnswer: Perfect fit—all predictions exactly equal observed values (no prediction error). This is suspicious and likely indicates overfitting, where the model memorizes data rather than learning generalizable patterns.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nWhy do we minimize SSR in OLS regression?\nAnswer: Minimizing SSR means finding the line (coefficients) that makes prediction errors as small as possible overall. This is the “best fit” line—the one that comes closest to all data points simultaneously, balancing errors across all observations.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nModel A has SSR = 500, Model B has SSR = 750. Which model fits better?\nAnswer: Model A fits better (assuming same data/same Y). Lower SSR means smaller total prediction errors. However, we should also check if Model A is more complex (more predictors), which might indicate overfitting."
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-15",
    "href": "psci_2075_study_guide.html#definition-15",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "19.1 Definition",
    "text": "19.1 Definition\nTSS measures total variance in Y—how much Y varies around its mean, ignoring any predictors.\n\\[TSS = \\sum_{i=1}^{n}(y_i - \\bar{y})^2\\]"
  },
  {
    "objectID": "psci_2075_study_guide.html#relationship-to-r²",
    "href": "psci_2075_study_guide.html#relationship-to-r²",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "19.2 Relationship to R²",
    "text": "19.2 Relationship to R²\n\\[R^2 = \\frac{TSS - SSR}{TSS} = 1 - \\frac{SSR}{TSS}\\]\nTSS = SSR + Explained Sum of Squares"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-18",
    "href": "psci_2075_study_guide.html#example-exam-questions-18",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "19.3 Example Exam Questions",
    "text": "19.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nGiven: TSS = 800, SSR = 200. Calculate R².\nAnswer: R² = 1 - (SSR/TSS) = 1 - (200/800) = 1 - 0.25 = 0.75\nThe model explains 75% of variance in Y.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nWhy is TSS the same for all models predicting the same Y?\nAnswer: TSS only depends on Y and its mean—it measures total variation in the outcome variable before considering any predictors. It’s the baseline we’re trying to explain with our models. All models start with the same TSS because they’re all trying to explain the same dependent variable.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nWhat does it mean if SSR = TSS?\nAnswer: R² = 0, meaning the model explains none of the variance in Y. The predictions are no better than simply guessing the mean of Y for everyone. The predictors are useless.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nA model has TSS = 1000, SSR = 300. How much variance is explained by the model?\nAnswer: Explained variance = TSS - SSR = 1000 - 300 = 700 This is 70% of the total (700/1000 = 0.70 = R²)"
  },
  {
    "objectID": "psci_2075_study_guide.html#key-formula-summary",
    "href": "psci_2075_study_guide.html#key-formula-summary",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "19.4 Key Formula Summary",
    "text": "19.4 Key Formula Summary\n\n\n\nComponent\nFormula\nMeaning\n\n\n\n\nTSS\n\\(\\sum(y_i - \\bar{y})^2\\)\nTotal variance\n\n\nSSR\n\\(\\sum(y_i - \\hat{y}_i)^2\\)\nUnexplained variance (error)\n\n\nESS\nTSS - SSR\nExplained variance\n\n\nR²\n1 - SSR/TSS\nProportion explained"
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-16",
    "href": "psci_2075_study_guide.html#definition-16",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "20.1 Definition",
    "text": "20.1 Definition\nOut of sample fit measures how well a model predicts new data it hasn’t seen. This is the ultimate test of model quality and generalizability!"
  },
  {
    "objectID": "psci_2075_study_guide.html#in-sample-vs.-out-of-sample",
    "href": "psci_2075_study_guide.html#in-sample-vs.-out-of-sample",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "20.2 In-Sample vs. Out-of-Sample",
    "text": "20.2 In-Sample vs. Out-of-Sample\n\n\n\nAspect\nIn-Sample\nOut-of-Sample\n\n\n\n\nData\nUsed to fit model\nNew, unseen data\n\n\nPerformance\nAlways looks better\nTrue test\n\n\nRisk\nCan overfit\nShows real predictive power\n\n\nPurpose\nModel development\nModel evaluation"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-19",
    "href": "psci_2075_study_guide.html#example-exam-questions-19",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "20.3 Example Exam Questions",
    "text": "20.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhy is out-of-sample performance more important than in-sample performance?\nAnswer: In-sample performance can be misleadingly good because the model has “seen” the data and can overfit to its specific patterns and noise. Out-of-sample performance shows whether the model learned genuine relationships that generalize to new data—the true test of a model’s usefulness.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nA model has in-sample R² = 0.92 but out-of-sample R² = 0.35. What’s the problem?\nAnswer: Overfitting! The model memorized the training data’s peculiarities (achieving 92% fit) but failed to learn generalizable patterns (only 35% fit on new data). The model is too complex for the amount of data or includes spurious relationships.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nGive an example of evaluating out-of-sample fit in a real research context.\nAnswer: Train an election prediction model on data from 2000-2016 elections, then test it on 2020 election data (held out). If predictions are accurate for 2020, the model generalizes well. If not, it overfit to historical patterns that didn’t hold.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nTrue or False: A model that perfectly fits training data (R² = 1.00) is ideal.\nAnswer: False! Perfect in-sample fit likely indicates severe overfitting—the model memorized noise and outliers rather than learning true patterns. It will perform poorly on new data."
  },
  {
    "objectID": "psci_2075_study_guide.html#definitions-2",
    "href": "psci_2075_study_guide.html#definitions-2",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "21.1 Definitions",
    "text": "21.1 Definitions\nMean Squared Error (MSE): Average squared prediction error \\[MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2\\]\nRoot Mean Squared Error (RMSE): Square root of MSE (original units!) \\[RMSE = \\sqrt{MSE}\\]"
  },
  {
    "objectID": "psci_2075_study_guide.html#why-rmse-is-useful",
    "href": "psci_2075_study_guide.html#why-rmse-is-useful",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "21.2 Why RMSE is Useful",
    "text": "21.2 Why RMSE is Useful\n\nIn same units as Y (interpretable!)\n“On average, predictions are off by RMSE units”\nLower is better\nCan compare across models"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-20",
    "href": "psci_2075_study_guide.html#example-exam-questions-20",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "21.3 Example Exam Questions",
    "text": "21.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nCalculate RMSE given these prediction errors: -10, 5, -8, 12\nAnswer: MSE = [(-10)² + (5)² + (-8)² + (12)²] / 4 = [100 + 25 + 64 + 144] / 4 = 333 / 4 = 83.25\nRMSE = √83.25 = 9.12\nOn average, predictions are off by about 9.12 units.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nA model predicting house prices has RMSE = $45,000. Interpret this.\nAnswer: On average, the model’s predictions are off by $45,000. For example, if the model predicts a house costs $300,000, the actual price is typically within the range of $255,000 to $345,000.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nModel A: RMSE = 12.5, Model B: RMSE = 18.3. Which is better?\nAnswer: Model A is better—it has lower average prediction error. Its predictions are typically about 6 units more accurate than Model B’s (18.3 - 12.5 = 5.8).\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nWhy do we take the square root of MSE to get RMSE?\nAnswer: MSE is in squared units (e.g., dollars²), which isn’t interpretable. Taking the square root returns RMSE to the original units of Y (dollars), making it meaningful—we can say “predictions are off by $X on average.”\n\n\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nTraining RMSE = 15, Test RMSE = 35. What does this indicate?\nAnswer: Overfitting. The model fits training data well (RMSE = 15) but performs much worse on new data (RMSE = 35). The large gap suggests the model learned training-specific patterns that don’t generalize."
  },
  {
    "objectID": "psci_2075_study_guide.html#definitions-3",
    "href": "psci_2075_study_guide.html#definitions-3",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "22.1 Definitions",
    "text": "22.1 Definitions\nTraining Data (60-80%): Used to fit the model (estimate coefficients)\nTesting Data (20-30%): Used to evaluate final model performance\n- NEVER used during model development - Provides unbiased performance estimate\nValidation Data (optional, ~10-20%): Used to tune/compare models\n- Select between model specifications - Tune hyperparameters - Common in complex modeling"
  },
  {
    "objectID": "psci_2075_study_guide.html#standard-workflows",
    "href": "psci_2075_study_guide.html#standard-workflows",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "22.2 Standard Workflows",
    "text": "22.2 Standard Workflows\n\n22.2.1 Simple Split (Training/Testing)\n\nSplit data (e.g., 70% train, 30% test)\nFit model on training data\nEvaluate on test data (ONCE at the end)\nReport test performance\n\n\n\n22.2.2 Three-Way Split (Training/Validation/Testing)\n\nSplit data (e.g., 60% train, 20% validation, 20% test)\nFit models on training data\nCompare models using validation data\nSelect best model\nFinal evaluation on test data (ONCE)"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-21",
    "href": "psci_2075_study_guide.html#example-exam-questions-21",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "22.3 Example Exam Questions",
    "text": "22.3 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhy can’t we use the same data to both fit and evaluate a model?\nAnswer: Using the same data for both gives an overly optimistic assessment. The model has already “seen” and adapted to that data’s patterns, so performance will be artificially inflated. We need unseen test data to get an honest estimate of how the model will perform on new observations.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nYou have 1,000 observations. Create a training/testing split.\nAnswer: - Training: 700 observations (70%) - Testing: 300 observations (30%)\nOr 80/20: 800 training, 200 testing. Either is acceptable; 70/30 or 80/20 are standard.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nWhat is the golden rule of test data?\nAnswer: NEVER use test data to make modeling decisions! It must remain completely unseen until final evaluation. Once you look at test performance and adjust your model, the test set is “contaminated” and no longer provides an unbiased assessment.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nWhat’s the purpose of a validation set?\nAnswer: The validation set lets us compare models or tune parameters without “using up” our test set. We can try different specifications and see which performs best on validation data, then get a final unbiased evaluation on the still-untouched test set.\n\n\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nA researcher splits data into train/test, fits a model, sees poor test performance, changes the model, and re-evaluates on the same test set. What’s wrong?\nAnswer: The researcher is using the test set to make modeling decisions, which “contaminates” it. The test performance is no longer an unbiased estimate because the model has been adapted based on that data. Should have used a validation set for model selection, saving the test set for final evaluation."
  },
  {
    "objectID": "psci_2075_study_guide.html#key-principles",
    "href": "psci_2075_study_guide.html#key-principles",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "22.4 Key Principles",
    "text": "22.4 Key Principles\n✓ Random split: Shuffle before splitting\n✓ No leakage: Test data never influences model fitting\n✓ Hold out: Test set untouched until final evaluation\n✓ Stratify (if needed): Maintain proportion of outcome in splits\n✗ Don’t peek: Looking at test data to make decisions invalidates it"
  },
  {
    "objectID": "psci_2075_study_guide.html#definitions-4",
    "href": "psci_2075_study_guide.html#definitions-4",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "23.1 Definitions",
    "text": "23.1 Definitions\nIn-Sample Predictions: \\(\\hat{y}_i\\) for observations used to fit the model\n- Based on training data - Model has “seen” these observations\nOut-of-Sample Predictions: \\(\\hat{y}_i\\) for new observations not used in fitting\n- Based on test/new data\n- Model has NOT seen these observations"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-22",
    "href": "psci_2075_study_guide.html#example-exam-questions-22",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "23.2 Example Exam Questions",
    "text": "23.2 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nWhy are in-sample predictions typically more accurate than out-of-sample predictions?\nAnswer: In-sample predictions are for data the model was fit to, so it has optimized its parameters specifically for these observations. Out-of-sample predictions are for new data, testing whether the model learned generalizable patterns rather than memorizing training data specifics.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nYou fit a model on 2010-2018 data to predict voter turnout: - In-sample: RMSE = 4.2 - Out-of-sample (2020 data): RMSE = 12.8\nWhat do these numbers tell you?\nAnswer: The model fits historical data well (RMSE = 4.2) but generalizes poorly to 2020 (RMSE = 12.8). This large gap suggests either: (1) overfitting to 2010-2018 patterns, or (2) 2020 was fundamentally different (e.g., COVID effects), making historical patterns less applicable.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nWhat’s the purpose of generating in-sample predictions if out-of-sample predictions are what we really care about?\nAnswer: In-sample predictions are useful for diagnostics (residual plots, identifying outliers, checking assumptions). Comparing in-sample vs. out-of-sample performance also reveals overfitting. But you’re right—ultimately, out-of-sample predictive accuracy is the real test.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nTrue or False: We should report in-sample fit as our model’s expected performance.\nAnswer: False! In-sample fit is optimistically biased. We should report out-of-sample performance as the expected performance on new data, since that’s an honest assessment of generalizability."
  },
  {
    "objectID": "psci_2075_study_guide.html#definition-17",
    "href": "psci_2075_study_guide.html#definition-17",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "24.1 Definition",
    "text": "24.1 Definition\nCounterfactual predictions answer “what if” questions by predicting outcomes under hypothetical scenarios that didn’t actually occur."
  },
  {
    "objectID": "psci_2075_study_guide.html#the-process",
    "href": "psci_2075_study_guide.html#the-process",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "24.2 The Process",
    "text": "24.2 The Process\n\nFit model on observed data\nCreate hypothetical scenarios (manipulate X values)\nGenerate predictions for these counterfactuals\nCompare to observed outcomes"
  },
  {
    "objectID": "psci_2075_study_guide.html#types-of-counterfactuals",
    "href": "psci_2075_study_guide.html#types-of-counterfactuals",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "24.3 Types of Counterfactuals",
    "text": "24.3 Types of Counterfactuals\nIndividual-level: “What would this person’s income be if they had 2 more years of education?”\nPopulation-level: “What would average turnout be if all states expanded voting access?”\nPolicy simulation: “What would crime rates be under different policing strategies?”"
  },
  {
    "objectID": "psci_2075_study_guide.html#example-exam-questions-23",
    "href": "psci_2075_study_guide.html#example-exam-questions-23",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "24.4 Example Exam Questions",
    "text": "24.4 Example Exam Questions\n\n\n\n\n\n\nNoteQuestion 1\n\n\n\nDefine counterfactual prediction in one sentence.\nAnswer: A counterfactual prediction estimates what an outcome would be under hypothetical conditions that differ from what was actually observed.\n\n\n\n\n\n\n\n\nNoteQuestion 2\n\n\n\nGiven the model: Income = 20,000 + 5,000(Education)\nPerson A has 12 years of education and earns $80,000. - Calculate Person A’s predicted income (in-sample prediction) - Calculate Person A’s counterfactual income if they had 16 years of education\nAnswer: - Observed: Income = 20,000 + 5,000(12) = $80,000 ✓ (prediction matches actual) - Counterfactual: Income = 20,000 + 5,000(16) = $100,000\nPerson A would be predicted to earn $100,000 if they had 16 years of education instead of 12.\n\n\n\n\n\n\n\n\nNoteQuestion 3\n\n\n\nWhy are counterfactual predictions useful for policy analysis?\nAnswer: They let us estimate policy effects before implementation. For example, we can predict what would happen if we changed a policy (e.g., “What if minimum wage increased?”) without actually implementing it. This helps policymakers make informed decisions and compare alternative policies.\n\n\n\n\n\n\n\n\nNoteQuestion 4\n\n\n\nWhat’s the key assumption when making counterfactual predictions?\nAnswer: That the relationships estimated from observed data will hold under the counterfactual scenario. This may not be true if the counterfactual is far outside the range of observed data (extrapolation) or if the relationship is context-dependent.\n\n\n\n\n\n\n\n\nNoteQuestion 5\n\n\n\nA model is trained on cities with population 100K-500K. A researcher uses it to predict outcomes for a city of 5 million. What’s the concern?\nAnswer: Extrapolation! The counterfactual (5M population) is far outside the range of observed data (100K-500K). The model may not accurately capture relationships at that scale—city dynamics might be fundamentally different at 5M than at the observed sizes.\n\n\n\n\n\n\n\n\nNoteQuestion 6\n\n\n\nCreate two counterfactual scenarios for this model:\nTest Score = 50 + 10(Study Hours) + 15(Tutoring)\nStudent studied 3 hours without tutoring (Tutoring = 0).\nAnswer: Observed: Score = 50 + 10(3) + 15(0) = 80\nCounterfactual 1 (more study): Score = 50 + 10(5) + 15(0) = 100 (+20 points)\nCounterfactual 2 (add tutoring): Score = 50 + 10(3) + 15(1) = 95 (+15 points)\nThese show the student would gain more from extra study (20) than from tutoring (15)."
  },
  {
    "objectID": "psci_2075_study_guide.html#real-world-applications",
    "href": "psci_2075_study_guide.html#real-world-applications",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "24.5 Real-World Applications",
    "text": "24.5 Real-World Applications\n\nMedicine: “What would patient outcomes be under different treatments?”\nEconomics: “What would GDP be under alternative tax policies?”\nPolitical Science: “What would election results be if turnout increased?”\nPublic Policy: “What would crime rates be with different enforcement?”"
  },
  {
    "objectID": "psci_2075_study_guide.html#cautions",
    "href": "psci_2075_study_guide.html#cautions",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "24.6 Cautions",
    "text": "24.6 Cautions\n⚠️ Extrapolation: Predictions far outside observed data may be unreliable\n⚠️ Assumption violations: Relationships may differ in counterfactual scenarios\n⚠️ Causal claims: Counterfactuals suggest but don’t prove causation\n⚠️ Confounding: Unmeasured variables may affect both X and Y"
  },
  {
    "objectID": "psci_2075_study_guide.html#what-to-memorize",
    "href": "psci_2075_study_guide.html#what-to-memorize",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "25.1 What to Memorize",
    "text": "25.1 What to Memorize\n\n25.1.1 Key Formulas\n\nR² = 1 - SSR/TSS\nRMSE = √(MSE) = √(Σ(y - ŷ)²/n)\nOdds = P/(1-P)\nLog-odds = ln(odds)\nLeverage threshold = 2(k+1)/n\nCook’s D threshold = 4/n or 1\n\n\n\n25.1.2 Interpretation Templates\n\nCoefficient: “A one-unit increase in X changes Y by β units, holding other variables constant”\nR²: “The model explains X% of the variance in Y”\nRMSE: “On average, predictions are off by X units”\nOdds ratio: “A one-unit increase in X multiplies the odds by e^β”"
  },
  {
    "objectID": "psci_2075_study_guide.html#exam-strategies",
    "href": "psci_2075_study_guide.html#exam-strategies",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "25.2 Exam Strategies",
    "text": "25.2 Exam Strategies\n\n25.2.1 Multiple Choice\n✓ Eliminate obviously wrong answers\n✓ Watch for “always/never” (usually wrong)\n✓ Check units and magnitudes\n✓ Remember assumptions matter!\n\n\n25.2.2 Short Answer\n✓ Define terms clearly\n✓ Give concrete examples\n✓ Show your reasoning\n✓ Use proper terminology\n\n\n25.2.3 Calculations\n✓ Show all work\n✓ Write formulas first\n✓ Check if answer makes sense\n✓ Include units\n\n\n25.2.4 Interpretation Questions\n✓ State what numbers mean in plain English\n✓ Connect to real-world context\n✓ Note limitations if relevant\n✓ Be precise with “holding constant” language"
  },
  {
    "objectID": "psci_2075_study_guide.html#common-mistakes-to-avoid",
    "href": "psci_2075_study_guide.html#common-mistakes-to-avoid",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "25.3 Common Mistakes to Avoid",
    "text": "25.3 Common Mistakes to Avoid\n❌ Confusing correlation with causation\n❌ Saying “significant” without specifying statistical vs. practical\n❌ Mixing up leverage, outliers, and influence\n❌ Treating log-odds as probabilities\n❌ Claiming R² measures causation\n❌ Ignoring assumptions when interpreting\n❌ Extrapolating beyond data range\n❌ Using test data for model selection"
  },
  {
    "objectID": "psci_2075_study_guide.html#quick-reference-decision-trees",
    "href": "psci_2075_study_guide.html#quick-reference-decision-trees",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "25.4 Quick Reference: Decision Trees",
    "text": "25.4 Quick Reference: Decision Trees\n\n25.4.1 Is this observation problematic?\nHigh leverage? → Yes → Large residual? → Yes → INFLUENTIAL! ⚠️\n                 ↓                        ↓\n                 No                       No → Monitor, not urgent\n                 ↓\n        Large residual? → Yes → Outlier, investigate\n                         ↓\n                         No → Normal observation ✓\n\n\n25.4.2 Which model fit metric should I use?\nSame DV, different predictors? → Different # predictors? → Yes → Adjusted R²\n                                                           ↓\n                                                           No → R² or RMSE\nDifferent DVs? → Cannot compare with R² or RMSE!\n\n\n25.4.3 Train/test or just train?\nNeed honest performance assessment? → Yes → Split train/test\n                                     ↓\n                                     No → Just training (exploratory only)"
  },
  {
    "objectID": "psci_2075_study_guide.html#problem-set-1-diagnostics",
    "href": "psci_2075_study_guide.html#problem-set-1-diagnostics",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "26.1 Problem Set 1: Diagnostics",
    "text": "26.1 Problem Set 1: Diagnostics\nQ1: Observation #47 has standardized residual = 3.2, leverage = 0.15, Cook’s D = 0.02, n = 100, k = 3. Is it problematic? Why or why not?\n\n\nAnswer\n\nLeverage threshold = 2(3+1)/100 = 0.08. Leverage = 0.15 &gt; 0.08 ✓ (high)\nStandardized residual = 3.2 &gt; 2 ✓ (large)\nCook’s D = 0.02 &lt; 4/100 = 0.04 ✓ (acceptable)\nConclusion: Has high leverage and large residual, but Cook’s D suggests it’s not very influential. Worth investigating but not urgent. The low Cook’s D indicates it’s not substantially changing regression results despite having concerning individual diagnostics.\n\nQ2: You see a funnel shape in your residual plot. What problem is this? What should you do?\n\n\nAnswer\n\nProblem: Heteroskedasticity (non-constant variance)\nActions: 1. Use robust standard errors (immediate fix for inference) 2. Transform Y (e.g., log transformation) to stabilize variance 3. Acknowledge limitation if transformations don’t help 4. Note: Coefficients are still unbiased; only SEs are affected"
  },
  {
    "objectID": "psci_2075_study_guide.html#problem-set-2-calculations",
    "href": "psci_2075_study_guide.html#problem-set-2-calculations",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "26.2 Problem Set 2: Calculations",
    "text": "26.2 Problem Set 2: Calculations\nQ3: Calculate R² and interpret: TSS = 500, SSR = 125\n\n\nAnswer\n\nR² = 1 - (SSR/TSS) = 1 - (125/500) = 1 - 0.25 = 0.75\nInterpretation: The model explains 75% of the variance in the dependent variable. The predictors account for three-quarters of why Y varies across observations.\n\nQ4: Errors: +15, -8, +12, -20. Calculate RMSE.\n\n\nAnswer\n\nMSE = [(15)² + (-8)² + (12)² + (-20)²] / 4\n= [225 + 64 + 144 + 400] / 4\n= 833 / 4 = 208.25\nRMSE = √208.25 = 14.43\nInterpretation: On average, predictions are off by about 14.43 units."
  },
  {
    "objectID": "psci_2075_study_guide.html#problem-set-3-interpretation",
    "href": "psci_2075_study_guide.html#problem-set-3-interpretation",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "26.3 Problem Set 3: Interpretation",
    "text": "26.3 Problem Set 3: Interpretation\nQ5: Logistic regression: β = 0.8 for Education. Interpret.\n\n\nAnswer\n\nEach additional year of education multiplies the odds of the outcome by e^0.8 = 2.23. The odds increase by 123% (since 2.23 - 1 = 1.23).\nFor example, if someone has odds of 1.0 (50% probability) with 12 years of education, they would have odds of 2.23 (69% probability) with 13 years of education, all else equal.\n\nQ6: Model A (training RMSE = 10, test RMSE = 12) vs. Model B (training RMSE = 8, test RMSE = 18). Which is better?\n\n\nAnswer\n\nModel A is better!\nAlthough Model B fits training data better (RMSE = 8 vs. 10), Model A generalizes much better to new data (test RMSE = 12 vs. 18). The large gap in Model B (8 → 18) indicates severe overfitting. Model A’s smaller gap (10 → 12) shows it learned generalizable patterns.\nOut-of-sample performance is what matters for real predictive accuracy."
  },
  {
    "objectID": "psci_2075_study_guide.html#definitions-no-notes",
    "href": "psci_2075_study_guide.html#definitions-no-notes",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "27.1 Definitions (No Notes)",
    "text": "27.1 Definitions (No Notes)\n\nDefine each term in your own words\nGive an example for each concept\nExplain why it matters"
  },
  {
    "objectID": "psci_2075_study_guide.html#visual-recognition",
    "href": "psci_2075_study_guide.html#visual-recognition",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "27.2 Visual Recognition",
    "text": "27.2 Visual Recognition\n\nIdentify problems from residual plots\nRead influence plots\nInterpret added variable plots"
  },
  {
    "objectID": "psci_2075_study_guide.html#calculations-1",
    "href": "psci_2075_study_guide.html#calculations-1",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "27.3 Calculations",
    "text": "27.3 Calculations\n\nCalculate R², SSR, TSS\nCalculate RMSE from errors\nCompute odds and log-odds\nFind leverage and Cook’s D thresholds"
  },
  {
    "objectID": "psci_2075_study_guide.html#interpretations",
    "href": "psci_2075_study_guide.html#interpretations",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "27.4 Interpretations",
    "text": "27.4 Interpretations\n\nExplain coefficients in context\nInterpret model fit statistics\nCompare models appropriately\nIdentify assumption violations"
  },
  {
    "objectID": "psci_2075_study_guide.html#conceptual-understanding",
    "href": "psci_2075_study_guide.html#conceptual-understanding",
    "title": "PSCI 2075 Final Exam Study Guide",
    "section": "27.5 Conceptual Understanding",
    "text": "27.5 Conceptual Understanding\n\nExplain difference between outliers, leverage, influence\nDescribe when to use LPM vs. logistic\nExplain training/testing/validation purposes\nArticulate in-sample vs. out-of-sample importance\n\n\nGood luck on your PSCI 2075 final exam! 🎓\nRemember: Understanding concepts &gt; memorizing formulas\n\nStudy guide created for PSCI 2075, CU Boulder | December 2025"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Alex Newhouse",
    "section": "",
    "text": "I am a PhD candidate in Political Science at the University of Colorado Boulder, specializing in computational analysis of political violence and extremism. My research develops and applies cutting-edge methods—machine learning, natural language processing, and social network analysis—to understand how online communities translate into offline political action.\nKey Experience:\n\nInvestigative Consultant to the House Select Committee on January 6th\n\nDeputy Director of the Center on Terrorism, Extremism & Counterterrorism\nPrincipal Investigator on $1.38M in federal research grants\nResearch partnerships with major technology companies"
  },
  {
    "objectID": "index.html#about",
    "href": "index.html#about",
    "title": "Alex Newhouse",
    "section": "",
    "text": "I am a PhD candidate in Political Science at the University of Colorado Boulder, specializing in computational analysis of political violence and extremism. My research develops and applies cutting-edge methods—machine learning, natural language processing, and social network analysis—to understand how online communities translate into offline political action.\nKey Experience:\n\nInvestigative Consultant to the House Select Committee on January 6th\n\nDeputy Director of the Center on Terrorism, Extremism & Counterterrorism\nPrincipal Investigator on $1.38M in federal research grants\nResearch partnerships with major technology companies"
  },
  {
    "objectID": "index.html#research-expertise",
    "href": "index.html#research-expertise",
    "title": "Alex Newhouse",
    "section": "Research Expertise",
    "text": "Research Expertise\n\n\nMethods:\n\nMachine Learning & NLP\nSocial Network Analysis\n\nCausal Inference\nData Science & Visualization\n\n\nSubstantive Areas:\n\nPolitical Violence & Extremism\nDigital Platform Research\nTechnology & Society\nComputational Social Science"
  },
  {
    "objectID": "index.html#featured-publications",
    "href": "index.html#featured-publications",
    "title": "Alex Newhouse",
    "section": "Featured Publications",
    "text": "Featured Publications\nKowert, R., Kilmer, E., and Newhouse, A. “Taking it to the Extreme: Prevalence and Nature of Extremist Sentiment in Games.” Frontiers in Psychology, 2024.\nNewhouse, A. “The Threat is the Network: The Multi-Node Structure of Neo-Fascist Accelerationism.” CTC Sentinel, 2021.\nMcGuffie, K. and Newhouse, A. “The Radicalization Risks of GPT-3 and Advanced Neural Language Models.” arXiv, 2020."
  },
  {
    "objectID": "index.html#research-impact",
    "href": "index.html#research-impact",
    "title": "Alex Newhouse",
    "section": "Research Impact",
    "text": "Research Impact\n$1.38M in grant funding (Principal Investigator) | 45+ students mentored | 276K article views | Featured in 8 major media outlets"
  },
  {
    "objectID": "index.html#media-recognition",
    "href": "index.html#media-recognition",
    "title": "Alex Newhouse",
    "section": "Media Recognition",
    "text": "Media Recognition\nResearch and expertise featured in: Washington Post • New York Times • NPR • BBC • Politico • Wired • Bloomberg • Meet the Press"
  },
  {
    "objectID": "index.html#current-projects",
    "href": "index.html#current-projects",
    "title": "Alex Newhouse",
    "section": "Current Projects",
    "text": "Current Projects\nDissertation Research: Developing computational frameworks to measure online-offline relationships in political violence\nCOVID-19 Visual Politics: Analyzing partisan differences in pandemic media coverage using computer vision and text analysis\nGaming & Extremism: Multi-platform investigation of recruitment and radicalization in digital gaming environments\n\nContact: alex.newhouse@colorado.edu"
  },
  {
    "objectID": "posts/2025-09-04-first-post/index.html",
    "href": "posts/2025-09-04-first-post/index.html",
    "title": "First post",
    "section": "",
    "text": "Testing blog!"
  },
  {
    "objectID": "teaching.html",
    "href": "teaching.html",
    "title": "Teaching & Mentorship",
    "section": "",
    "text": "My approach centers on active learning, methodological rigor, and practical application—helping students develop both theoretical understanding and technical skills they can apply to pressing societal challenges.\nI believe in making complex political science concepts accessible through hands-on learning and real-world applications, particularly in quantitative methods and comparative politics."
  },
  {
    "objectID": "teaching.html#teaching-philosophy",
    "href": "teaching.html#teaching-philosophy",
    "title": "Teaching & Mentorship",
    "section": "",
    "text": "My approach centers on active learning, methodological rigor, and practical application—helping students develop both theoretical understanding and technical skills they can apply to pressing societal challenges.\nI believe in making complex political science concepts accessible through hands-on learning and real-world applications, particularly in quantitative methods and comparative politics."
  },
  {
    "objectID": "teaching.html#teaching-impact",
    "href": "teaching.html#teaching-impact",
    "title": "Teaching & Mentorship",
    "section": "Teaching Impact",
    "text": "Teaching Impact\n45+ students mentored | 15 independent projects supervised | 7 courses taught | 3 institutions"
  },
  {
    "objectID": "teaching.html#courses-taught",
    "href": "teaching.html#courses-taught",
    "title": "Teaching & Mentorship",
    "section": "Courses Taught",
    "text": "Courses Taught\n\nUniversity of Colorado Boulder\nTeaching Assistant\n\nPSCI 2075: Quantitative Methods (Fall 2025, Spring 2024)\n\nComprehensive introduction to statistical analysis in political science, covering descriptive statistics, hypothesis testing, regression analysis, and research design. Emphasized practical application using R and real political datasets.\n\nPSCI 1101: Introduction to Western Political Thought (Spring 2025)\n\nSurvey of foundational political philosophy from ancient Greece through Enlightenment thinkers, connecting classical ideas to contemporary political challenges.\n\nPSCI 2223: Introduction to Comparative Politics (Fall 2024)\n\nExploration of political systems, institutions, and processes across different countries and regions, with emphasis on democratization, authoritarianism, and political development.\n\nPSCI 1824: Introduction to International Relations (Fall 2023)\n\nComprehensive overview of international relations theory and practice, covering security studies, international political economy, and global governance challenges.\n\n\n\n\nMiddlebury Institute of International Studies\nAdjunct Professor\n\nDigital Extremism (Spring 2023)\n\nSelf-designed course examining the intersection of technology and political violence. Covered online radicalization processes, platform dynamics, and content moderation strategies. Students conducted original research on extremist communities across digital platforms.\n\n\n\n\nMiddlebury College\nAdjunct Professor\n\nINTD 1027: The Study of Post-War Fascism (Winter 2026)\n\nSelf-designed interdisciplinary course examining the transformation of fascism and fascist movements following World War II. Covered philosophical and political roots, activism, violence, and research ethics across Italy, Russia, the UK, and the US. Students produced original research designs on topics in contemporary fascism studies.\n\nOnline Extremism (January 2021)\n\nIntensive winter term course on digital radicalization and online political violence. Integrated computational methods with political science theory, providing students hands-on experience with social media data analysis."
  },
  {
    "objectID": "teaching.html#teaching-resources",
    "href": "teaching.html#teaching-resources",
    "title": "Teaching & Mentorship",
    "section": "Teaching Resources",
    "text": "Teaching Resources\n\nQuantitative Methods Materials\n\nTutorial 1: Getting Started with Greek Notation and Descriptive Stats\nTutorial 2: An Introduction to R’s pipe operator\nTutorial 4: Variable Transformations in R - Understanding distributions, log transforms, and data recoding\nTutorial 5: Using AI Chatbots to Learn R Programming - Effective strategies for LLM-assisted coding and learning\nTutorial 6: Understanding File Systems and Working Directories in R and RStudio"
  },
  {
    "objectID": "teaching.html#student-research-mentorship",
    "href": "teaching.html#student-research-mentorship",
    "title": "Teaching & Mentorship",
    "section": "Student Research Mentorship",
    "text": "Student Research Mentorship\nI have supervised 45 undergraduate and graduate research assistants and 15 independent projects since 2019. My mentorship philosophy emphasizes methodological rigor, ethical research practices, and practical skill development.\n\nCurrent Projects\n\nApocalyptic Language Analysis: Four undergraduate RAs working on qualitative coding for social media research\n\n\n\nPast Student Projects\n\n\n\nItalian Neofascism & Political Violence\nMilitant Accelerationism & Coalition-Building\n\nFrench Far-Right on Encrypted Platforms\n\n\n\nAI Implications for Domestic Counterterrorism\nChristian Identity Movements\nGaming Platform Radicalization"
  },
  {
    "objectID": "teaching.html#research-teaching-integration",
    "href": "teaching.html#research-teaching-integration",
    "title": "Teaching & Mentorship",
    "section": "Research-Teaching Integration",
    "text": "Research-Teaching Integration\nMy courses integrate cutting-edge research methods with substantive political science questions. Students learn to:\n\nApply computational methods to real political phenomena using Python, R, and specialized software\nCritically evaluate digital media, online communities, and algorithmic systems\n\nNavigate ethical challenges of studying sensitive political topics\nConnect theoretical frameworks to empirical analysis through hands-on projects\nDevelop professional skills in data visualization, statistical analysis, and research communication\n\nSignature Approach: Each course includes substantial research components where students collect and analyze original data, preparing them for graduate study or professional research roles.\n\nInterested in collaboration or guest lectures? Contact alex.newhouse@colorado.edu"
  },
  {
    "objectID": "research.html",
    "href": "research.html",
    "title": "Research",
    "section": "",
    "text": "My research addresses a fundamental question in the study of political violence: How do online communities translate into offline political action? I argue that digital platforms are not mere communication channels but constitutive environments where political identities are formed, grievances are framed, and collective action is coordinated. Understanding political violence in the 21st century requires new theoretical frameworks and computational methods capable of tracing these online-offline dynamics.\nThis research agenda makes three interconnected contributions:\n\nTheoretical: I develop a framework conceptualizing digital platforms as identity laboratories, coordination infrastructure, and meaning-making spaces—each with distinct implications for mobilization potential.\nMethodological: I combine machine learning, natural language processing, and social network analysis to measure phenomena that traditional methods cannot capture at scale.\nSubstantive: My empirical work reveals how network structures, ideological coherence, and platform affordances shape when online extremism produces real-world violence.\n\nCore Research Questions:\n\nUnder what conditions do online communities produce offline political violence?\nHow do platform characteristics shape radicalization and mobilization processes?\nWhat role do emerging technologies (AI, gaming platforms) play in extremist recruitment?"
  },
  {
    "objectID": "research.html#research-statement",
    "href": "research.html#research-statement",
    "title": "Research",
    "section": "",
    "text": "My research addresses a fundamental question in the study of political violence: How do online communities translate into offline political action? I argue that digital platforms are not mere communication channels but constitutive environments where political identities are formed, grievances are framed, and collective action is coordinated. Understanding political violence in the 21st century requires new theoretical frameworks and computational methods capable of tracing these online-offline dynamics.\nThis research agenda makes three interconnected contributions:\n\nTheoretical: I develop a framework conceptualizing digital platforms as identity laboratories, coordination infrastructure, and meaning-making spaces—each with distinct implications for mobilization potential.\nMethodological: I combine machine learning, natural language processing, and social network analysis to measure phenomena that traditional methods cannot capture at scale.\nSubstantive: My empirical work reveals how network structures, ideological coherence, and platform affordances shape when online extremism produces real-world violence.\n\nCore Research Questions:\n\nUnder what conditions do online communities produce offline political violence?\nHow do platform characteristics shape radicalization and mobilization processes?\nWhat role do emerging technologies (AI, gaming platforms) play in extremist recruitment?"
  },
  {
    "objectID": "research.html#citation-metrics",
    "href": "research.html#citation-metrics",
    "title": "Research",
    "section": "Citation Metrics",
    "text": "Citation Metrics\nGoogle Scholar Profile: 1,093 total citations | Research Areas: collective behavior, online communities, extremism, conspiracy theories, video games\nImpact Highlights:\n\nFeatured in 8+ major media outlets\n276K+ total article views\nResearch cited in policy reports and academic literature"
  },
  {
    "objectID": "research.html#featured-publications",
    "href": "research.html#featured-publications",
    "title": "Research",
    "section": "Featured Publications",
    "text": "Featured Publications\n\nPeer-Reviewed Articles\nKowert, R., Kilmer, E., and Newhouse, A. (2024). “Taking it to the Extreme: Prevalence and Nature of Extremist Sentiment in Games.” Frontiers in Psychology, 15:1410620.\nThis study addresses the gap in research knowledge around extremist sentiment in games by evaluating prevalence, location, nature, and impact through an online survey of 423 game players, revealing alarmingly high rates of extremist content exposure and normalization within gaming cultures.\n\nFunded by: Department of Homeland Security (DHS # EMW-2022-GR-00036)\nKey Finding: More than half of all game players report experiencing some form of hate, harassment or abuse within gaming spaces, with extremist rhetoric becoming culturally normalized\n\nNewhouse, A. (2021). “The Threat is the Network: The Multi-Node Structure of Neo-Fascist Accelerationism.” CTC Sentinel, 14(5).\nSince 2015, the Atomwaffen Division has received bulk academic and media attention in coverage of the neo-fascist accelerationist movement, but evidence reveals it was not the apex of a hierarchy but rather one node in a larger network of violent accelerationists built on membership fluidity, frequent communications, and a shared goal of social destruction.\n\nInnovation: Novel network-based framework for understanding extremist movements\nPolicy Impact: Enforcement against individuals and groups is necessary but not sufficient—focus on specific groups may not tackle the root of the issue\nCitations: Extensively cited in terrorism studies and cited in recent Studies in Conflict & Terrorism article\n\nMcGuffie, K. and Newhouse, A. (2020). “The Radicalization Risks of GPT-3 and Advanced Neural Language Models.” arXiv preprint, 2009.06807.\nWe show GPT-3’s strength in generating text that accurately emulates interactive, informational, and influential content that could be utilized for radicalizing individuals into violent far-right extremist ideologies and behaviors. While OpenAI’s preventative measures are strong, the possibility of unregulated copycat technology represents significant risk for large-scale online radicalization and recruitment.\n\nImpact: 28 citations and featured in major tech policy discussions\nMedia Coverage: Extensive coverage in The Register, Vice, and other tech outlets\nPolicy Influence: Already in 2020, Kris McGuffie and Alex Newhouse highlighted the potential for abuse of generative language models by assessing GPT-3, revealing significant risk for large-scale online radicalization and recruitment\n\n\n\nOther Major Publications\nNewhouse, A. (2020). “Far-right activists on social media telegraphed violence weeks in advance of the attack on the US Capitol.” The Conversation.\nNewhouse, A. (2020). “Parler is bringing together mainstream conservatives, anti-Semites and white supremacists as the social media platform attracts millions of Trump supporters.” The Conversation.\n\nImpact: 241,000 views for Parler analysis\n\nMultiple GNET Research Reports (2021-2022) on accelerationist movements and extremist narratives, including analysis of the Great Replacement theory and Boogaloo movement."
  },
  {
    "objectID": "research.html#research-areas-methods",
    "href": "research.html#research-areas-methods",
    "title": "Research",
    "section": "Research Areas & Methods",
    "text": "Research Areas & Methods\n\nSubstantive Focus Areas\nPolitical Violence & Extremism\n\nNeo-fascist accelerationism and network structures\nOnline-offline radicalization pathways\n\nGaming platforms as extremist recruitment venues\nAI and synthetic media threats\n\nTechnology & Society\n\nDigital platform governance and content moderation\nAI safety and misuse potential\nSocial media data analysis and behavioral patterns\nGaming culture and political socialization\n\n\n\nMethodological Expertise\nComputational Methods\n\nMachine Learning (PyTorch, Scikit-Learn, Hugging Face Transformers)\nNatural Language Processing and text-as-data approaches\nSocial Network Analysis (igraph, statnet)\nTime-series analysis and causal inference\n\nFeatured Technical Work:\n\nDistilBERT for Political Text Classification - Automated detection of extremist content using transformer models (94% F1-score)\n\nData & Platforms\n\nLarge-scale social media data collection and analysis\nGaming platform research and mixed-methods approaches\nSurvey research and experimental design\nQualitative coding and content analysis"
  },
  {
    "objectID": "research.html#current-research-projects",
    "href": "research.html#current-research-projects",
    "title": "Research",
    "section": "Current Research Projects",
    "text": "Current Research Projects\n\nDissertation Research\n“Measuring Online-Offline Relationships in Political Violence”\nDeveloping computational frameworks to understand how digital engagement translates into offline political action. Uses machine learning, natural language processing, social network analysis, and time-series causal inference to examine extremist community dynamics.\n\nCommittee: Andrew Q. Philips (co-chair), Jennifer Fitzgerald (co-chair), Alexandra Siegel\nExternal Advisor: Brian Keegan (Information Science)\nExpected Completion: May 2027\n\n\n\nCOVID-19 Media Framing Study\n“Partisan Visual Politics During a Pandemic” (with Andrew Q. Philips and Komal P. Kaur)\nInvestigating partisan differences in visual and textual framing of pandemic coverage across U.S. news outlets using computer vision and text analysis methods.\n\n\nGaming & Extremism Research\n“Multi-Platform Radicalization in Digital Gaming”\nComprehensive study of recruitment and radicalization processes within digital gaming environments, including direct partnerships with major gaming platforms for harm mitigation strategies.\n\n\nPolicy & Financial Markets\n“Political Messaging and Market Responses” (under review)\nAnalysis of how political communications affect financial market behavior and investor decision-making."
  },
  {
    "objectID": "research.html#research-impact-media-coverage",
    "href": "research.html#research-impact-media-coverage",
    "title": "Research",
    "section": "Research Impact & Media Coverage",
    "text": "Research Impact & Media Coverage\n\nAcademic Recognition\n\nResearch extensively cited in terrorism studies literature\nWork featured in CTC Sentinel, premier counterterrorism publication\nPublications in top-tier psychology and computer science venues\n\n\n\nPolicy & Industry Impact\n\nJanuary 6th Committee: Investigative consultant providing expertise on extremist movements\nTechnology Partnerships: Direct collaboration with gaming companies (Roblox, Spectrum Labs)\nFederal Funding: $1.38M in grants as Principal Investigator\n\n\n\nMedia & Public Engagement\nMajor Media Features: Washington Post • New York Times • NPR (Morning Edition, All Things Considered) • BBC • Politico • Wired • Bloomberg Radio • Meet the Press Now\nHigh-Impact Articles:\n\nParler analysis: 241,000 views\nJanuary 6th prediction: 35,400 views\n\nExpert Commentary Topics:\n\nAI safety and misuse potential\nGaming platform moderation\nExtremist recruitment strategies\n\nSocial media radicalization\nTechnology policy implications"
  },
  {
    "objectID": "research.html#funding-grants",
    "href": "research.html#funding-grants",
    "title": "Research",
    "section": "Funding & Grants",
    "text": "Funding & Grants\n\nAs Principal Investigator\n\nDepartment of Homeland Security (2022-2024): Gaming and extremism research - $350,000\nMultiple Federal Grants (2019-2024): Extremism and technology studies - $1.38M total\n\n\n\nAs Co-Investigator\n\nLogically Partnership (2022): Social media data infrastructure for extremism research\nVarious Industry Partnerships: Gaming platform safety research"
  },
  {
    "objectID": "research.html#future-research-directions",
    "href": "research.html#future-research-directions",
    "title": "Research",
    "section": "Future Research Directions",
    "text": "Future Research Directions\nEmerging Technologies & Political Violence\n\nAI-generated disinformation and radicalization\nVirtual/augmented reality environments as political spaces\nBlockchain and decentralized platform governance\n\nComparative Extremism Studies\n\nCross-national analysis of digital radicalization patterns\nComparative platform governance approaches\nInternational cooperation in countering online extremism\n\nMethodological Innovation\n\nAdvanced causal inference methods for social media data\nMulti-modal analysis combining text, visual, and network data\nReal-time detection and intervention systems\n\n\nFor collaboration inquiries, media requests, or access to datasets, contact alex.newhouse@colorado.edu.\nGoogle Scholar: Alex Newhouse • ORCID: Available upon request"
  },
  {
    "objectID": "tutorial2-pipe.html",
    "href": "tutorial2-pipe.html",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "",
    "text": "When you first start learning R, your code might look something like this:\n\nresult &lt;- function3(function2(function1(data, arg1), arg2), arg3)\n\nThis nested approach works, but it’s hard to read and understand. What if there was a way to make your code read more like a sentence, flowing from left to right? Enter the pipe operator.\nThe pipe operator allows you to chain functions together in a way that’s intuitive and readable. Instead of nesting functions inside each other, you can “pipe” the output of one function directly into the next function as input."
  },
  {
    "objectID": "tutorial2-pipe.html#introduction",
    "href": "tutorial2-pipe.html#introduction",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "",
    "text": "When you first start learning R, your code might look something like this:\n\nresult &lt;- function3(function2(function1(data, arg1), arg2), arg3)\n\nThis nested approach works, but it’s hard to read and understand. What if there was a way to make your code read more like a sentence, flowing from left to right? Enter the pipe operator.\nThe pipe operator allows you to chain functions together in a way that’s intuitive and readable. Instead of nesting functions inside each other, you can “pipe” the output of one function directly into the next function as input."
  },
  {
    "objectID": "tutorial2-pipe.html#what-is-the-pipe-operator",
    "href": "tutorial2-pipe.html#what-is-the-pipe-operator",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "What is the Pipe Operator?",
    "text": "What is the Pipe Operator?\nThe pipe operator takes the result from the expression on its left side and passes it as the first argument to the function on its right side. Think of it like a literal pipe in plumbing—data flows through it from one function to the next.\nIn R, there are two main pipe operators you’ll encounter:\n\nNative pipe |&gt; (introduced in R 4.1.0, 2021)\nMagrittr pipe %&gt;% (from the magrittr package, popularized by tidyverse)"
  },
  {
    "objectID": "tutorial2-pipe.html#the-native-pipe",
    "href": "tutorial2-pipe.html#the-native-pipe",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "The Native Pipe |>",
    "text": "The Native Pipe |&gt;\nThe native pipe |&gt; is built directly into R (no packages required). Here’s how it works:\n\nBasic Syntax\n\ndata |&gt; function1() |&gt; function2() |&gt; function3()\n\nThis is equivalent to:\n\nfunction3(function2(function1(data)))\n\n\n\nSimple Example\nLet’s say you want to: 1. Take a vector of numbers 2. Calculate the square root of each 3. Round to 2 decimal places 4. Calculate the mean\nWithout pipes:\n\nnumbers &lt;- c(4, 9, 16, 25, 36)\nresult &lt;- mean(round(sqrt(numbers), 2))\nresult\n\n[1] 4\n\n\nWith the native pipe:\n\nnumbers &lt;- c(4, 9, 16, 25, 36)\nresult &lt;- numbers |&gt; \n  sqrt() |&gt; \n  round(2) |&gt; \n  mean()\nresult\n\n[1] 4\n\n\nMuch more readable! You can follow the data flow from left to right, top to bottom."
  },
  {
    "objectID": "tutorial2-pipe.html#the-magrittr-pipe",
    "href": "tutorial2-pipe.html#the-magrittr-pipe",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "The Magrittr Pipe %>%",
    "text": "The Magrittr Pipe %&gt;%\nThe %&gt;% pipe comes from the magrittr package and is widely used in the tidyverse ecosystem (dplyr, ggplot2, etc.). It works very similarly to the native pipe but has some additional features.\n\nLoading the Package\n\nlibrary(magrittr)  # For standalone use\n# OR\nlibrary(dplyr)     # Automatically loads %&gt;%\n# OR  \nlibrary(tidyverse) # Loads entire tidyverse, including %&gt;%\n\n\n\nBasic Usage\nThe same example with %&gt;%:\n\nlibrary(magrittr)\nnumbers &lt;- c(4, 9, 16, 25, 36)\nresult &lt;- numbers %&gt;% \n  sqrt() %&gt;% \n  round(2) %&gt;% \n  mean()\nresult"
  },
  {
    "objectID": "tutorial2-pipe.html#practical-data-analysis-examples",
    "href": "tutorial2-pipe.html#practical-data-analysis-examples",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Practical Data Analysis Examples",
    "text": "Practical Data Analysis Examples\nLet’s look at more realistic examples using a dataset. We’ll use the built-in mtcars dataset.\n\nExample 1: Data Summarization\nTask: Find the average miles per gallon (mpg) for cars with more than 4 cylinders, rounded to 1 decimal place.\nWithout pipes:\n\nresult &lt;- round(mean(mtcars[mtcars$cyl &gt; 4, \"mpg\"]), 1)\nresult\n\n[1] 16.6\n\n\nWith native pipe:\n\nresult &lt;- mtcars |&gt; \n  subset(cyl &gt; 4) |&gt; \n  subset(select = mpg) |&gt; \n  unlist() |&gt; \n  mean() |&gt; \n  round(1)\nresult\n\n[1] 16.6\n\n\n\n\nExample 2: Using with dplyr\nIf you’re using dplyr (part of tidyverse), pipes become even more powerful:\n\nlibrary(dplyr)\n\n# Find the 3 most fuel-efficient cars by transmission type\nmtcars |&gt; \n  group_by(am) |&gt; \n  arrange(desc(mpg)) |&gt; \n  slice_head(n = 3) |&gt; \n  select(mpg, am, cyl)"
  },
  {
    "objectID": "tutorial2-pipe.html#key-differences-between-and",
    "href": "tutorial2-pipe.html#key-differences-between-and",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Key Differences Between |> and %>%",
    "text": "Key Differences Between |&gt; and %&gt;%\nWhile both pipes work similarly for basic operations, there are some important differences:\n\n1. Availability\n\n|&gt; is built into R 4.1.0+ (no packages needed)\n%&gt;% requires the magrittr package or tidyverse\n\n\n\n2. Placeholder Usage\nMagrittr pipe %&gt;% with placeholder:\n\n# When you need the piped value in a position other than first argument\ndata %&gt;% \n  lm(y ~ x, data = .)  # The dot (.) represents the piped data\n\nNative pipe |&gt; with placeholder:\n\n# R 4.2.0+ syntax\ndata |&gt; \n  lm(y ~ x, data = _)  # Underscore (_) as placeholder\n\n# Alternative for all R 4.1.0+ versions\ndata |&gt; \n  (\\(x) lm(y ~ z, data = x))()  # Anonymous function\n\n\n\n3. Performance\nThe native pipe |&gt; is slightly faster since it’s built into R’s core."
  },
  {
    "objectID": "tutorial2-pipe.html#when-to-use-which-pipe",
    "href": "tutorial2-pipe.html#when-to-use-which-pipe",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "When to Use Which Pipe?",
    "text": "When to Use Which Pipe?\nUse the native pipe |&gt; when:\n\nYou’re using R 4.1.0 or later\nYou want to minimize package dependencies\nYou’re doing straightforward piping (most common case)\nPerformance is critical\n\nUse the magrittr pipe %&gt;% when:\n\nYou’re working with tidyverse packages\nYou need advanced features like the dot placeholder\nYou’re working with legacy code that uses %&gt;%\nYou’re collaborating with others who use tidyverse"
  },
  {
    "objectID": "tutorial2-pipe.html#best-practices",
    "href": "tutorial2-pipe.html#best-practices",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Best Practices",
    "text": "Best Practices\n\n1. Format for Readability\n\n# Good: One function per line, properly indented\ndata |&gt; \n  filter(condition) |&gt; \n  group_by(variable) |&gt; \n  summarise(mean_value = mean(value))\n\n# Avoid: Everything on one line (hard to read)\ndata |&gt; filter(condition) |&gt; group_by(variable) |&gt; summarise(mean_value = mean(value))\n\n\n\n2. Don’t Overuse Pipes\n\n# Sometimes simple assignment is clearer\nx &lt;- mean(data$variable)\n\n# Instead of\nx &lt;- data |&gt; pull(variable) |&gt; mean()\n\n\n\n3. Break Long Chains\nIf your pipe chain gets very long (&gt;10 steps), consider breaking it into smaller chunks with intermediate variables."
  },
  {
    "objectID": "tutorial2-pipe.html#common-errors-and-solutions",
    "href": "tutorial2-pipe.html#common-errors-and-solutions",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Common Errors and Solutions",
    "text": "Common Errors and Solutions\n\nError: Object not found\n\n# This won't work - data isn't defined\ndata |&gt; mean()\n\n# Solution: Make sure your starting object exists\nmy_data &lt;- c(1, 2, 3, 4, 5)\nmy_data |&gt; mean()"
  },
  {
    "objectID": "tutorial2-pipe.html#conclusion",
    "href": "tutorial2-pipe.html#conclusion",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Conclusion",
    "text": "Conclusion\nThe pipe operator is a powerful tool that makes R code more readable and intuitive. Whether you use the native pipe |&gt; or the magrittr pipe %&gt;%, the key is consistency within your projects.\nStart incorporating pipes into your workflow gradually—begin with simple chains and work your way up to more complex data manipulation tasks. Your future self (and your collaborators) will thank you for the cleaner, more readable code!"
  },
  {
    "objectID": "tutorial2-pipe.html#additional-resources",
    "href": "tutorial2-pipe.html#additional-resources",
    "title": "The R Pipe Operator: Making Your Code Flow",
    "section": "Additional Resources",
    "text": "Additional Resources\n\nR for Data Science - Pipes chapter\nmagrittr documentation\nR 4.1.0 release notes"
  },
  {
    "objectID": "tutorial5.html",
    "href": "tutorial5.html",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "",
    "text": "Large Language Models (LLMs) like ChatGPT, Claude, and others can be extremely useful for writing and implementing code. When used properly, they can also be powerful learning aids that can actually help you develop R skills and overall familiarity with programming. However, blindly copying and pasting code without understanding will not help you learn and will likely result in worse answers than not using AI at all.\nThis tutorial will teach you how to effectively collaborate with AI chatbots to enhance your learning while maintaining academic integrity and developing genuine programming skills."
  },
  {
    "objectID": "tutorial5.html#introduction",
    "href": "tutorial5.html#introduction",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "",
    "text": "Large Language Models (LLMs) like ChatGPT, Claude, and others can be extremely useful for writing and implementing code. When used properly, they can also be powerful learning aids that can actually help you develop R skills and overall familiarity with programming. However, blindly copying and pasting code without understanding will not help you learn and will likely result in worse answers than not using AI at all.\nThis tutorial will teach you how to effectively collaborate with AI chatbots to enhance your learning while maintaining academic integrity and developing genuine programming skills."
  },
  {
    "objectID": "tutorial5.html#learning-objectives",
    "href": "tutorial5.html#learning-objectives",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nBy the end of this tutorial, you’ll be able to:\n\nFormulate effective prompts that get you helpful, educational responses\nCritically evaluate AI-generated code before using it\nLearn from AI explanations rather than just copying code\nDebug and modify AI-generated solutions\nRecognize when AI advice is wrong and how to verify it"
  },
  {
    "objectID": "tutorial5.html#understanding-what-llms-can-and-cannot-do",
    "href": "tutorial5.html#understanding-what-llms-can-and-cannot-do",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Understanding What LLMs Can and Cannot Do",
    "text": "Understanding What LLMs Can and Cannot Do\n\nWhat LLMs Excel At\n✅ Code Explanation\n\nBreaking down complex R syntax\nExplaining what specific functions do\nClarifying statistical concepts\n\n✅ Debugging Help\n\nIdentifying syntax errors\nSuggesting fixes for common problems\nExplaining error messages\n\n✅ Alternative Approaches\n\nShowing different ways to solve the same problem\nSuggesting more efficient code\nIntroducing new functions or packages\n\n✅ Learning Support\n\nProviding step-by-step explanations\nCreating practice examples\nAnswering conceptual questions\n\n\n\nWhat LLMs May Struggle With\n❌ Domain-Specific Context\n\nUnderstanding your specific assignment requirements\nKnowing your dataset’s unique characteristics\nApplying course-specific conventions\n\n❌ Current Information\n\nLatest R package updates\nRecent statistical methods - Current best practices (beyond training data)\n\n❌ Complex Statistical Reasoning\n\nChoosing appropriate statistical tests\nInterpreting results in context\nMaking methodological decisions\n\n❌ Academic Integrity\n\nUnderstanding your course’s specific policies\nKnowing what constitutes your own work\nBalancing assistance with independent learning"
  },
  {
    "objectID": "tutorial5.html#crafting-effective-prompts",
    "href": "tutorial5.html#crafting-effective-prompts",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Crafting Effective Prompts",
    "text": "Crafting Effective Prompts\n\nThe CLEAR Framework\nUse this framework to structure your prompts:\nContext\n\nProvide relevant background\n\nLevel\n\nSpecify your experience level\n\nExample\n\nInclude sample data or code\n\nAction\n\nBe specific about what you want\n\nRestriction\n\nSet boundaries on the response\n\n\n\nExample: Poor vs. Good Prompts\n❌ Poor Prompt:\n\"Help me summarize a variable in R\"\n✅ Good Prompt:\n\"I'm an undergraduate political science student learning how to code in R. I have a dataset with variables for GDP per capita (gdp_pc) and democracy scores (democracy) for 50 countries, and I need to run descriptive statistics for one or more off the variables.\n\nHere's a sample of my data:\ncountry | gdp_pc | democracy\n--------|--------|----------\nUSA     | 65000  | 8.5\nGermany | 48000  | 9.1\n\nCan you show me how to:\n1. Summarize the variable?\n2. Create a histogram of the variable?\n3. Understand why we should use descriptive statistics?\n\nPlease explain each step so I can understand the logic, and use comments in the code.\""
  },
  {
    "objectID": "tutorial5.html#working-through-simple-examples",
    "href": "tutorial5.html#working-through-simple-examples",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Working Through Simple Examples",
    "text": "Working Through Simple Examples\nLet’s start with basic data analysis tasks that are common in undergraduate coursework. We’ll build complexity gradually.\n\nExample 1: Getting Basic Statistics\nScenario: You have a dataset of country information and need to calculate descriptive statistics.\nYour Prompt:\nI'm new to R and have a dataset called 'countries' with variables for population, GDP, and region. I need to calculate the mean, median, and standard deviation for population. Can you show me how to do this step by step with explanations?\nGood AI Response:\n\n\nCode\n# Calculate basic statistics for population\nmean(countries$population)          # Average population\nmedian(countries$population)        # Middle value when sorted\nsd(countries$population)           # Standard deviation (spread)\n\n# Get all statistics at once\nsummary(countries$population)      # Shows min, max, quartiles, mean\n\n\n✅ What Makes This Good:\n\nSimple, clear code\nComments explain what each function does\nShows both individual functions and summary approach\n\n🤔 Follow-Up Questions to Ask:\n1. \"What does standard deviation tell me about the data?\"\n2. \"Why might median be different from mean?\"\n3. \"What if my data has missing values?\"\n\n\nExample 2: Data Filtering and Grouping\nYour Prompt:\nI want to find the average GDP for European countries only. My dataset has a 'region' column with values like \"Europe\", \"Asia\", etc. How do I filter the data and then calculate the mean? Please explain each step.\nGood AI Response:\n\n\nCode\nlibrary(dplyr)  # Load the dplyr package for data manipulation\n\n# Method 1: Filter then calculate mean\neuropean_countries &lt;- countries %&gt;%\n  filter(region == \"Europe\")        # Keep only European countries\n\nmean(european_countries$gdp)        # Calculate mean GDP\n\n# Method 2: Do it all in one step\neurope_avg_gdp &lt;- countries %&gt;%\n  filter(region == \"Europe\") %&gt;%    # Filter to Europe\n  summarize(avg_gdp = mean(gdp))    # Calculate mean\n\n# Method 3: Compare all regions\ncountries %&gt;%\n  group_by(region) %&gt;%              # Group by region\n  summarize(avg_gdp = mean(gdp))    # Calculate mean for each group\n\n\n✅ Learning Opportunities:\n\nShows multiple approaches (good for learning!)\nIntroduces the pipe operator %&gt;%\nDemonstrates filtering and grouping concepts\n\n🤔 Questions to Deepen Understanding:\n1. \"What's the difference between filter() and select()?\"\n2. \"Can you explain how the pipe operator (%&gt;%) works?\"\n3. \"How would I filter for multiple conditions?\"\n\n\nExample 3: Creating New Variables\nYour Prompt:\nI need to create a new variable that categorizes countries as \"High\", \"Medium\", or \"Low\" GDP based on their GDP values. How do I create this new column? I'm a beginner so please explain the logic.\nGood AI Response:\n\n\nCode\nlibrary(dplyr)\n\n# Method 1: Using ifelse for simple categories\ncountries &lt;- countries %&gt;%\n  mutate(gdp_category = ifelse(gdp &gt; 50000, \"High\",\n                        ifelse(gdp &gt; 20000, \"Medium\", \"Low\")))\n\n# Method 2: Using case_when (cleaner for multiple categories)\ncountries &lt;- countries %&gt;%\n  mutate(gdp_category = case_when(\n    gdp &gt; 50000 ~ \"High\",\n    gdp &gt; 20000 ~ \"Medium\",\n    TRUE ~ \"Low\"                    # Everything else gets \"Low\"\n  ))\n\n# Check your work\ntable(countries$gdp_category)       # Count how many in each category\n\n\n✅ Good Teaching Elements:\n\nShows two methods for comparison\nExplains the logic of nested conditions\nIncludes verification step\n\n🤔 Learning Questions:\n1. \"What does the TRUE in case_when() do?\"\n2. \"How do I choose the cutoff values for categories?\"\n3. \"What if I want to base categories on percentiles instead?\""
  },
  {
    "objectID": "tutorial5.html#critical-evaluation-of-ai-code",
    "href": "tutorial5.html#critical-evaluation-of-ai-code",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Critical Evaluation of AI Code",
    "text": "Critical Evaluation of AI Code\n\nAlways Ask Yourself These Questions\n\nDoes this code make sense for my problem?\n\nAre the variable names correct?\nIs the statistical method appropriate?\nDo the steps follow a logical order?\n\nCan I explain what each line does?\n\nIf not, ask the AI to explain further\nTest your understanding by modifying small parts\n\nDoes this follow best practices?\n\nAre there proper comments?\nIs the code readable and well-structured?\nAre packages loaded appropriately?"
  },
  {
    "objectID": "tutorial5.html#learning-focused-interaction-strategies",
    "href": "tutorial5.html#learning-focused-interaction-strategies",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Learning-Focused Interaction Strategies",
    "text": "Learning-Focused Interaction Strategies\n\nStrategy 1: Request Step-by-Step Explanations\nInstead of just asking for code, ask for the reasoning:\n\"Before showing me the code, can you explain the logic of why we use a line of best fit in a bivariate scatter plot? Then show me the code with comments explaining how each part implements that logic.\"\n\n\nStrategy 2: Ask for Alternative Approaches\n\"You showed me one way to generate a graph in R. Are there other methods I could use? What are the pros and cons of each approach?\"\n\n\nStrategy 3: Request Practice Problems\n\"Can you create a similar but simpler example with made-up data so I can practice the concepts before applying them to my real dataset?\"\n\n\nStrategy 4: Seek Conceptual Clarification\n\"I understand the code, but I'm confused about interpreting the line of best fit. Can you explain what I should take away from a scatter plot with a regression line in the context of political science research?\""
  },
  {
    "objectID": "tutorial5.html#common-pitfalls-and-how-to-avoid-them",
    "href": "tutorial5.html#common-pitfalls-and-how-to-avoid-them",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Common Pitfalls and How to Avoid Them",
    "text": "Common Pitfalls and How to Avoid Them\n\nPitfall 1: The Copy-Paste Trap\nProblem: Copying code without understanding leads to:\n\nInability to modify code for your specific needs\nPoor performance on exams or future assignments\nIncorrect analyses when assumptions don’t hold\n\nSolution: Always ask “why” and “how” questions\n\"Why do we use this specific function instead of alternatives?\"\n\"How would I modify this code if my variable names were different?\"\n\"What would happen if I changed this parameter?\"\n\n\nPitfall 2: Assuming AI is Always Right\nProblem: LLMs can make mistakes, especially with:\n\nRecent R package updates\nDomain-specific best practices\nComplex statistical procedures\n\nSolution: Always verify important claims\n\"Can you provide documentation or references for this approach?\"\n\"How can I verify that this method is appropriate for my data?\"\n\"What are the limitations or assumptions of this technique?\"\n\n\nPitfall 3: Over-Reliance on AI\nProblem: Using AI for every small problem prevents skill development\nSolution: Try first, then ask for help\n# Your workflow should be:\n1. Attempt the problem yourself\n2. Identify specific stuck points\n3. Ask targeted questions about those points\n4. Try to implement and understand the solution\n5. Ask follow-up questions if needed"
  },
  {
    "objectID": "tutorial5.html#advanced-tips-for-better-learning",
    "href": "tutorial5.html#advanced-tips-for-better-learning",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Advanced Tips for Better Learning",
    "text": "Advanced Tips for Better Learning\n\nTip 1: Ask for Multiple Solutions\n\"Show me three different ways to visualize the relationship between income and voting probability, and explain when each approach is most useful.\"\n\n\nTip 2: Request Error Handling\n\"What are common errors students make when running logistic regression, and how can I avoid or fix them?\"\n\n\nTip 3: Seek Connections to Theory\n\"How does this statistical analysis connect to political science theories about voter participation?\"\n\n\nTip 4: Practice Explanation\n\"If I had to explain these results to someone who doesn't know statistics, how would I describe what the model tells us about voter behavior?\""
  },
  {
    "objectID": "tutorial5.html#building-your-programming-intuition",
    "href": "tutorial5.html#building-your-programming-intuition",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Building Your Programming Intuition",
    "text": "Building Your Programming Intuition\nThe goal isn’t to become dependent on AI, but to use it as a learning accelerator. Here’s how:\n\nDevelop Pattern Recognition\n\nNotice common code structures AI suggests\nLearn to recognize when similar patterns apply\nBuild your own library of template solutions\n\n\n\nBuild Debugging Skills\n\nAsk AI to explain error messages\nLearn common debugging strategies\nPractice identifying problems independently\n\n\n\nUnderstand the “Why” Behind the Code\n\nAlways ask for explanations of approach choices\nRequest information about alternatives\nLearn the reasoning behind coding decisions"
  },
  {
    "objectID": "tutorial5.html#conclusion",
    "href": "tutorial5.html#conclusion",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Conclusion",
    "text": "Conclusion\nAI chatbots can be powerful allies in learning R and statistics, but they work best when you approach them as teaching assistants rather than homework completion services. The key principles are:\n\nStart with your own effort - Try first, then ask for help\nAsk for understanding - Don’t just request code\nVerify and test - AI can make mistakes\nLearn actively - Engage with explanations and ask follow-ups\nMaintain integrity - Follow your course policies\n\nRemember: The goal is to become a competent data analyst who can think critically about statistical problems. AI should enhance your learning journey, not replace the learning process itself."
  },
  {
    "objectID": "tutorial5.html#quick-reference-effective-prompt-templates",
    "href": "tutorial5.html#quick-reference-effective-prompt-templates",
    "title": "Using AI Chatbots to Learn R Programming",
    "section": "Quick Reference: Effective Prompt Templates",
    "text": "Quick Reference: Effective Prompt Templates\n\nFor Debugging\n\"I'm getting this error: [paste error]. Here's my code: [paste code]. I'm trying to [describe goal]. Can you help me understand what's wrong and how to fix it?\"\n\n\nFor Learning Concepts\n\"I'm learning about [concept] in my political science methods class. Can you explain [specific aspect] in simple terms and show how it applies to [your domain example]?\"\n\n\nFor Code Review\n\"Here's my code for [task]: [paste code]. Does this look correct? Are there ways I could improve it or make it more readable?\"\n\n\nFor Alternative Approaches\n\"I solved [problem] using [current approach]. Are there other ways to do this? What are the trade-offs between different methods?\""
  },
  {
    "objectID": "tutorial4.html",
    "href": "tutorial4.html",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "",
    "text": "Political science data rarely comes in perfect, analysis-ready form. Before running any statistical analyses, you’ll often need to transform your variables to make them more suitable for modeling or to better understand their underlying patterns. This tutorial will walk you through essential variable transformation techniques, focusing on why these transformations matter for political science research.\nBy the end of this tutorial, you’ll understand:\n\nHow to identify and interpret different types of distributions\nWhen and why to apply logarithmic transformations\nEssential techniques for recoding categorical variables\nBest practices for handling missing data and outliers"
  },
  {
    "objectID": "tutorial4.html#introduction",
    "href": "tutorial4.html#introduction",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "",
    "text": "Political science data rarely comes in perfect, analysis-ready form. Before running any statistical analyses, you’ll often need to transform your variables to make them more suitable for modeling or to better understand their underlying patterns. This tutorial will walk you through essential variable transformation techniques, focusing on why these transformations matter for political science research.\nBy the end of this tutorial, you’ll understand:\n\nHow to identify and interpret different types of distributions\nWhen and why to apply logarithmic transformations\nEssential techniques for recoding categorical variables\nBest practices for handling missing data and outliers"
  },
  {
    "objectID": "tutorial4.html#setting-up-loading-libraries-and-data",
    "href": "tutorial4.html#setting-up-loading-libraries-and-data",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Setting Up: Loading Libraries and Data",
    "text": "Setting Up: Loading Libraries and Data\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.4     ✔ readr     2.1.5\n✔ forcats   1.0.1     ✔ stringr   1.6.0\n✔ ggplot2   4.0.0     ✔ tibble    3.3.0\n✔ lubridate 1.9.4     ✔ tidyr     1.3.1\n✔ purrr     1.2.0     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(scales)\n\n\nAttaching package: 'scales'\n\nThe following object is masked from 'package:purrr':\n\n    discard\n\nThe following object is masked from 'package:readr':\n\n    col_factor\n\nset.seed(1234)  # For reproducible examples\n\nFor this tutorial, we’ll work with both simulated data and a real-world example using country-level political and economic indicators.\n\n# Create sample data that mimics real political science variables\ncountries &lt;- data.frame(\n  country = paste(\"Country\", 1:100),\n  gdp_per_capita = rlnorm(100, meanlog = 8, sdlog = 1.5),  # Log-normal distribution\n  population = rlnorm(100, meanlog = 15, sdlog = 2),       # Highly skewed\n  democracy_score = rbeta(100, 2, 2) * 10,                # Bounded 0-10\n  election_turnout = rnorm(100, 65, 15),                   # Normal-ish\n  regime_type = sample(c(\"Democracy\", \"Hybrid\", \"Autocracy\"), 100,\n                      prob = c(0.4, 0.3, 0.3), replace = TRUE)\n)\n\nhead(countries)\n\n    country gdp_per_capita population democracy_score election_turnout\n1 Country 1      487.55416  7489758.2        6.349214         58.29560\n2 Country 2     4519.44222  1264975.2        6.907414         76.44765\n3 Country 3    15163.73142  3730254.4        5.522234         87.07578\n4 Country 4       88.36301  1196659.4        6.917598         71.65497\n5 Country 5     5674.21231   626561.9        5.874055         58.67417\n6 Country 6     6368.27453  4565225.9        7.069599         64.39998\n  regime_type\n1   Democracy\n2      Hybrid\n3   Autocracy\n4   Autocracy\n5   Democracy\n6   Democracy"
  },
  {
    "objectID": "tutorial4.html#part-1-understanding-distributions",
    "href": "tutorial4.html#part-1-understanding-distributions",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Part 1: Understanding Distributions",
    "text": "Part 1: Understanding Distributions\n\nWhat Do Distributions Tell Us?\nThe distribution of a variable shows us how values are spread across the range of possible outcomes. In political science, understanding distributions helps us:\n\nChoose appropriate statistical methods\nIdentify unusual cases or outliers\nMake valid comparisons across groups\nCommunicate findings effectively\n\n\n\nVisualizing Distributions\nLet’s examine the distribution of GDP per capita in our sample:\n\n# Basic histogram\ncountries %&gt;%\n  ggplot(aes(x = gdp_per_capita)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Distribution of GDP per Capita\",\n       x = \"GDP per Capita (USD)\",\n       y = \"Count\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\nWhat do you notice? The distribution is heavily right-skewed—most countries cluster at lower GDP levels, with a few very wealthy countries creating a long right tail.\n\n\nTypes of Distributions in Political Science\nNormal Distribution: Symmetric, bell-shaped curve. Many statistical tests assume normality.\n\n# Election turnout - closer to normal\ncountries %&gt;%\n  ggplot(aes(x = election_turnout)) +\n  geom_histogram(bins = 15, fill = \"darkgreen\", alpha = 0.7) +\n  labs(title = \"Distribution of Election Turnout\",\n       x = \"Turnout (%)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\nSkewed Distributions: Common with economic variables, population sizes, conflict casualties.\n\n# Population - highly right-skewed\ncountries %&gt;%\n  ggplot(aes(x = population)) +\n  geom_histogram(bins = 20, fill = \"coral\", alpha = 0.7) +\n  labs(title = \"Distribution of Population\",\n       x = \"Population\",\n       y = \"Count\") +\n  scale_x_continuous(labels = label_scientific())"
  },
  {
    "objectID": "tutorial4.html#part-2-the-power-of-logarithmic-transformations",
    "href": "tutorial4.html#part-2-the-power-of-logarithmic-transformations",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Part 2: The Power of Logarithmic Transformations",
    "text": "Part 2: The Power of Logarithmic Transformations\n\nWhy Log Transformations Matter\nLogarithmic transformations are crucial in political science because they:\n\nReduce skewness in right-skewed distributions\nStabilize variance across different scales\nMake relationships linear that are otherwise exponential\nAllow meaningful interpretation of percentage changes\n\n\n\nWhen to Use Log Transformations\nUse log transformations when:\n\nVariables span several orders of magnitude (GDP, population, military spending)\nYou observe exponential relationships\nYou want to interpret effects as percentage changes\nThe variable has a long right tail\n\n\n\nApplying Log Transformations\n\n# Add log-transformed variables\ncountries &lt;- countries %&gt;%\n  mutate(\n    log_gdp = log(gdp_per_capita),\n    log_population = log(population)\n  )\n\n\n\nComparing Original vs. Log-Transformed\n\n# Create side-by-side comparison\np1 &lt;- countries %&gt;%\n  ggplot(aes(x = gdp_per_capita)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Original GDP per Capita\", x = \"GDP per Capita\") +\n  theme_minimal()\n\np2 &lt;- countries %&gt;%\n  ggplot(aes(x = log_gdp)) +\n  geom_histogram(bins = 20, fill = \"steelblue\", alpha = 0.7) +\n  labs(title = \"Log GDP per Capita\", x = \"Log(GDP per Capita)\") +\n  theme_minimal()\n\n# Display plots side by side (you might need gridExtra package)\n# grid.arrange(p1, p2, ncol = 2)\n\nKey Insight: The log transformation converts the right-skewed distribution into something much closer to normal!\n\n\nInterpreting Log-Transformed Variables\nWhen you use log-transformed variables in regression:\n\nA 1-unit change in log(X) represents a 100% increase in X\nA 0.1-unit change in log(X) represents approximately a 10% increase in X\nThis makes economic interpretations much more intuitive\n\n\n# Example: How does log GDP relate to democracy scores?\ncountries %&gt;%\n  ggplot(aes(x = log_gdp, y = democracy_score)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", color = \"red\") +\n  labs(title = \"Relationship between Log GDP and Democracy\",\n       x = \"Log(GDP per Capita)\",\n       y = \"Democracy Score\") +\n  theme_minimal()\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "tutorial4.html#part-3-recoding-variables",
    "href": "tutorial4.html#part-3-recoding-variables",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Part 3: Recoding Variables",
    "text": "Part 3: Recoding Variables\n\nWhy Recode Variables?\nRecoding involves changing how variables are categorized or valued. Common reasons:\n\nSimplifying analysis: Converting continuous variables to categories\nFixing data problems: Standardizing inconsistent coding\nCreating meaningful groups: Collapsing small categories\nHandling missing data: Deciding how to treat different types of missingness\n\n\n\nCreating Categorical Variables from Continuous Ones\n\n# Create GDP categories\ncountries &lt;- countries %&gt;%\n  mutate(\n    gdp_category = case_when(\n      gdp_per_capita &lt; 5000 ~ \"Low Income\",\n      gdp_per_capita &lt; 20000 ~ \"Middle Income\",\n      gdp_per_capita &gt;= 20000 ~ \"High Income\"\n    ),\n    # Alternative using quantiles\n    gdp_tertile = case_when(\n      gdp_per_capita &lt;= quantile(gdp_per_capita, 0.33) ~ \"Bottom Third\",\n      gdp_per_capita &lt;= quantile(gdp_per_capita, 0.67) ~ \"Middle Third\",\n      TRUE ~ \"Top Third\"\n    )\n  )\n\n# Check the distribution\ntable(countries$gdp_category)\n\n\n  High Income    Low Income Middle Income \n           11            72            17 \n\n\n\n\nRecoding Categorical Variables\n\n# Sometimes you need to collapse categories\ncountries &lt;- countries %&gt;%\n  mutate(\n    simple_regime = case_when(\n      regime_type == \"Democracy\" ~ \"Democratic\",\n      regime_type %in% c(\"Hybrid\", \"Autocracy\") ~ \"Non-Democratic\"\n    )\n  )\n\ntable(countries$simple_regime)\n\n\n    Democratic Non-Democratic \n            43             57 \n\n\n\n\nCreating Dummy Variables\nFor regression analysis, you often need to convert categorical variables into numeric dummy variables:\n\n# Create dummy variables for regime types\ncountries &lt;- countries %&gt;%\n  mutate(\n    is_democracy = ifelse(regime_type == \"Democracy\", 1, 0),\n    is_hybrid = ifelse(regime_type == \"Hybrid\", 1, 0),\n    is_autocracy = ifelse(regime_type == \"Autocracy\", 1, 0)\n  )\n\n# Check correlations (should be negative - if one is 1, others are 0)\ncor(countries[c(\"is_democracy\", \"is_hybrid\", \"is_autocracy\")])\n\n             is_democracy  is_hybrid is_autocracy\nis_democracy    1.0000000 -0.5416386   -0.5550941\nis_hybrid      -0.5416386  1.0000000   -0.3985498\nis_autocracy   -0.5550941 -0.3985498    1.0000000"
  },
  {
    "objectID": "tutorial4.html#part-4-handling-missing-data-and-outliers",
    "href": "tutorial4.html#part-4-handling-missing-data-and-outliers",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Part 4: Handling Missing Data and Outliers",
    "text": "Part 4: Handling Missing Data and Outliers\n\nIdentifying Missing Data Patterns\n\n# Introduce some missing data for demonstration\ncountries_with_missing &lt;- countries %&gt;%\n  mutate(\n    # Randomly assign some missing values\n    democracy_score = ifelse(runif(n()) &lt; 0.1, NA, democracy_score),\n    election_turnout = ifelse(runif(n()) &lt; 0.05, NA, election_turnout)\n  )\n\n# Check missing data patterns\nsummary(countries_with_missing)\n\n   country          gdp_per_capita        population        democracy_score \n Length:100         Min.   :    88.36   Min.   :1.081e+04   Min.   :0.9347  \n Class :character   1st Qu.:   778.29   1st Qu.:1.071e+06   1st Qu.:3.9636  \n Mode  :character   Median :  1674.76   Median :3.491e+06   Median :5.7426  \n                    Mean   :  8739.23   Mean   :3.223e+07   Mean   :5.3331  \n                    3rd Qu.:  6046.71   3rd Qu.:1.147e+07   3rd Qu.:6.8295  \n                    Max.   :136419.04   Max.   :1.439e+09   Max.   :9.1644  \n                                                            NA's   :11      \n election_turnout regime_type           log_gdp       log_population  \n Min.   :14.06    Length:100         Min.   : 4.481   Min.   : 9.288  \n 1st Qu.:55.15    Class :character   1st Qu.: 6.657   1st Qu.:13.881  \n Median :64.25    Mode  :character   Median : 7.423   Median :15.066  \n Mean   :63.95                       Mean   : 7.765   Mean   :15.082  \n 3rd Qu.:73.24                       3rd Qu.: 8.707   3rd Qu.:16.255  \n Max.   :98.78                       Max.   :11.823   Max.   :21.088  \n NA's   :8                                                            \n gdp_category       gdp_tertile        simple_regime       is_democracy \n Length:100         Length:100         Length:100         Min.   :0.00  \n Class :character   Class :character   Class :character   1st Qu.:0.00  \n Mode  :character   Mode  :character   Mode  :character   Median :0.00  \n                                                          Mean   :0.43  \n                                                          3rd Qu.:1.00  \n                                                          Max.   :1.00  \n                                                                        \n   is_hybrid     is_autocracy \n Min.   :0.00   Min.   :0.00  \n 1st Qu.:0.00   1st Qu.:0.00  \n Median :0.00   Median :0.00  \n Mean   :0.28   Mean   :0.29  \n 3rd Qu.:1.00   3rd Qu.:1.00  \n Max.   :1.00   Max.   :1.00  \n                              \n\n\n\n\nIdentifying Outliers\n\n# Box plot to identify outliers\ncountries %&gt;%\n  ggplot(aes(y = gdp_per_capita)) +\n  geom_boxplot() +\n  labs(title = \"GDP per Capita - Identifying Outliers\",\n       y = \"GDP per Capita\") +\n  theme_minimal()\n\n\n\n\n\n\n\n# Statistical approach: values beyond 1.5 * IQR\noutlier_threshold &lt;- quantile(countries$gdp_per_capita, 0.75) +\n                     1.5 * IQR(countries$gdp_per_capita)\n\ncountries %&gt;%\n  filter(gdp_per_capita &gt; outlier_threshold) %&gt;%\n  select(country, gdp_per_capita)\n\n       country gdp_per_capita\n1    Country 3       15163.73\n2   Country 20      111720.20\n3   Country 31       15575.37\n4   Country 41       26219.13\n5   Country 57       35303.11\n6   Country 59       33152.20\n7   Country 62      136419.04\n8   Country 66       42857.10\n9   Country 68       23196.13\n10  Country 69       21902.31\n11  Country 75       66529.87\n12  Country 93       38520.51\n13 Country 100       71802.58\n\n\n\n\nHandling Outliers\n\n# Option 1: Remove outliers (use cautiously!)\ncountries_no_outliers &lt;- countries %&gt;%\n  filter(gdp_per_capita &lt;= outlier_threshold)\n\n# Option 2: Winsorize (cap at certain percentiles)\ncountries_winsorized &lt;- countries %&gt;%\n  mutate(\n    gdp_winsorized = case_when(\n      gdp_per_capita &gt; quantile(gdp_per_capita, 0.95) ~ quantile(gdp_per_capita, 0.95),\n      gdp_per_capita &lt; quantile(gdp_per_capita, 0.05) ~ quantile(gdp_per_capita, 0.05),\n      TRUE ~ gdp_per_capita\n    )\n  )"
  },
  {
    "objectID": "tutorial4.html#part-5-best-practices-and-common-pitfalls",
    "href": "tutorial4.html#part-5-best-practices-and-common-pitfalls",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Part 5: Best Practices and Common Pitfalls",
    "text": "Part 5: Best Practices and Common Pitfalls\n\nDocumentation is Key\n\n# Always document your transformations\ncountries_final &lt;- countries %&gt;%\n  mutate(\n    # Log transformation for skewed economic variables\n    log_gdp_pc = log(gdp_per_capita),  # Natural log of GDP per capita\n    log_pop = log(population),         # Natural log of population\n\n    # Standardized democracy score (0-1 scale)\n    democracy_01 = democracy_score / 10,\n\n    # Binary regime classification\n    democratic = ifelse(regime_type == \"Democracy\", 1, 0)\n  ) %&gt;%\n  # Keep original variables for comparison\n  select(country, gdp_per_capita, log_gdp_pc, democracy_score, democracy_01,\n         regime_type, democratic, everything())\n\n\n\nCommon Mistakes to Avoid\n\nTaking logs of zero or negative values - Add a small constant if necessary\nOver-transforming - Not every skewed variable needs transformation\nLosing track of original scales - Keep both versions when possible\nMechanical outlier removal - Investigate outliers before removing them\n\n\n\nChecking Your Work\n\n# Always examine your transformations\nsummary(countries_final[c(\"gdp_per_capita\", \"log_gdp_pc\", \"democracy_score\", \"democracy_01\")])\n\n gdp_per_capita        log_gdp_pc     democracy_score   democracy_01    \n Min.   :    88.36   Min.   : 4.481   Min.   :0.9347   Min.   :0.09347  \n 1st Qu.:   778.29   1st Qu.: 6.657   1st Qu.:3.8713   1st Qu.:0.38713  \n Median :  1674.76   Median : 7.423   Median :5.7161   Median :0.57161  \n Mean   :  8739.23   Mean   : 7.765   Mean   :5.2812   Mean   :0.52812  \n 3rd Qu.:  6046.71   3rd Qu.: 8.707   3rd Qu.:6.8229   3rd Qu.:0.68229  \n Max.   :136419.04   Max.   :11.823   Max.   :9.1644   Max.   :0.91644  \n\n# Visualize relationships\ncountries_final %&gt;%\n  ggplot(aes(x = log_gdp_pc, y = democracy_01)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\") +\n  labs(title = \"Log GDP vs. Standardized Democracy Score\",\n       x = \"Log(GDP per Capita)\",\n       y = \"Democracy Score (0-1 scale)\")\n\n`geom_smooth()` using formula = 'y ~ x'"
  },
  {
    "objectID": "tutorial4.html#conclusion",
    "href": "tutorial4.html#conclusion",
    "title": "Variable Transformations in R: Understanding Distributions and Data Cleaning",
    "section": "Conclusion",
    "text": "Conclusion\nVariable transformations are fundamental tools in political science research. Key takeaways:\n\nUnderstand your data first - Always visualize distributions before transforming\nLog transformations are powerful for right-skewed economic/demographic variables\nThoughtful recoding can simplify analysis and improve interpretation\nDocument everything - Future you will thank present you\nKeep originals - Preserve untransformed variables for robustness checks"
  },
  {
    "objectID": "tutorial6.html",
    "href": "tutorial6.html",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "",
    "text": "If you’ve ever found yourself confused about why R can’t find your data file, or why your script works on your computer but not your classmate’s, or why you keep getting “file not found” errors even though you know the file is there—this tutorial is for you!\nUnderstanding how your computer organizes files and how R navigates through them is absolutely crucial for doing data analysis. It’s like learning to read a map before going on a road trip. Once you understand the basic concepts, you’ll spend less time wrestling with technical issues and more time focusing on your actual research."
  },
  {
    "objectID": "tutorial6.html#why-do-i-need-to-understand-file-systems",
    "href": "tutorial6.html#why-do-i-need-to-understand-file-systems",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "",
    "text": "If you’ve ever found yourself confused about why R can’t find your data file, or why your script works on your computer but not your classmate’s, or why you keep getting “file not found” errors even though you know the file is there—this tutorial is for you!\nUnderstanding how your computer organizes files and how R navigates through them is absolutely crucial for doing data analysis. It’s like learning to read a map before going on a road trip. Once you understand the basic concepts, you’ll spend less time wrestling with technical issues and more time focusing on your actual research."
  },
  {
    "objectID": "tutorial6.html#what-is-a-file-system",
    "href": "tutorial6.html#what-is-a-file-system",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "What is a file system?",
    "text": "What is a file system?\nA file system is simply how your computer organizes and stores files. Think of it like a giant filing cabinet with folders inside folders inside folders. Every file on your computer has an “address” that tells you exactly where to find it.\nLet’s start with some basics that apply to both Windows and Mac:\n\nFiles are individual documents (like my_data.csv or homework.R)\nFolders (also called directories) are containers that hold files and other folders\nPaths are the “addresses” that tell you exactly where a file or folder is located\n\n\nUnderstanding Your Home Directory\nYour home directory is a special folder that belongs just to you. It’s like your personal office space on the computer. This is where your operating system stores your personal files like Documents, Downloads, Desktop, etc.\n\nWindowsMac\n\n\nOn Windows, your home directory is typically:\nC:\\Users\\YourName\\\nInside your home directory, you’ll find familiar folders like:\n\nDocuments\\ - where many programs save files by default\nDesktop\\ - the files you see on your desktop\nDownloads\\ - where your web browser saves downloaded files\nPictures\\, Music\\, Videos\\ - for media files\n\n\n\nOn Mac, your home directory is typically:\n/Users/YourName/\nInside your home directory, you’ll find familiar folders like:\n\nDocuments/ - where many programs save files by default\nDesktop/ - the files you see on your desktop\nDownloads/ - where your web browser saves downloaded files\nPictures/, Music/, Movies/ - for media files\n\n\n\n\nWhy does this matter for R? When you first open RStudio, it usually sets your working directory to your home directory. Understanding this helps you navigate to where your files actually are!"
  },
  {
    "objectID": "tutorial6.html#operating-system-differences",
    "href": "tutorial6.html#operating-system-differences",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Operating System Differences",
    "text": "Operating System Differences\n\nWindowsMac\n\n\nOn Windows, your file system starts with drive letters like C:, D:, etc. The main drive is usually C:. Folders are separated by backslashes (\\).\nA typical Windows path might look like:\nC:\\Users\\YourName\\Documents\\POLS101\\homework1.R\nThis means:\n\nStart at the C: drive\nGo to the Users folder\nThen to the folder with your username\nThen to Documents\nThen to POLS101\nFinally, find the file homework1.R\n\n\n\nOn Mac, the file system starts with a forward slash (/) and doesn’t use drive letters. Folders are separated by forward slashes (/).\nA typical Mac path might look like:\n/Users/YourName/Documents/POLS101/homework1.R\nThis means:\n\nStart at the root directory (/)\nGo to the Users folder\nThen to the folder with your username\nThen to Documents\nThen to POLS101\nFinally, find the file homework1.R"
  },
  {
    "objectID": "tutorial6.html#what-is-a-working-directory",
    "href": "tutorial6.html#what-is-a-working-directory",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "What is a working directory?",
    "text": "What is a working directory?\nYour working directory is like your current location in the file system. It’s where R is “standing” right now. When you tell R to open a file, it looks for that file starting from your working directory.\nThink of it this way: if you’re in a library and someone says “go get the book on the third shelf,” you need to know which section of the library you’re currently in to know which third shelf they mean!\nLet’s see what your current working directory is:\n\n# This shows you where R is currently \"standing\"\ngetwd()\n\n[1] \"/Users/alexnewhouse/alexbnewhouse.github.io\"\n\n\nWhen you first open RStudio, your working directory is usually set to your home folder. But this might not be where your data files are stored!"
  },
  {
    "objectID": "tutorial6.html#common-file-system-mistakes-and-how-to-fix-them",
    "href": "tutorial6.html#common-file-system-mistakes-and-how-to-fix-them",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Common file system mistakes (and how to fix them)",
    "text": "Common file system mistakes (and how to fix them)\n\nMistake #1: “But I can see the file!”\nThe Problem: You can see your data file in your file explorer/finder, but R says it can’t find it.\nWhat’s happening: Just because you can see the file doesn’t mean R knows where to look for it. R only looks in your current working directory (and its subdirectories) by default.\nThe Solution: Either move your file to your working directory, or tell R exactly where to find your file.\n\n\nMistake #2: Hard-coding full paths\nThe Problem: Your code works on your computer but breaks on your classmate’s computer.\n\n# This will only work on YOUR computer\ndata &lt;- read.csv(\"C:/Users/YourName/Documents/POLS101/data.csv\")  # Windows\ndata &lt;- read.csv(\"/Users/YourName/Documents/POLS101/data.csv\")   # Mac\n\nWhat’s happening: You’ve written the specific path that exists on your computer, but your classmate has a different username and maybe even a different folder structure.\nThe Solution: Use relative paths (explained below) or set up your project properly.\n\n\nMistake #3: Not organizing your project files\nThe Problem: Your files are scattered all over your computer, making it hard to keep track of what goes with what project.\nThe Solution: Create a dedicated folder for each project and keep everything related to that project inside it."
  },
  {
    "objectID": "tutorial6.html#absolute-vs.-relative-paths",
    "href": "tutorial6.html#absolute-vs.-relative-paths",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Absolute vs. Relative Paths",
    "text": "Absolute vs. Relative Paths\nUnderstanding the difference between absolute and relative paths is crucial:\n\nAbsolute Paths\nAn absolute path gives the complete address from the very top of your file system:\n\nWindowsMac\n\n\nC:\\Users\\YourName\\Documents\\POLS101\\data\\survey_results.csv\n\n\n/Users/YourName/Documents/POLS101/data/survey_results.csv\n\n\n\n\n\nRelative Paths\nA relative path gives directions from your current location (working directory):\ndata/survey_results.csv\nThis says: “from where I am now, go into the ‘data’ folder and find ‘survey_results.csv’”"
  },
  {
    "objectID": "tutorial6.html#best-practices-for-organizing-your-r-projects",
    "href": "tutorial6.html#best-practices-for-organizing-your-r-projects",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Best practices for organizing your R projects",
    "text": "Best practices for organizing your R projects\n\n1. Create a project folder\nFor each research project or class, create a dedicated folder:\n\nWindowsMac\n\n\nC:\\Users\\YourName\\Documents\\POLS101\\\n\n\n/Users/YourName/Documents/POLS101/\n\n\n\n\n\n2. Use a consistent folder structure\nInside your project folder, create subfolders like:\nPOLS101/\n  ├── data/           # Store your datasets here\n  ├── scripts/        # Store your R code here\n  ├── output/         # Store graphs, tables, etc. here\n  └── documents/      # Store papers, notes, etc. here\n\n\n3. Use RStudio Projects\nRStudio has a fantastic feature called “Projects” that makes file management much easier. When you create an RStudio Project, it automatically sets your working directory to the project folder.\nHere’s how to create a new project:\n\nWindowsMac\n\n\n\nIn RStudio, go to File → New Project\nChoose “New Directory”\nChoose “New Project”\nGive it a name (like “POLS101”)\nChoose where to put it (like C:\\Users\\YourName\\Documents\\)\nClick “Create Project”\n\n\n\n\nIn RStudio, go to File → New Project\nChoose “New Directory”\nChoose “New Project”\nGive it a name (like “POLS101”)\nChoose where to put it (like /Users/YourName/Documents/)\nClick “Create Project”"
  },
  {
    "objectID": "tutorial6.html#working-with-files-in-r",
    "href": "tutorial6.html#working-with-files-in-r",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Working with files in R",
    "text": "Working with files in R\nNow let’s look at how to actually work with files in your R code:\n\nSetting your working directory\nIf you’re not using RStudio Projects, you might need to set your working directory manually:\n\n# Set working directory (adjust the path for your computer)\nsetwd(\"/Users/YourName/Documents/POLS101\")  # Mac\nsetwd(\"C:/Users/YourName/Documents/POLS101\")  # Windows (note: forward slashes work in R even on Windows!)\n\nPro tip: Notice that even on Windows, you can use forward slashes (/) in R code. This makes your code more portable between operating systems!\n\n\nLoading data with relative paths\nOnce your working directory is set correctly, you can use relative paths:\n\n# This will work for anyone who has the same folder structure\nlibrary(tidyverse)\n\n# Load data from the 'data' subfolder\nsurvey_data &lt;- read_csv(\"data/survey_results.csv\")\n\n# Create a plot and save it to the 'output' subfolder\nsurvey_data %&gt;% \n  ggplot(aes(x = age, y = income)) + \n  geom_point() + \n  ggsave(\"output/age_income_plot.png\")\n\n\n\nChecking if files exist\nBefore trying to load a file, you can check if R can find it:\n\n# Check if a file exists\nfile.exists(\"data/survey_results.csv\")\n\n# List all files in a directory\nlist.files(\"data/\")\n\n# List all CSV files in a directory\nlist.files(\"data/\", pattern = \"*.csv\")"
  },
  {
    "objectID": "tutorial6.html#r-markdown-knitting-and-file-paths-a-common-source-of-confusion",
    "href": "tutorial6.html#r-markdown-knitting-and-file-paths-a-common-source-of-confusion",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "R Markdown, Knitting, and File Paths: A Common Source of Confusion",
    "text": "R Markdown, Knitting, and File Paths: A Common Source of Confusion\nOne of the biggest sources of confusion for students is understanding how R Markdown (.Rmd) files handle file paths differently when you’re working interactively versus when you knit the document. This is critically important for completing assignments!\n\nThe Big Difference: Interactive R vs. Knitting\nWhen you run code chunks interactively in RStudio (clicking the green arrow or pressing Ctrl/Cmd+Enter), R uses your current working directory. But when you knit your R Markdown document, R temporarily changes the working directory to wherever your .Rmd file is saved.\nThis means:\n\nRunning chunks interactively: Uses getwd() (your current working directory)\nKnitting the document: Uses the folder where your .Rmd file is located as the working directory\n\n\n\nWhy This Causes Problems\nHere’s a typical scenario that trips up students:\n\n# Your project structure:\nPOLS101/\n  ├── homework1.Rmd          # Your assignment file\n  ├── data/\n  │   └── survey_data.csv    # Your data file\n  └── scripts/\n      └── analysis.R\n\nThe Problem:\n\nYour working directory is set to POLS101/\nYou run this code in a chunk and it works fine:\n\ndata &lt;- read_csv(\"data/survey_data.csv\")  # Works when running interactively\n\nBut when you knit, you get “file not found” errors!\n\nWhat’s happening: When you knit, R looks for data/survey_data.csv starting from where homework1.Rmd is located (which is POLS101/), so it actually looks for POLS101/data/survey_data.csv - which is correct! But if your working directory was set to something else when running interactively, the paths might not match.\n\n\nSolutions for R Markdown Success\n\nSolution 1: Keep your .Rmd file in your project root\nThe easiest solution is to save your .Rmd file in the main project folder:\nPOLS101/                   # &lt;- Your project folder\n  ├── homework1.Rmd        # &lt;- Save your .Rmd here\n  ├── data/\n  │   └── survey_data.csv\n  └── output/\n      └── plots/\nThen use relative paths from your project root:\n\n# This will work both interactively AND when knitting\ndata &lt;- read_csv(\"data/survey_data.csv\")\nggplot(data, aes(x = age)) + geom_histogram()\nggsave(\"output/plots/age_histogram.png\")\n\n\n\nSolution 2: Use the here package (highly recommended!)\nThe here package solves this problem elegantly:\n\n# Install and load the here package\ninstall.packages(\"here\")\nlibrary(here)\n\n# Use here() to build file paths that work everywhere\ndata &lt;- read_csv(here(\"data\", \"survey_data.csv\"))\nggsave(here(\"output\", \"plots\", \"age_histogram.png\"))\n\nThe here() function automatically finds your project root (where your .Rproj file is, or other indicators) and builds paths from there.\n\n\nSolution 3: Check your working directory in your .Rmd file\nAdd this to the top of your R Markdown document to see where knitting thinks it is:\n\n# Check working directory when knitting\ngetwd()\n\n# List files to see what's available\nlist.files()\n\n# Check if your data file exists from this location\nfile.exists(\"data/survey_data.csv\")\n\n\n\n\nCommon R Markdown Assignment Workflow\nHere’s a workflow that works reliably for assignments:\n\nCreate an RStudio Project for your assignment\nSave your .Rmd file in the project root folder\nCreate subfolders for data, output, etc.\nUse relative paths or the here package in your .Rmd file\nTest by knitting early and often - don’t wait until the last minute!\n\n\n# Example assignment .Rmd structure:\n# This should work both interactively and when knitting\n\nlibrary(tidyverse)\nlibrary(here)  # Recommended!\n\n# Load data (works both ways)\ndata &lt;- read_csv(here(\"data\", \"assignment_data.csv\"))\n\n# Do your analysis\nsummary_stats &lt;- data %&gt;% \n  summarize(mean_age = mean(age),\n            mean_income = mean(income))\n\n# Save your plot (works both ways) \ndata %&gt;% \n  ggplot(aes(x = age, y = income)) + \n  geom_point() + \n  ggsave(here(\"output\", \"scatter_plot.png\"))\n\n\n\nTesting Your Assignment Before Submission\nBefore submitting your assignment, always:\n\nKnit your document - don’t just run the chunks!\nCheck that the HTML file contains all your plots and output\nVerify that any saved files (plots, tables) were created where you expected\n\n\n\nTroubleshooting R Markdown File Path Issues\nIf your .Rmd file won’t knit due to file path errors:\nStep 1: Add this diagnostic chunk at the top of your .Rmd:\n\n# Diagnostic information\ncat(\"Current working directory:\", getwd(), \"\\n\")\ncat(\"Files in current directory:\", paste(list.files(), collapse = \", \"), \"\\n\")\ncat(\"Does data folder exist?\", file.exists(\"data\"), \"\\n\")\ncat(\"Files in data folder:\", paste(list.files(\"data\"), collapse = \", \"), \"\\n\")\n\nStep 2: Compare the output when you: - Run the chunk interactively - Knit the document\nStep 3: Adjust your file paths based on what you see. If knitting shows you’re in a different location than you expected, adjust your paths accordingly.\n\n\nWhy This Matters for Your Grades\nMany students lose points on assignments because:\n\nTheir code runs fine in RStudio but the .Rmd won’t knit\nTheir knitted HTML is missing plots because the save paths were wrong\nThey submit .Rmd files that professors can’t knit because of path issues\n\nUnderstanding this concept will save you time and frustration, and help ensure your assignments are complete when you submit them!"
  },
  {
    "objectID": "tutorial6.html#practical-example-setting-up-a-research-project",
    "href": "tutorial6.html#practical-example-setting-up-a-research-project",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Practical example: Setting up a research project",
    "text": "Practical example: Setting up a research project\nLet’s walk through setting up a complete research project:\n\nStep 1: Create your project structure\n\nWindowsMac\n\n\nUsing File Explorer:\n\nNavigate to C:\\Users\\YourName\\Documents\\\nCreate a new folder called “Research_Project”\nInside that folder, create subfolders: data, scripts, output, documents\n\n\n\nUsing Finder:\n\nNavigate to /Users/YourName/Documents/\nCreate a new folder called “Research_Project”\nInside that folder, create subfolders: data, scripts, output, documents\n\n\n\n\n\n\nStep 2: Create an RStudio Project\n\nOpen RStudio\nFile → New Project → Existing Directory\nBrowse to your “Research_Project” folder\nClick “Create Project”\n\n\n\nStep 3: Test your setup\nCreate a new R script and try this code:\n\n# Check where you are\ngetwd()\n\n# Create some sample data\nlibrary(tidyverse)\nsample_data &lt;- data.frame(\n  id = 1:10,\n  score = rnorm(10, mean = 75, sd = 10)\n)\n\n# Save it to your data folder\nwrite_csv(sample_data, \"data/sample_data.csv\")\n\n# Load it back (to test that it works)\nloaded_data &lt;- read_csv(\"data/sample_data.csv\")\nprint(loaded_data)\n\n# Create a simple plot and save it\nloaded_data %&gt;% \n  ggplot(aes(x = id, y = score)) + \n  geom_point() + \n  geom_line() +\n  labs(title = \"Sample Data Plot\",\n       x = \"ID\",\n       y = \"Score\") +\n  ggsave(\"output/sample_plot.png\")\n\nprint(\"Success! Your project structure is working correctly.\")"
  },
  {
    "objectID": "tutorial6.html#troubleshooting-common-issues",
    "href": "tutorial6.html#troubleshooting-common-issues",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Troubleshooting common issues",
    "text": "Troubleshooting common issues\n\n“Cannot open file ‘filename’: No such file or directory”\nSolutions:\n\nCheck your working directory: getwd()\nCheck if the file exists: file.exists(\"your_filename\")\nList files in your directory: list.files()\nMake sure the file name is spelled correctly (including the extension!)\n\n\n\n“Permission denied” errors\n\nWindowsMac\n\n\n\nMake sure the file isn’t open in Excel or another program\nCheck that you have write permissions to the folder\nTry running RStudio as administrator (right-click → “Run as administrator”)\n\n\n\n\nMake sure the file isn’t open in Excel or another program\nCheck the file permissions in Finder (right-click → Get Info)\nYou might need to change the folder permissions\n\n\n\n\n\n\nOneDrive and cloud storage complications (Windows users especially!)\nThe Problem: OneDrive automatically takes over your Documents, Desktop, and Pictures folders on Windows, moving them to the cloud. This can cause major confusion because your file paths might look like:\nC:\\Users\\YourName\\OneDrive\\Documents\\POLS101\\data.csv\ninstead of the expected:\nC:\\Users\\YourName\\Documents\\POLS101\\data.csv\nWhat’s happening: Microsoft OneDrive “hijacks” your default folders to sync them to the cloud. While this can be useful for backup, it changes where your files are actually stored and can make file paths unpredictable.\nCommon signs you have OneDrive issues:\n\nYour Documents folder has a cloud icon next to it\nFile paths include “OneDrive” in them\nFiles sometimes can’t be found even though you can see them in File Explorer\nYou get “sync pending” or “file not available” errors\n\nSolutions:\n\nCheck your actual file locations - Use getwd() in R to see where you really are, and list.files() to see what’s actually in your working directory.\nUse the full OneDrive path - If your files are in OneDrive, use the full path:\n\n# If your files are in OneDrive, you might need:\ndata &lt;- read_csv(\"C:/Users/YourName/OneDrive/Documents/POLS101/data.csv\")\n\nMove your project outside OneDrive - Create your R projects in a folder that’s NOT managed by OneDrive:\nC:\\Users\\YourName\\R_Projects\\POLS101\\\nTurn off OneDrive folder redirection (advanced):\n\nRight-click OneDrive icon in system tray → Settings\nGo to “Backup” tab → “Manage backup”\nTurn off backup for Documents, Pictures, Desktop folders\nWarning: This will move your files back to local folders\n\nUse RStudio Projects - They help manage paths automatically, regardless of where your project folder is located.\n\nPro tip: If you’re unsure whether OneDrive is affecting your files, open File Explorer and look at the address bar when you navigate to Documents. If it shows “OneDrive” in the path, then OneDrive is managing that folder.\n\n\nPaths with spaces or special characters\nIf your folders or files have spaces in their names, you need to put quotes around the entire path:\n\n# Correct way to handle spaces in file names\ndata &lt;- read_csv(\"data/survey results 2024.csv\")\n\n# Or use underscores instead of spaces (recommended)\ndata &lt;- read_csv(\"data/survey_results_2024.csv\")"
  },
  {
    "objectID": "tutorial6.html#advanced-tips",
    "href": "tutorial6.html#advanced-tips",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Advanced tips",
    "text": "Advanced tips\n\nUsing the here package\nThe here package is fantastic for making your code work across different computers and operating systems:\n\n# Install if you don't have it\ninstall.packages(\"here\")\nlibrary(here)\n\n# The here() function automatically finds your project root\ndata &lt;- read_csv(here(\"data\", \"survey_results.csv\"))\n\n# This works the same on Windows, Mac, and Linux!\nggsave(here(\"output\", \"my_plot.png\"))\n\n\n\nEnvironment variables and shortcuts\nR has some built-in shortcuts for common locations:\n\n# Your home directory\npath.expand(\"~\")\n\n# Desktop (usually)\nfile.path(path.expand(\"~\"), \"Desktop\")\n\n# Check all your environment variables\nSys.getenv()"
  },
  {
    "objectID": "tutorial6.html#summary-and-key-takeaways",
    "href": "tutorial6.html#summary-and-key-takeaways",
    "title": "Understanding File Systems and Working Directories in R and RStudio",
    "section": "Summary and key takeaways",
    "text": "Summary and key takeaways\n\nYour computer’s file system is like a filing cabinet - everything has a specific location\nWorking directory is where R is currently “standing” - check it with getwd()\nUse RStudio Projects - they make file management much easier\nOrganize your projects - create dedicated folders with consistent structure\nUse relative paths - they make your code work on different computers\nForward slashes work everywhere - even on Windows, use / in your R code\nCheck if files exist - use file.exists() and list.files() to debug issues\n\nThe most important thing is to be intentional about where you put your files and consistent in how you organize your projects. A little bit of organization at the beginning will save you hours of frustration later!\nRemember: everyone struggles with file paths and working directories when they’re learning R. Don’t get discouraged—with practice, it becomes second nature. And when in doubt, ask for help!"
  },
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Blog Posts",
    "section": "",
    "text": "Welcome to my blog, where I share thoughts on computational social science, extremism research, technology policy, and academic life.\nSubscribe to the RSS feed to stay updated on new posts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFirst post\n\n\n\nresearch\n\ntechnology\n\nacademia\n\n\n\nBrief description of your post\n\n\n\n\n\nDec 15, 2024\n\n\nAlex Newhouse\n\n1 min\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "cv.html",
    "href": "cv.html",
    "title": "Alex Newhouse",
    "section": "",
    "text": "alex.newhouse@colorado.edu | Boulder, CO | linkedin.com/in/alexbnewhouse"
  },
  {
    "objectID": "cv.html#education",
    "href": "cv.html#education",
    "title": "Alex Newhouse",
    "section": "Education",
    "text": "Education\nUniversity of Colorado Boulder, Boulder, CO\nDoctor of Philosophy, Political Science, Anticipated Graduation: May 2027\n\nResearch focus: Measuring online-offline relationships in political violence using machine learning, natural language processing, social network analysis, and time-series causal inference\nComparative Politics and Political Methodology subfields\nPSCI committee: Andrew Q. Philips (co-chair), Jennifer Fitzgerald (co-chair), Alexandra Siegel\nOther advisor: Brian Keegan (Information Science, CU Boulder)\n\nGeorgia Institute of Technology, Atlanta, GA (Remote Student)\nMaster of Science, Analytics, Graduation: Fall 2021\n\nCoursework in Python, R, JavaScript, Deep and Machine Learning, and Data Visualization\n\nMiddlebury Institute of International Studies (MIIS), Monterey, CA\nMaster of Arts, Nonproliferation and Terrorism Studies, Graduation: Dec. 2018\n\nFinancial Crimes Management Specialization; Foreign Language of Study: Russian\n\nMiddlebury College, Middlebury, VT\nBachelor of Arts, Political Science and English Literature, Russian Minor, Graduation: May 2017\n\nGraduated Summa cum Laude, departmental honors in English"
  },
  {
    "objectID": "cv.html#peer-reviewed-publications",
    "href": "cv.html#peer-reviewed-publications",
    "title": "Alex Newhouse",
    "section": "Peer-Reviewed Publications",
    "text": "Peer-Reviewed Publications\nBenton, A., Newhouse, A., and Philips, A. “Mind the Gap! Policy Dissonance and Financial Markets.” May 2025 (under review).\nNewhouse, A. and Kowert, R. “Extremist Identity Creation Through Performative Infighting on Steam.” Frontiers in Psychology (forthcoming).\nKowert, R., Kilmer, E., and Newhouse, A. “Taking it to the Extreme: Prevalence and Nature of Extremist Sentiment in Games.” Frontiers in Psychology. August 2024.\nKowert, R., Kilmer, E., and Newhouse, A. “Culturally Justified Hate: Prevalence and Mental Health Impact of Dark Participation in Games.” Proceedings of the 57th Hawaii International Conference on System Sciences. January 2024.\nNewhouse, A. “The Threat is the Network: The Multi-Node Structure of Neo-Fascist Accelerationism.” CTC Sentinel. June 2021."
  },
  {
    "objectID": "cv.html#book-chapters",
    "href": "cv.html#book-chapters",
    "title": "Alex Newhouse",
    "section": "Book Chapters",
    "text": "Book Chapters\nNewhouse, A. and Kowert, R. “Recruitment and mobilization in digital gaming spaces.” Handbuch Gaming & Rechtsextremismus (ed. Linda Schlegel). Forthcoming.\nKowert, R. and Newhouse, A. “Digital games as cultural assets of influence.” The Sociology of Violent Extremism (ed. Suraj Lakhani and Amarnath Amarasingam). May 2025.\nNewhouse, A. and Kowert, R. “Digital games as vehicles for extremist recruitment and mobilization.” Gaming and Extremism: The Radicalization of Digital Playgrounds (ed. Linda Schlegel and Rachel Kowert). March 2024."
  },
  {
    "objectID": "cv.html#pre-prints-and-other-publications",
    "href": "cv.html#pre-prints-and-other-publications",
    "title": "Alex Newhouse",
    "section": "Pre-Prints and Other Publications",
    "text": "Pre-Prints and Other Publications\nKriner, M., Conroy, M., Newhouse, A., and Lewis, J. “Understanding Accelerationist Narratives: The Great Replacement Theory.” Global Network on Extremism and Technology. May 2022.\nKowert, R., Botehlo, A., and Newhouse, A. “Breaking the Building Blocks of Hate: A Case Study of Minecraft Servers.” Anti-Defamation League. July 2022.\nShadnia, D., Newhouse, A., Kriner, M., and Bradley, A. “Militant Accelerationism Coalitions: A Case-Study in Neo-Fascist Accelerationist Coalition-Building Online.” Center on Terrorism, Extremism, and Counterterrorism. June 2022.\nNewhouse, A. and Gunesch, N. “The Boogaloo Movement Wants to Be Seen as Anti-Racist, But It Has a White Supremacist Fringe.” Center on Terrorism, Extremism, and Counterterrorism. May 2021.\nMcGuffie, K. and Newhouse, A. “The Radicalization Risks of GPT-3 and Advanced Neural Language Models.” Arxiv pre-print. September 2020.\n[Author list truncated] “Release Strategies and the Social Impacts of Language Models.” Arxiv pre-print. August 2019.\nNewhouse, A. “From classifieds to crypto: how white supremacist groups have embraced crowdfunding.” Center on Terrorism, Extremism, and Counterterrorism. 2019."
  },
  {
    "objectID": "cv.html#public-writing",
    "href": "cv.html#public-writing",
    "title": "Alex Newhouse",
    "section": "Public Writing",
    "text": "Public Writing\nJackson, D., Conroy, M., and Newhouse, A. “Insiders’ View of the January 6th Committee’s Social Media Investigation.” Just Security. January 2023.\nNewhouse, A. “Far-right activists on social media telegraphed violence weeks in advance of the attack on the US Capitol.” The Conversation. January 2021. (35,400 views)\nNewhouse, A. “Parler is bringing together mainstream conservatives, anti-Semites and white supremacists as the social media platform attracts millions of Trump supporters.” The Conversation. November 2020, updated January 2021. (241,000 views)"
  },
  {
    "objectID": "cv.html#conference-presentations",
    "href": "cv.html#conference-presentations",
    "title": "Alex Newhouse",
    "section": "Conference Presentations",
    "text": "Conference Presentations\n“Partisan Polarization Shapes Visual Framing of COVID-19 in U.S. Media.” Paper Presentation. Midwest Political Science Association Annual Meeting. April 2025.\n“Elite Influence in an Online Neo-Fascist Forum.” Paper Presentation. Midwest Political Science Association Annual Meeting. April 2025.\n“Media Bias in COVID-19 Coverage.” Paper Presentation. Midwest Political Science Association Annual Meeting. April 2024.\n“Manifesting a Coup: Cross-Website Toxicity and Extremism on the Dot Win Network Prior to January 6, 2021.” Poster Session. New Frontiers in Text as Data. November 2023.\n“Mitigating Harm in Design.” Game Developers Conference. March 2023.\n“Accelerationism Research in Practice.” Terrorism and Social Media Conference. June 2022.\n“The Landscape of Extremist Behavior in Games.” Game Developers Conference. March 2022.\n“The Aesthetics of Acceleration.” Reactionary Digital Politics: Ideologies, Rhetorics, Aesthetics. June 2021."
  },
  {
    "objectID": "cv.html#grants-and-funding",
    "href": "cv.html#grants-and-funding",
    "title": "Alex Newhouse",
    "section": "Grants and Funding",
    "text": "Grants and Funding\nMiddlebury Conflict Transformation Grant, FY 2022 - $50,000, Principal Investigator\nDHS Terrorism and Targeted Violence Prevention Grant, FY 2022 - $700,000, Principal Investigator\nDHS Terrorism and Targeted Violence Prevention Grant, FY 2021 – $630,000, Principal Investigator"
  },
  {
    "objectID": "cv.html#teaching-experience",
    "href": "cv.html#teaching-experience",
    "title": "Alex Newhouse",
    "section": "Teaching Experience",
    "text": "Teaching Experience\n\nAs Instructor of Record\n\nDigital Extremism, Middlebury Institute of International Studies, Spring 2023 (self-designed)\nThe Study of Post-War Fascism, Middlebury College, Winter 2026 (self-designed)\nOnline Extremism, Middlebury College, Jan. 2021 (self-designed)\n\n\n\nAs Teaching Assistant\n\nIntroduction to Western Political Thought, CU Boulder, Spring 2025\nIntroduction to Comparative Politics, CU Boulder, Fall 2024\nQuantitative Research Methods, CU Boulder, Spring 2024\nIntroduction to International Relations, CU Boulder, Fall 2023"
  },
  {
    "objectID": "cv.html#research-positions-and-labs",
    "href": "cv.html#research-positions-and-labs",
    "title": "Alex Newhouse",
    "section": "Research Positions and Labs",
    "text": "Research Positions and Labs\n\nFellowships\nSenior Non-Resident Fellow, MIIS, Mar. 2025 – Present\n\n\nLabs\nGraduate Affiliate, Institute for Behavioral Science, 2023 – Present\nLab Member, Colorado Laboratory on Users, Media, and Networks, 2023 – Present\nLab Member, American Politics Research Lab, 2024 - Present\n\n\nPaid Research Positions\nGraduate Research Assistant, CU Boulder, 2025 – Present - Exploratory quantitative and qualitative analysis for Jennifer Fitzgerald and Jeffrey Nonmacher’s work on global perceptions of inequality\nGraduate Research Assistant, CU Boulder, 2025 – Present - Qualitative and quantitative support for Alexandra Siegel’s book project on the interactions between online influencers and political regimes\nGraduate Research Assistant, CU Boulder, 2024 – Present - “‘Mind the Gap!’ Policy Dissonance and Financial Markets”, supervised by Andrew Q. Philips and in collaboration with Allyson L. Benton\nGraduate Research Assistant, CU Boulder, 2023 – Present - “Partisan Polarization Shapes Visual Framing of COVID-19 in U.S. Media”, supervised by Andrew Q. Philips and in collaboration with Komal P. Kaur\nGraduate Research Assistant, CU Boulder, Summer 2024 - Data analysis and synthesis of original survey data on Australian attitudes around Voice to Parliament referendum, supervised by Carew Boulding\nGraduate Research Assistant, MIIS, Fall 2018 - Assisted with launch of the Center on Terrorism, Extremism, and Counterterrorism, and built first project to secure funding from technology industry partner, supervised by Jason Blazakis\nGraduate Research Assistant, MIIS, Fall 2018 - UNIDIR-funded project collecting data on global cybersecurity projects, supervised by Elaine Korzak\nGraduate Research Assistant, MIIS, Feb. 2018 – Dec. 2018 - Contributed to META Lab research and education projects, supervised by Philip Murphy\nGraduate Research Assistant, MIIS and Naval Postgraduate School, Feb. 2018 – Aug. 2018 - Lead data analyst on processing and analysis of Islamic State primary-source documents, supervised by Craig Whiteside\nUndergraduate Research Assistant, Middlebury, Aug. 2016 – May 2017 - Data analysis and background research on intervention, civil conflict, and the CNN Effect, supervised by Orion Lewis\nUndergraduate Research Assistant, MIIS, Feb. 2016 – May 2016 - Contributed to various research and analysis projects at the James Martin Center for Nonproliferation Studies"
  },
  {
    "objectID": "cv.html#mentorship-and-supervision",
    "href": "cv.html#mentorship-and-supervision",
    "title": "Alex Newhouse",
    "section": "Mentorship and Supervision",
    "text": "Mentorship and Supervision\nManaged a total of 30 Middlebury undergraduate and graduate research assistants between 2019-2023.\nSample projects:\n\nPartnership with Roblox to detect and mitigate violent and hateful networks of users\nPartnership with Spectrum Labs to develop datasets of online toxicity, white supremacy, and violent behavior in 7 languages\nLong-term research and policy support for a video social media platform’s policy enforcement team\n\nSupervised independent projects for 15 Middlebury undergraduate interns and 4 graduate fellows.\nSample projects supervised:\n\n“Italian Neofascism and the Years of Lead: A Closer Look at the Nuclei Armati Rivoluzionari.”\n“Archeofuturism and Its Parallels to Militant Accelerationism.”\n“The French Far-Right on Telegram: Laundering Extremism Through Support For Political Activism.”\n“Christian Identity’s New Role on the Extreme Right.”\n“The AI Revolution and Its Implications on Domestic Counterterrorism.”"
  },
  {
    "objectID": "cv.html#professional-experience",
    "href": "cv.html#professional-experience",
    "title": "Alex Newhouse",
    "section": "Professional Experience",
    "text": "Professional Experience\nMiddlebury Institute of International Studies, Monterey, CA\nSenior Research Fellow – Emerging Tech, CTEC, June 2023-August 2024\n\nSupervised CTEC’s portfolio of projects that engage with the video game industry\nLed cutting-edge research into the influence of emerging technologies like AI on worldwide extremism and terrorism trends\nAssisted with CTEC’s relationships with the tech industry\nEngaged with media, published research, and served as a leading voice at CTEC on various extremism-related issues\nCoordinated CTEC’s data science projects and analyst resources\n\nDeputy Director, CTEC, April 2021-May 2023\n\nOversaw CTEC’s research, operations, and development\nHired and managed a team of full-time, part-time, and student researchers\nDirectly supervised external relationships with technology companies, government agencies, academia, and nonprofits\nServed as Principal Investigator on several research projects, including two funded by grants from the Department of Homeland Security\nBuilt CTEC’s Data Science functions, including overseeing development of innovative tools using AI, machine learning, NLP, and social network analysis\n\nData Analyst and Research Lead, CTEC, Jan. 2019-April 2021\n\nBuilt and delivered an end-to-end data analysis and visualization platform for understanding trends in hate and violence on social video platforms at scale\nLed development of glossary of right-wing terminology, slang, and images\nRaised money to expand CTEC by developing client relationships in the tech industry\n\nUnited States House of Representatives, Washington, DC\nInvestigative Consultant, Select Committee to Investigate the January 6th Attack, Feb. 2022-Oct. 2022\n\nDesigned and executed cross-platform investigations on how online communities contributed to the January 6th attack on the U.S. Capitol\nProvided expertise in online extremism, including the activities of Oath Keepers, Proud Boys, QAnon, Three Percenters, and neo-fascists, to investigators and Members of Congress\nDrafted sections of public hearings conducted by Members of Congress in June-July 2022\nDelivered written material included in the Committee’s final report\n\nSony Interactive Entertainment, San Mateo, CA\nData Governance Analyst, Jan. 2019-Jan. 2020\n\nManaged and coordinated PlayStation’s data-sharing process\nAssisted with creation and implementation of privacy policies and standards\nOperationalized SIE’s privacy-by-design goals so that users have safety, security, control, and transparency over their own data\nCollaborated closely with privacy lawyers to develop a shared language that allows for understanding of and compliance with privacy legislation, like the GDPR and CCPA\n\nSony Interactive Entertainment, San Diego, CA\nGlobal Fraud Monitoring Analyst Intern, Summer 2018\n\nWorked on fraud monitoring and development of fraud prevention rules for PlayStation Network\nLed project streamlining cooperation between Customer Service and Global Fraud Management\nAssisted Holiday Season 2018 planning by creating Splunk dashboards, revising runbooks, and laying out on-call procedures\n\nUber, San Francisco, CA\nCrisis Management Intern, Fall 2017\n\nWorked with the Crisis Management/Business Continuity Team to develop business continuity plans for all divisions within Uber\nDeveloped set of metrics using SQL for assessing crisis response and impact, which the Crisis Management Team incorporated into its performance reports and incident database\nCooperated with a variety of stakeholders to help develop a global Crisis Management Playbook\nAssisted Protective Intelligence team with research for a city launch geopolitical risk assessment\n\nGameSpot – CBS Interactive, San Francisco, CA\nEditorial Intern, Summers 2014, 2015, 2016"
  },
  {
    "objectID": "cv.html#media-appearances",
    "href": "cv.html#media-appearances",
    "title": "Alex Newhouse",
    "section": "Media Appearances",
    "text": "Media Appearances\nRadio/Podcast: Bloomberg, BBC, NPR’s Morning Edition, All Things Considered, and On Point.\nWritten: Washington Post, New York Times, Vice, Axios, Politico, FiveThirtyEight, Wired, ProPublica.\nVideo: Meet the Press Now, Denver7."
  },
  {
    "objectID": "cv.html#technical-skills",
    "href": "cv.html#technical-skills",
    "title": "Alex Newhouse",
    "section": "Technical Skills",
    "text": "Technical Skills\nProgramming languages: R, Python, MATLAB, Stata\nMachine learning frameworks: PyTorch, Scikit-Learn, Caret, Hugging Face Transformers\nSocial Network Analysis: igraph, statnet\nQuery languages: SQL, Splunk"
  },
  {
    "objectID": "CLAUDE.html",
    "href": "CLAUDE.html",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.\n\n\nThis is a personal academic website built with Quarto, a scientific publishing system. The site belongs to Alex Newhouse, a PhD candidate in Political Science specializing in computational analysis of political violence and extremism.\n\n\n\nQuarto Website Structure: - _quarto.yml - Main configuration file defining website structure, theme (lumen), and navbar - *.qmd - Quarto markdown files that generate the website pages - docs/ - Output directory where rendered HTML files are published (GitHub Pages) - posts/ - Directory containing blog post subdirectories - profpic.jpeg - Profile image used on the main page\nKey Pages: - index.qmd - Homepage with personal bio and research overview - cv.qmd - Academic CV - research.qmd - Research projects and publications - teaching.qmd - Teaching experience and tutorial links - posts.qmd - Blog listing page - tutorial*.qmd - Educational content for quantitative methods\n\n\n\nBuild and Preview:\nquarto render          # Build the entire website to docs/\nquarto preview         # Live preview with auto-reload during development\nquarto render &lt;file&gt;   # Render specific file\nPublishing: The site automatically publishes from the docs/ directory to GitHub Pages when changes are pushed to the main branch.\n\n\n\nAcademic Focus: Content should maintain an academic tone appropriate for a PhD candidate’s professional website. The site showcases research on political violence, extremism, and computational social science.\nTeaching Materials: Tutorial files (tutorial1.qmd, tutorial2-pipe.qmd, etc.) contain educational content for quantitative methods courses. These should be pedagogically sound and accessible to undergraduate students.\nBlog Posts: Located in posts/ subdirectories, focused on research, technology, and academia.\n\n\n\n\nUse lowercase with hyphens for multi-word filenames\nTutorial files: tutorial[number]-[topic].qmd\nBlog posts: posts/YYYY-MM-DD-[title]/index.qmd\n\n\n\n\n\nSearch functionality enabled\nRSS feed for blog posts\nResponsive design with trestles template on homepage\nSocial media links (LinkedIn, Bluesky, GitHub, Email)\nCategory filtering and sorting for blog posts"
  },
  {
    "objectID": "CLAUDE.html#project-overview",
    "href": "CLAUDE.html#project-overview",
    "title": "CLAUDE.md",
    "section": "",
    "text": "This is a personal academic website built with Quarto, a scientific publishing system. The site belongs to Alex Newhouse, a PhD candidate in Political Science specializing in computational analysis of political violence and extremism."
  },
  {
    "objectID": "CLAUDE.html#architecture",
    "href": "CLAUDE.html#architecture",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Quarto Website Structure: - _quarto.yml - Main configuration file defining website structure, theme (lumen), and navbar - *.qmd - Quarto markdown files that generate the website pages - docs/ - Output directory where rendered HTML files are published (GitHub Pages) - posts/ - Directory containing blog post subdirectories - profpic.jpeg - Profile image used on the main page\nKey Pages: - index.qmd - Homepage with personal bio and research overview - cv.qmd - Academic CV - research.qmd - Research projects and publications - teaching.qmd - Teaching experience and tutorial links - posts.qmd - Blog listing page - tutorial*.qmd - Educational content for quantitative methods"
  },
  {
    "objectID": "CLAUDE.html#development-commands",
    "href": "CLAUDE.html#development-commands",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Build and Preview:\nquarto render          # Build the entire website to docs/\nquarto preview         # Live preview with auto-reload during development\nquarto render &lt;file&gt;   # Render specific file\nPublishing: The site automatically publishes from the docs/ directory to GitHub Pages when changes are pushed to the main branch."
  },
  {
    "objectID": "CLAUDE.html#content-guidelines",
    "href": "CLAUDE.html#content-guidelines",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Academic Focus: Content should maintain an academic tone appropriate for a PhD candidate’s professional website. The site showcases research on political violence, extremism, and computational social science.\nTeaching Materials: Tutorial files (tutorial1.qmd, tutorial2-pipe.qmd, etc.) contain educational content for quantitative methods courses. These should be pedagogically sound and accessible to undergraduate students.\nBlog Posts: Located in posts/ subdirectories, focused on research, technology, and academia."
  },
  {
    "objectID": "CLAUDE.html#file-naming",
    "href": "CLAUDE.html#file-naming",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Use lowercase with hyphens for multi-word filenames\nTutorial files: tutorial[number]-[topic].qmd\nBlog posts: posts/YYYY-MM-DD-[title]/index.qmd"
  },
  {
    "objectID": "CLAUDE.html#website-features",
    "href": "CLAUDE.html#website-features",
    "title": "CLAUDE.md",
    "section": "",
    "text": "Search functionality enabled\nRSS feed for blog posts\nResponsive design with trestles template on homepage\nSocial media links (LinkedIn, Bluesky, GitHub, Email)\nCategory filtering and sorting for blog posts"
  }
]